{"clipboardHistory":[{"value":"# Apply Themes\necho \"Applying themes...\"\nln -sf \"$DOTFILES_DIR/.config/gtk-3.0\" \"$HOME/.config/gtk-3.0\"\nln -sf \"$DOTFILES_DIR/.config/gtk-4.0\" \"$HOME/.config/gtk-4.0\"\nln -sf \"$DOTFILES_DIR/.config/kvantum\" \"$HOME/.config/kvantum\"\n\n","recorded":"2024-12-23 16:23:56.038272470","filePath":"null","pinned":false},{"value":"# Symlink Dotfiles\necho \"Symlinking dotfiles...\"\nln -sf \"$DOTFILES_DIR/Hyprland/.config/hypr\" \"$HOME/.config/hypr\"\nln -sf \"$DOTFILES_DIR/Hyprland/.config/kitty\" \"$HOME/.config/kitty\"\nln -sf \"$DOTFILES_DIR/Hyprland/.config/waybar\" \"$HOME/.config/waybar\"\nln -sf \"$DOTFILES_DIR/Hyprland/.config/wlogout\" \"$HOME/.config/wlogout\"\nln -sf \"$DOTFILES_DIR/Btop/.config/btop\" \"$HOME/.config/hypr\"\nln -sf \"$DOTFILES_DIR/Clipse/.config/clipse\" \"$HOME/.config/hypr\"\nln -sf \"$DOTFILES_DIR/dunst/.config/dunst\" \"$HOME/.config/hypr\"\nln -sf \"$DOTFILES_DIR/Gtk/.config/gtk-2.0\" \"$HOME/.config/hypr\"\nln -sf \"$DOTFILES_DIR/Gtk/.config/gtk-3.0\" \"$HOME/.config/hypr\"\nln -sf \"$DOTFILES_DIR/Gtk/.config/gtk-4.0\" \"$HOME/.config/hypr\"\nln -sf \"$DOTFILES_DIR/kvantum/.config/Kvantum\" \"$HOME/.config/hypr\"\nln -sf \"$DOTFILES_DIR/neofetch/.config/neofetch\" \"$HOME/.config/hypr\"\nln -sf \"$DOTFILES_DIR/\" \"$HOME/.config/hypr\"\n# Add more symlinks here as needed.","recorded":"2024-12-23 16:23:44.456866555","filePath":"null","pinned":false},{"value":"neofetch/.config/neofetch/","recorded":"2024-12-23 16:22:37.696914497","filePath":"null","pinned":false},{"value":"kvantum/.config/Kvantum/","recorded":"2024-12-23 16:22:13.134445249","filePath":"null","pinned":false},{"value":"Gtk/.config/gtk-2.0/","recorded":"2024-12-23 16:21:26.593406207","filePath":"null","pinned":false},{"value":"ln -sf \"$DOTFILES_DIR/\" \"$HOME/.config/hypr\"","recorded":"2024-12-23 16:20:57.284352068","filePath":"null","pinned":false},{"value":"dunst/.config/dunst/","recorded":"2024-12-23 16:20:25.976682134","filePath":"null","pinned":false},{"value":"Clipse/.config/clipse/","recorded":"2024-12-23 16:20:06.751324357","filePath":"null","pinned":false},{"value":"Btop/.config/btop/","recorded":"2024-12-23 16:19:38.278596606","filePath":"null","pinned":false},{"value":"ln -sf \"$DOTFILES_DIR/Hyprland/.config/hypr\" \"$HOME/.config/hypr\"\n\n","recorded":"2024-12-23 16:17:19.560446484","filePath":"null","pinned":false},{"value":"ln -sf \"$DOTFILES_DIR/Hyprland/.config/hypr\" \"$HOME/.config/hypr\"","recorded":"2024-12-23 16:17:04.471571399","filePath":"null","pinned":false},{"value":"ln -sf \"$DOTFILES_DIR/waybar\" \"$HOME/.config/waybar\"\nln -sf \"$DOTFILES_DIR/rofi\" \"$HOME/.config/rofi\"\nln -sf \"$DOTFILES_DIR/kitty\" \"$HOME/.config/kitty\"\nln -sf \"$DOTFILES_DIR/Btop\" \"$HOME/.con\"","recorded":"2024-12-23 16:16:59.348188065","filePath":"null","pinned":false},{"value":"#!/bin/bash\n\n# Log File Setup\nLOG_FILE=\"install_log.txt\"\nexec \u003e \u003e(tee -a \"$LOG_FILE\") 2\u003e\u00261\n\n# Update System\necho \"Updating system packages...\"\nsudo pacman -Syu --noconfirm\n\n# Install System Packages\necho \"Installing system packages...\"\nsudo pacman -S --needed --noconfirm aircrack-ng alsa-utils ani-cli ark baobab base base-devel bash-completion blueman bluetui bluez-utils brightnessctl btop chafa clipse-bin crunch discord dolphin dunst efibootmgr egl-wayland eog evince fd firefox fish flatpak git github-desktop-bin gnome-keyring grim grub gruvbox-dark-gtk gst-plugin-pipewire gtk-engine-murrine gvfs gvfs-afc gvfs-google hashcat hcxtools htop hypridle hyprland hyprland-qtutils hyprlock hyprpaper hyprpicker intel-media-driver intel-ucode iwd jdk-openjdk jq kate kitty konsole kvantum kvantum-theme-otto-git libpulse libreoffice-fresh libva-intel-driver linux linux-firmware linux-headers linux-lts linux-lts-headers lxappearance nano neofetch neovim network-manager-applet networkmanager nodejs npm ntfs-3g opencv os-prober pacman-contrib paru paru-debug pipewire pipewire-alsa pipewire-jack pipewire-pulse plasma-meta plasma-workspace plymouth plymouth-kcm polkit-kde-agent python-matplotlib python-pandas python-pip python-pipx python-scikit-learn python-tensorflow qt5-wayland qt6-wayland qt6ct ranger reflector ripgrep rofi-lbonn-wayland-git slurp smartmontools sof-firmware spotify stow streamlink texlive-basic texlive-fontsextra texlive-latexextra texlive-meta thunar thunar-archive-plugin thunar-media-tags-plugin thunar-shares-plugin thunar-vcs-plugin thunar-volman ttf-dejavu ttf-jetbrains-mono-nerd udiskie unzip vim virtualbox visual-studio-code-bin vulkan-intel vulkan-radeon waybar wf-recorder wget whitesur-cursor-theme-git whitesur-icon-theme wireless_tools wireplumber wl-clipboard wlogout wofi xdg-desktop-portal-hyprland xdg-utils xf86-video-amdgpu xf86-video-ati xf86-video-nouveau xf86-video-vmware xorg-xinit yay yay-debug zathura zathura-pdf-poppler zen-browser-bin zram-generator zsh\n\n# Install AUR Packages\necho \"Installing AUR packages...\"\nif ! command -v paru \u0026\u003e /dev/null; then\n    echo \"Paru not found. Installing paru...\"\n    git clone https://aur.archlinux.org/paru.git\n    cd paru\n    makepkg -si --noconfirm\n    cd ..\n    rm -rf paru\nfi\nparu -S --needed --noconfirm ani-cli bluetui-debug clipse-bin crunch crunch-debug github-desktop-bin gruvbox-dark-gtk hyprland-qtutils kvantum-theme-otto-git paru paru-debug rofi-lbonn-wayland-git rofi-lbonn-wayland-git-debug spotify visual-studio-code-bin visual-studio-code-bin-debug waybar-git-debug whitesur-cursor-theme-git whitesur-icon-theme wlogout wlogout-debug yay yay-debug zen-browser-bin\n\n# Clone Dotfiles Repository\necho \"Cloning dotfiles repository...\"\nDOTFILES_DIR=\"$HOME/dotfiles\"\nif [ ! -d \"$DOTFILES_DIR\" ]; then\n    git clone git@github.com:reddy-bhavesh/dotfiles.git \"$DOTFILES_DIR\"\nelse\n    echo \"Dotfiles repository already exists. Pulling latest changes...\"\n    git -C \"$DOTFILES_DIR\" pull\nfi\n\n# Symlink Dotfiles\necho \"Symlinking dotfiles...\"\nln -sf \"$DOTFILES_DIR/.config/hypr\" \"$HOME/.config/hypr\"\nln -sf \"$DOTFILES_DIR/.config/waybar\" \"$HOME/.config/waybar\"\nln -sf \"$DOTFILES_DIR/.config/rofi\" \"$HOME/.config/rofi\"\nln -sf \"$DOTFILES_DIR/.config/kitty\" \"$HOME/.config/kitty\"\n# Add more symlinks here as needed.\n\n# Install Fonts\necho \"Installing fonts...\"\nFONT_DIR=\"$HOME/.local/share/fonts\"\nmkdir -p \"$FONT_DIR\"\ncp -r \"$DOTFILES_DIR/fonts/*\" \"$FONT_DIR\"\nfc-cache -fv\n\n# Apply Themes\necho \"Applying themes...\"\nln -sf \"$DOTFILES_DIR/.config/gtk-3.0\" \"$HOME/.config/gtk-3.0\"\nln -sf \"$DOTFILES_DIR/.config/gtk-4.0\" \"$HOME/.config/gtk-4.0\"\nln -sf \"$DOTFILES_DIR/.config/kvantum\" \"$HOME/.config/kvantum\"\n\n# Enable Services\necho \"Enabling services...\"\nsudo systemctl enable bluetooth\nsudo systemctl enable NetworkManager\n\n# Error Handling Completed\nif [ $? -ne 0 ]; then\n    echo \"Some tasks encountered errors. Check $LOG_FILE for details.\"\nelse\n    echo \"Installation completed successfully.\"\nfi\n","recorded":"2024-12-23 16:12:39.473508544","filePath":"null","pinned":false},{"value":"git@github.com:reddy-bhavesh/dotfiles.git","recorded":"2024-12-23 16:06:17.437914920","filePath":"null","pinned":false},{"value":"pacman -Qqe | grep -i appname\n","recorded":"2024-12-23 16:03:06.656560334","filePath":"null","pinned":false},{"value":"xdg-settings get default-web-browser\nxdg-settings get default-terminal\n","recorded":"2024-12-23 16:02:52.988692002","filePath":"null","pinned":false},{"value":"fc-list\n","recorded":"2024-12-23 16:02:09.718883061","filePath":"null","pinned":false},{"value":"ldd $(which Hyprland)\n","recorded":"2024-12-23 15:59:50.803979853","filePath":"null","pinned":false},{"value":".config/\n","recorded":"2024-12-23 15:56:36.757324571","filePath":"null","pinned":false},{"value":".config/","recorded":"2024-12-23 15:56:29.700883199","filePath":"null","pinned":false},{"value":"pacman -Qqm \u003e aur-package-list.txt\n","recorded":"2024-12-23 15:54:04.033854522","filePath":"null","pinned":false},{"value":"pacman -Qqe \u003e package-list.txt\n","recorded":"2024-12-23 15:53:55.095298860","filePath":"null","pinned":false},{"value":"[godzeus@arch ~]$ pacman -Qq\na52dec\nabseil-cpp\naccounts-qml-module\naccountsservice\nacl\nadobe-source-code-pro-fonts\nadwaita-cursors\nadwaita-icon-theme\nadwaita-icon-theme-legacy\naha\naircrack-ng\nalsa-card-profiles\nalsa-lib\nalsa-topology-conf\nalsa-ucm-conf\nalsa-utils\nani-cli\naom\nappstream\nappstream-qt\napr\napr-util\naquamarine\narchlinux-appstream-data\narchlinux-keyring\nargon2\naria2\naribb24\nark\nat-spi2-core\natkmm\nattica\nattr\naudit\nautoconf\nautomake\navahi\nayatana-ido\nbaloo\nbaloo-widgets\nbaobab\nbase\nbase-devel\nbash\nbash-completion\nbinutils\nbison\nblas\nbluedevil\nblueman\nbluetui\nbluetui-debug\nbluez\nbluez-libs\nbluez-qt\nbluez-utils\nbolt\nboost-libs\nbreeze\nbreeze-gtk\nbreeze-icons\nbrightnessctl\nbrotli\nbtop\nbubblewrap\nbzip2\nc-ares\nca-certificates\nca-certificates-mozilla\nca-certificates-utils\ncairo\ncairomm\ncantarell-fonts\ncatch2\ncblas\ncdparanoia\nchafa\ncheck\nchrono-date\ncifs-utils\nclinfo\nclipse-bin\nclucene\ncmake\ncomposefs\nconvertlit\ncoreutils\ncppdap\ncrunch\ncrunch-debug\ncryptsetup\ncurl\ndav1d\ndb5.3\ndbus\ndbus-broker\ndbus-broker-units\ndbus-glib\ndbus-units\ndconf\nddcutil\ndebugedit\ndefault-cursors\ndesktop-file-utils\ndevice-mapper\ndiffutils\ndiscord\ndiscount\ndiscover\ndjvulibre\ndkms\ndmidecode\ndolphin\ndouble-conversion\ndrkonqi\nduktape\ndunst\ndvisvgm\ne2fsprogs\nebook-tools\neditorconfig-core-c\nefibootmgr\nefivar\negl-wayland\neglexternalplatform\nell\nenchant\neog\nethtool\nevince\nexempi\nexiv2\nexo\nexpat\nfaad2\nfakeroot\nfd\nffcall\nffmpeg\nffmpeg4.4\nfftw\nfile\nfilesystem\nfindutils\nfirefox\nfish\nflac\nflatpak\nflex\nfmt\nfontconfig\nframeworkintegration\nfreerdp2\nfreetype2\nfribidi\nfuse-common\nfuse2\nfuse3\nfzf\ngawk\ngc\ngcc\ngcc-libs\ngcr\ngcr-4\ngd\ngdb\ngdb-common\ngdbm\ngdk-pixbuf2\ngeoclue\ngettext\nghostscript\ngiflib\ngirara\ngit\ngithub-desktop-bin\nglib-networking\nglib2\nglib2-devel\nglibc\nglibmm\nglslang\nglu\ngmp\ngnome-desktop\ngnome-desktop-common\ngnome-keyring\ngnu-free-fonts\ngnupg\ngnutls\ngo\ngobject-introspection\ngobject-introspection-runtime\ngperftools\ngpgme\ngpm\ngraphene\ngraphite\ngraphviz\ngrep\ngrim\ngroff\ngrub\ngruvbox-dark-gtk\ngsettings-desktop-schemas\ngsettings-system-schemas\ngsfonts\ngsm\ngspell\ngssdp\ngst-plugin-pipewire\ngst-plugins-bad-libs\ngst-plugins-base\ngst-plugins-base-libs\ngstreamer\ngtest\ngtk-engine-murrine\ngtk-layer-shell\ngtk-update-icon-cache\ngtk2\ngtk3\ngtk4\ngtkmm3\ngts\nguile\ngupnp\ngupnp-igd\ngvfs\ngvfs-afc\ngvfs-goa\ngvfs-google\ngzip\nharfbuzz\nharfbuzz-icu\nhashcat\nhcxtools\nhdf5\nhicolor-icon-theme\nhidapi\nhighway\nhtop\nhunspell\nhwdata\nhwloc\nhyphen\nhyprcursor\nhyprgraphics\nhypridle\nhyprland\nhyprland-qtutils\nhyprlang\nhyprlock\nhyprpaper\nhyprpicker\nhyprutils\nhyprwayland-scanner\ni2c-tools\niana-etc\nicu\nijs\nimath\nintel-gmmlib\nintel-media-driver\nintel-oneapi-common\nintel-oneapi-compiler-shared-runtime-libs\nintel-oneapi-openmp\nintel-oneapi-tcm\nintel-ucode\niproute2\niptables\niputils\niso-codes\niw\niwd\njansson\njava-environment-common\njava-runtime-common\njbig2dec\njbigkit\njdk-openjdk\njq\njson-c\njson-glib\njsoncpp\nkaccounts-integration\nkactivitymanagerd\nkarchive\nkate\nkauth\nkbd\nkbookmarks\nkcmutils\nkcodecs\nkcolorscheme\nkcompletion\nkconfig\nkconfigwidgets\nkcoreaddons\nkcrash\nkdbusaddons\nkde-cli-tools\nkde-gtk-config\nkdeclarative\nkdecoration\nkded\nkdeplasma-addons\nkdesu\nkdnssd\nkdsoap-qt6\nkdsoap-ws-discovery-client\nkeyutils\nkfilemetadata\nkgamma\nkglobalaccel\nkglobalacceld\nkguiaddons\nkholidays\nki18n\nkiconthemes\nkidletime\nkinfocenter\nkio\nkio-extras\nkio-fuse\nkirigami\nkirigami-addons\nkitemmodels\nkitemviews\nkitty\nkitty-shell-integration\nkitty-terminfo\nkjobwidgets\nkmenuedit\nkmod\nknewstuff\nknotifications\nknotifyconfig\nkonsole\nkpackage\nkparts\nkpipewire\nkpty\nkquickcharts\nkrb5\nkrdp\nkrunner\nkscreen\nkscreenlocker\nkservice\nksshaskpass\nkstatusnotifieritem\nksvg\nksystemstats\nktexteditor\nktextwidgets\nkunitconversion\nkuserfeedback\nkvantum\nkvantum-theme-otto-git\nkwallet\nkwallet-pam\nkwayland\nkwidgetsaddons\nkwin\nkwindowsystem\nkwrited\nkxmlgui\nl-smash\nlame\nlapack\nlayer-shell-qt\nlcms2\nldb\nlevel-zero-loader\nlibabw\nlibaccounts-glib\nlibaccounts-qt\nlibadwaita\nlibaec\nlibappindicator-gtk3\nlibarchive\nlibass\nlibassuan\nlibasyncns\nlibatasmart\nlibatomic_ops\nlibavc1394\nlibavif\nlibayatana-appindicator\nlibayatana-indicator\nlibb2\nlibblockdev\nlibblockdev-crypto\nlibblockdev-fs\nlibblockdev-loop\nlibblockdev-mdraid\nlibblockdev-nvme\nlibblockdev-part\nlibblockdev-swap\nlibbluray\nlibbpf\nlibbs2b\nlibbsd\nlibbytesize\nlibcanberra\nlibcap\nlibcap-ng\nlibcdio\nlibcdio-paranoia\nlibcdr\nlibcloudproviders\nlibcmis\nlibcolord\nlibcups\nlibcurl-gnutls\nlibdaemon\nlibdatrie\nlibdbusmenu-glib\nlibdbusmenu-gtk3\nlibdc1394\nlibdca\nlibde265\nlibdecor\nlibdeflate\nlibdisplay-info\nlibdmtx\nlibdovi\nlibdrm\nlibdvbpsi\nlibdvdnav\nlibdvdread\nlibe-book\nlibebml\nlibedit\nlibei\nlibelf\nlibepoxy\nlibepubgen\nlibetonyek\nlibevdev\nlibevent\nlibexif\nlibexttextcat\nlibfdk-aac\nlibffi\nlibfontenc\nlibfreeaptx\nlibfreehand\nlibgcrypt\nlibgdata\nlibgirepository\nlibglvnd\nlibgoa\nlibgpg-error\nlibgtop\nlibgudev\nlibgxps\nlibhandy\nlibheif\nlibice\nlibidn\nlibidn2\nlibiec61883\nlibimagequant\nlibimobiledevice\nlibimobiledevice-glue\nlibinih\nlibinput\nlibisl\nlibixion\nlibjpeg-turbo\nlibjxl\nlibkexiv2\nlibksba\nlibkscreen\nlibksysguard\nliblangtag\nliblc3\nlibldac\nlibldap\nlibliftoff\nlibluv\nliblzf\nlibmad\nlibmalcontent\nlibmatroska\nlibmbim\nlibmd\nlibmfx\nlibmm-glib\nlibmnl\nlibmodplug\nlibmpc\nlibmpcdec\nlibmpdclient\nlibmpeg2\nlibmspub\nlibmtp\nlibmwaw\nlibmysofa\nlibndp\nlibnet\nlibnetfilter_conntrack\nlibnewt\nlibnfnetlink\nlibnftnl\nlibnghttp2\nlibnghttp3\nlibngtcp2\nlibnice\nlibnl\nlibnm\nlibnma\nlibnma-common\nlibnotify\nlibnsl\nlibnumbertext\nlibnvme\nlibodfgen\nlibogg\nlibomxil-bellagio\nlibopenmpt\nliborcus\nlibp11-kit\nlibpagemaker\nlibpaper\nlibpcap\nlibpciaccess\nlibpeas\nlibpgm\nlibpipewire\nlibplacebo\nlibplasma\nlibplist\nlibpng\nlibproxy\nlibpsl\nlibpulse\nlibqaccessibilityclient-qt6\nlibqalculate\nlibqmi\nlibqrtr-glib\nlibqxp\nlibraqm\nlibraw1394\nlibreoffice-fresh\nlibrevenge\nlibrsvg\nlibrsync\nlibsamplerate\nlibsasl\nlibseccomp\nlibsecret\nlibsigc++\nlibsigsegv\nlibsixel\nlibsm\nlibsndfile\nlibsodium\nlibsoup\nlibsoup3\nlibsoxr\nlibspectre\nlibssh\nlibssh2\nlibstaroffice\nlibstemmer\nlibsynctex\nlibsysprof-capture\nlibtar\nlibtasn1\nlibteam\nlibthai\nlibtheora\nlibtiff\nlibtirpc\nlibtommath\nlibtool\nlibtpms\nlibunibreak\nlibunistring\nlibunwind\nlibupnp\nliburing\nlibusb\nlibusbmuxd\nlibutempter\nlibutf8proc\nlibuv\nlibva\nlibva-intel-driver\nlibvdpau\nlibverto\nlibvisio\nlibvlc\nlibvorbis\nlibvpl\nlibvpx\nlibvterm\nlibwacom\nlibwbclient\nlibwebp\nlibwireplumber\nlibwpd\nlibwps\nlibx11\nlibxau\nlibxaw\nlibxcb\nlibxcomposite\nlibxcrypt\nlibxcursor\nlibxcvt\nlibxdamage\nlibxdg-basedir\nlibxdmcp\nlibxext\nlibxfce4ui\nlibxfce4util\nlibxfixes\nlibxfont2\nlibxft\nlibxi\nlibxinerama\nlibxkbcommon\nlibxkbcommon-x11\nlibxkbfile\nlibxml2\nlibxmlb\nlibxmu\nlibxpm\nlibxpresent\nlibxrandr\nlibxrender\nlibxshmfence\nlibxslt\nlibxss\nlibxt\nlibxtst\nlibxv\nlibxxf86vm\nlibyaml\nlibyuv\nlibzip\nlibzmf\nlicenses\nlilv\nlinux\nlinux-api-headers\nlinux-firmware\nlinux-firmware-whence\nlinux-headers\nlinux-lts\nlinux-lts-headers\nllvm-libs\nlm_sensors\nlmdb\nlpsolve\nlsof\nlua\nlua51-lpeg\nluajit\nlv2\nlxappearance\nlz4\nlzo\nm4\nmailcap\nmake\nmd4c\nmdadm\nmedia-player-info\nmesa\nmesa-utils\nmeson\nmilou\nminizip\nmkinitcpio\nmkinitcpio-busybox\nmobile-broadband-provider-info\nmodemmanager\nmodemmanager-qt\nmpdecimal\nmpfr\nmpg123\nmpv\nmsgpack-c\nmtdev\nmujs\nnano\nncurses\nneofetch\nneon\nneovim\nnet-tools\nnetpbm\nnettle\nnetwork-manager-applet\nnetworkmanager\nnetworkmanager-qt\nninja\nnm-connection-editor\nnode-gyp\nnodejs\nnodejs-nopt\nnoto-fonts\nnoto-fonts-emoji\nnpm\nnpth\nnspr\nnss\nntfs-3g\nocean-sound-theme\nocl-icd\nonetbb\noniguruma\nopenal\nopencore-amr\nopencv\nopenexr\nopenjpeg2\nopenssh\nopenssl\nopenxr\nopus\norc\nos-prober\nostree\noxygen\noxygen-sounds\np11-kit\npacman\npacman-contrib\npacman-mirrorlist\npahole\npam\npambase\npango\npangomm\nparted\nparu\nparu-debug\npatch\npciutils\npcre\npcre2\npcsclite\nperl\nperl-error\nperl-mailtools\nperl-timedate\nphonon-qt6\nphonon-qt6-vlc\npinentry\npipewire\npipewire-alsa\npipewire-audio\npipewire-jack\npipewire-pulse\npixman\npkgconf\nplasma-activities\nplasma-activities-stats\nplasma-browser-integration\nplasma-desktop\nplasma-disks\nplasma-firewall\nplasma-integration\nplasma-meta\nplasma-nm\nplasma-pa\nplasma-systemmonitor\nplasma-thunderbolt\nplasma-vault\nplasma-welcome\nplasma-workspace\nplasma-workspace-wallpapers\nplasma5support\nplayerctl\nplymouth\nplymouth-kcm\npolkit\npolkit-kde-agent\npolkit-qt6\npoppler\npoppler-data\npoppler-glib\npoppler-qt6\npopt\nportaudio\npotrace\npowerdevil\nppp\nprint-manager\nprison\nprocps-ng\nprotobuf\npsmisc\npugixml\npulseaudio-qt\npurpose\npybind11\npython\npython-absl\npython-argcomplete\npython-astor\npython-astunparse\npython-attrs\npython-autocommand\npython-bleach\npython-cachetools\npython-cairo\npython-certifi\npython-cffi\npython-charset-normalizer\npython-click\npython-colorama\npython-contourpy\npython-cryptography\npython-cycler\npython-dateutil\npython-distro\npython-docopt\npython-fastjsonschema\npython-filelock\npython-flatbuffers\npython-fonttools\npython-gast\npython-gobject\npython-google-auth\npython-google-auth-oauthlib\npython-grpcio\npython-grpcio-tools\npython-h11\npython-h5py\npython-html5lib\npython-httplib2\npython-idna\npython-isodate\npython-jaraco.collections\npython-jaraco.context\npython-jaraco.functools\npython-jaraco.text\npython-joblib\npython-keras\npython-keyutils\npython-kiwisolver\npython-lxml\npython-mako\npython-markdown\npython-markdown-it-py\npython-markupsafe\npython-matplotlib\npython-mdurl\npython-ml-dtypes\npython-more-itertools\npython-numpy\npython-oauth2client\npython-oauthlib\npython-opt_einsum\npython-optree\npython-ordered-set\npython-outcome\npython-packaging\npython-pandas\npython-pasta\npython-pillow\npython-pip\npython-pipx\npython-platformdirs\npython-pooch\npython-protobuf\npython-psutil\npython-pyasn1\npython-pyasn1-modules\npython-pycountry\npython-pycparser\npython-pycryptodome\npython-pydot\npython-pygdbmi\npython-pygments\npython-pyparsing\npython-pysocks\npython-pytz\npython-requests\npython-requests-oauthlib\npython-rich\npython-rsa\npython-scikit-learn\npython-scipy\npython-sentry_sdk\npython-setuptools\npython-six\npython-sniffio\npython-sortedcontainers\npython-tensorboard_plugin_wit\npython-tensorflow\npython-tensorflow-estimator\npython-termcolor\npython-threadpoolctl\npython-tomli\npython-tqdm\npython-trio\npython-trio-websocket\npython-trove-classifiers\npython-typing_extensions\npython-uc-micro-py\npython-urllib3\npython-userpath\npython-validate-pyproject\npython-webencodings\npython-websocket-client\npython-werkzeug\npython-wheel\npython-wrapt\npython-wsproto\npython-yaml\nqca-qt6\nqcoro\nqhull\nqqc2-breeze-style\nqqc2-desktop-style\nqrencode\nqt5-base\nqt5-declarative\nqt5-svg\nqt5-translations\nqt5-wayland\nqt5-x11extras\nqt6-5compat\nqt6-base\nqt6-declarative\nqt6-multimedia\nqt6-multimedia-ffmpeg\nqt6-positioning\nqt6-quick3d\nqt6-quicktimeline\nqt6-scxml\nqt6-sensors\nqt6-shadertools\nqt6-speech\nqt6-svg\nqt6-tools\nqt6-translations\nqt6-virtualkeyboard\nqt6-wayland\nqt6-webchannel\nqt6-webengine\nqt6-websockets\nqt6-webview\nqt6ct\nqtkeychain-qt6\nranger\nraptor\nrasqal\nrav1e\nre2\nreadline\nredland\nreflector\nrhash\nripgrep\nripgrep-all\nrofi-lbonn-wayland-git\nrofi-lbonn-wayland-git-debug\nrtkit\nrubberband\nrust\nsamba\nsbc\nscdoc\nsdbus-cpp\nsddm\nsddm-kcm\nsdl12-compat\nsdl2\nseatd\nsed\nsemver\nserd\nserf\nshaderc\nshadow\nshared-mime-info\nsignon-kwallet-extension\nsignon-plugin-oauth2\nsignon-ui\nsignond\nslang\nslurp\nsmartmontools\nsmbclient\nsnappy\nsndio\nsocat\nsof-firmware\nsolid\nsonnet\nsord\nsound-theme-freedesktop\nsource-highlight\nspdlog\nspeex\nspeexdsp\nspirv-tools\nspotify\nsqlite\nsratom\nsrt\nstartup-notification\nstow\nstreamlink\nsubversion\nsudo\nsvt-av1\nsyndication\nsyntax-highlighting\nsystemd\nsystemd-libs\nsystemd-sysvcompat\nsystemsettings\ntaglib\ntalloc\ntar\ntdb\ntensorboard\ntensorflow\ntevent\ntexinfo\ntexlive-basic\ntexlive-bibtexextra\ntexlive-bin\ntexlive-binextra\ntexlive-context\ntexlive-fontsextra\ntexlive-fontsrecommended\ntexlive-fontutils\ntexlive-formatsextra\ntexlive-games\ntexlive-humanities\ntexlive-latex\ntexlive-latexextra\ntexlive-latexrecommended\ntexlive-luatex\ntexlive-mathscience\ntexlive-meta\ntexlive-metapost\ntexlive-music\ntexlive-pictures\ntexlive-plaingeneric\ntexlive-pstricks\ntexlive-publishers\ntexlive-xetex\nthunar\nthunar-archive-plugin\nthunar-media-tags-plugin\nthunar-shares-plugin\nthunar-vcs-plugin\nthunar-volman\ntinysparql\ntomlplusplus\ntpm2-tss\ntree-sitter\ntree-sitter-c\ntree-sitter-lua\ntree-sitter-markdown\ntree-sitter-query\ntree-sitter-vim\ntree-sitter-vimdoc\ntslib\nttf-dejavu\nttf-hack\nttf-jetbrains-mono-nerd\ntzdata\nuchardet\nudiskie\nudisks2\nunibilium\nunzip\nupower\nusbmuxd\nusbutils\nutil-linux\nutil-linux-libs\nv4l-utils\nvapoursynth\nverdict\nvid.stab\nvim\nvim-runtime\nvirtualbox\nvirtualbox-host-dkms\nvisual-studio-code-bin\nvisual-studio-code-bin-debug\nvlc\nvmaf\nvolume_key\nvulkan-headers\nvulkan-icd-loader\nvulkan-intel\nvulkan-radeon\nvulkan-tools\nvulkan-validation-layers\nwaybar\nwaybar-git-debug\nwayland\nwayland-protocols\nwayland-utils\nwebp-pixbuf-loader\nwebrtc-audio-processing-1\nwf-recorder\nwget\nwhich\nwhitesur-cursor-theme-git\nwhitesur-icon-theme\nwireless_tools\nwireplumber\nwl-clipboard\nwlogout\nwlogout-debug\nwlroots0.17\nwoff2\nwofi\nwpa_supplicant\nx264\nx265\nxcb-proto\nxcb-util\nxcb-util-cursor\nxcb-util-errors\nxcb-util-image\nxcb-util-keysyms\nxcb-util-renderutil\nxcb-util-wm\nxcb-util-xrm\nxdg-dbus-proxy\nxdg-desktop-portal\nxdg-desktop-portal-hyprland\n[godzeus@arch ~]$ pacman -Qe\naircrack-ng 1.7-4\nalsa-utils 1.2.13-2\nani-cli 4.9-1\nark 24.12.0-1\nbaobab 47.0-1\nbase 3-2\nbase-devel 1-2\nbash-completion 2.15.0-1\nblueman 2.4.3-1\nbluetui 0.6-1\nbluez-utils 5.79-1\nbrightnessctl 0.5.1-3\nbtop 1.4.0-4\nchafa 1.14.5-1\nclipse-bin 1.1.0-1\ncrunch 3.6-1\ndiscord 0.0.78-1\ndolphin 24.12.0.1-1\ndunst 1.12.0-1\nefibootmgr 18-3\negl-wayland 4:1.1.17-1\neog 47.0-1\nevince 46.3.1-2\nfd 10.2.0-1\nfirefox 133.0.3-2\nfish 3.7.1-2\nflatpak 1:1.15.12-1\ngit 2.47.1-1\ngithub-desktop-bin 3.4.8_linux1-1\ngnome-keyring 1:46.2-1\ngrim 1.4.1-2\ngrub 2:2.12-3\ngruvbox-dark-gtk 1.0.2-1\ngst-plugin-pipewire 1:1.2.7-1\ngtk-engine-murrine 0.98.2-4\ngvfs 1.56.1-1\ngvfs-afc 1.56.1-1\ngvfs-google 1.56.1-1\nhashcat 1:6.2.6-2\nhcxtools 6.3.4-1\nhtop 3.3.0-3\nhypridle 0.1.5-1\nhyprland 0.46.2-1\nhyprland-qtutils 0.1.1-3\nhyprlock 0.6.0-1\nhyprpaper 0.7.3-2\nhyprpicker 0.4.1-1\nintel-media-driver 24.4.4-1\nintel-ucode 20241112-1\niwd 3.3-1\njdk-openjdk 23.0.1.u0-1\njq 1.7.1-2\nkate 24.12.0-1\nkitty 0.37.0-1\nkonsole 24.12.0-1\nkvantum 1.1.3-1\nkvantum-theme-otto-git r25.f50ea46-1\nlibpulse 17.0+r43+g3e2bb8a1e-1\nlibreoffice-fresh 24.8.3-1\nlibva-intel-driver 2.4.1-3\nlinux 6.12.4.arch1-1\nlinux-firmware 20241210.b00a7f7e-1\nlinux-headers 6.12.4.arch1-1\nlinux-lts 6.6.65-1\nlinux-lts-headers 6.6.65-1\nlxappearance 0.6.3-5\nnano 8.2-1\nneofetch 7.1.0-2\nneovim 0.10.3-1\nnetwork-manager-applet 1.36.0-1\nnetworkmanager 1.50.0-1\nnodejs 23.4.0-1\nnpm 10.9.2-1\nntfs-3g 2022.10.3-1\nopencv 4.10.0-15\nos-prober 1.81-2\npacman-contrib 1.10.6-2\nparu 2.0.4-1\nparu-debug 2.0.4-1\npipewire 1:1.2.7-1\npipewire-alsa 1:1.2.7-1\npipewire-jack 1:1.2.7-1\npipewire-pulse 1:1.2.7-1\nplasma-meta 6.1-1\nplasma-workspace 6.2.4-1\nplymouth 24.004.60-9\nplymouth-kcm 6.2.4-1\npolkit-kde-agent 6.2.4-1\npython-matplotlib 3.9.3-1\npython-pandas 2.2.2-3\npython-pip 24.3.1-1\npython-pipx 1.7.1-1\npython-scikit-learn 1.6.0-1\npython-tensorflow 2.18.0-3\nqt5-wayland 5.15.16+kde+r59-3\nqt6-wayland 6.8.1-1\nqt6ct 0.9-12\nranger 1.9.4-1\nreflector 2023-2\nripgrep 14.1.1-1\nrofi-lbonn-wayland-git 1.7.5.wayland3.r52.g0abd8878-1\nslurp 1.5.0-1\nsmartmontools 7.4-2\nsof-firmware 2024.09.2-1\nspotify 1:1.2.50.335-1\nstow 2.4.1-1\nstreamlink 7.0.0-1\ntexlive-basic 2024.2-3\ntexlive-fontsextra 2024.2-3\ntexlive-latexextra 2024.2-3\ntexlive-meta 2024.2-3\nthunar 4.18.11-3\nthunar-archive-plugin 0.5.2-3\nthunar-media-tags-plugin 0.4.0-4\nthunar-shares-plugin 1:0.3.2-3\nthunar-vcs-plugin 0.2.0-3\nthunar-volman 4.18.0-4\nttf-dejavu 2.37+18+g9b5d1b2f-7\nttf-jetbrains-mono-nerd 3.3.0-1\nudiskie 2.5.3-1\nunzip 6.0-21\nvim 9.1.0866-1\nvirtualbox 7.1.4-2\nvisual-studio-code-bin 1.95.3-1\nvulkan-intel 1:24.3.1-3\nvulkan-radeon 1:24.3.1-3\nwaybar 0.11.0-4\nwf-recorder 0.5.0-2\nwget 1.25.0-1\nwhitesur-cursor-theme-git r11.63d04b8-1\nwhitesur-icon-theme 2024.09.07-1\nwireless_tools 30.pre9-4\nwireplumber 0.5.7-1\nwl-clipboard 1:2.2.1-2\nwlogout 1.2.2-0\nwofi 1.4.1-1\nxdg-desktop-portal-hyprland 1.3.9-1\nxdg-utils 1.2.1-1\nxf86-video-amdgpu 23.0.0-2\nxf86-video-ati 1:22.0.0-2\nxf86-video-nouveau 1.0.18-1\nxf86-video-vmware 13.4.0-3\nxorg-xinit 1.4.2-2\nyay 12.4.2-1\nyay-debug 12.4.2-1\nzathura 0.5.8-1\nzathura-pdf-poppler 0.3.3-1\nzen-browser-bin 1.0.2.b.0-1\nzram-generator 1.2.1-1\nzsh 5.9-5\n\n[godzeus@arch ~]$ pacman -Qd\na52dec 0.8.0-2\nabseil-cpp 20240722.0-1\naccounts-qml-module 0.7-6\naccountsservice 23.13.9-2\nacl 2.3.2-1\nadobe-source-code-pro-fonts 2.042u+1.062i+1.026vf-2\nadwaita-cursors 47.0-1\nadwaita-icon-theme 47.0-1\nadwaita-icon-theme-legacy 46.2-3\naha 0.5.1-3\nalsa-card-profiles 1:1.2.7-1\nalsa-lib 1.2.13-1\nalsa-topology-conf 1.2.5.1-4\nalsa-ucm-conf 1.2.13-2\naom 3.11.0-1\nappstream 1.0.4-1\nappstream-qt 1.0.4-1\napr 1.7.5-3\napr-util 1.6.3-2\naquamarine 0.5.1-1\narchlinux-appstream-data 20241214-1\narchlinux-keyring 20241203-1\nargon2 20190702-6\naria2 1.37.0-1\naribb24 1.0.3-4\nat-spi2-core 2.54.0-2\natkmm 2.28.4-1\nattica 6.9.0-1\nattr 2.5.2-1\naudit 4.0.2-2\nautoconf 2.72-1\nautomake 1.17-1\navahi 1:0.8+r194+g3f79789-2\nayatana-ido 0.10.4-1\nbaloo 6.9.0-1\nbaloo-widgets 24.12.0-1\nbash 5.2.037-1\nbinutils 2.43+r4+g7999dae6961-1\nbison 3.8.2-8\nblas 3.12.0-5\nbluedevil 1:6.2.4-1\nbluetui-debug 0.5.1-1\nbluez 5.79-1\nbluez-libs 5.79-1\nbluez-qt 6.9.0-1\nbolt 0.9.8-1\nboost-libs 1.86.0-3\nbreeze 6.2.4-1\nbreeze-gtk 6.2.4-1\nbreeze-icons 6.9.0-1\nbrotli 1.1.0-2\nbubblewrap 0.11.0-1\nbzip2 1.0.8-6\nc-ares 1.34.3-1\nca-certificates 20240618-1\nca-certificates-mozilla 3.107-1\nca-certificates-utils 20240618-1\ncairo 1.18.2-2\ncairomm 1.14.5-1\ncantarell-fonts 1:0.303.1-2\ncatch2 3.7.1-1\ncblas 3.12.0-5\ncdparanoia 10.2-9\ncheck 0.15.2-2\nchrono-date 3.0.3-1\ncifs-utils 7.1-1\nclinfo 3.0.23.01.25-1\nclucene 2.3.3.4-15\ncmake 3.31.3-1\ncomposefs 1.0.4-1\nconvertlit 1.8-12\ncoreutils 9.5-2\ncppdap 1.58.0-2\ncrunch-debug 3.6-1\ncryptsetup 2.7.5-1\ncurl 8.11.1-3\ndav1d 1.5.0-1\ndb5.3 5.3.28-5\ndbus 1.14.10-2\ndbus-broker 36-4\ndbus-broker-units 36-4\ndbus-glib 0.112-4\ndbus-units 36-4\ndconf 0.40.0-3\nddcutil 2.1.4-2\ndebugedit 5.1-1\ndefault-cursors 3-1\ndesktop-file-utils 0.28-1\ndevice-mapper 2.03.29-1\ndiffutils 3.10-1\ndiscount 3.0.0.d-1\ndiscover 6.2.4-1\ndjvulibre 3.5.28-6\ndkms 3.1.3-1\ndmidecode 3.6-1\ndouble-conversion 3.3.0-2\ndrkonqi 6.2.4-1\nduktape 2.7.0-7\ndvisvgm 3.4.2-1\ne2fsprogs 1.47.1-4\nebook-tools 0.2.2-8\neditorconfig-core-c 0.12.9-1\nefivar 39-1\neglexternalplatform 1.2-2\nell 0.71-1\nenchant 2.8.2-1\nethtool 1:6.9-1\nexempi 2.6.5-1\nexiv2 0.28.3-1\nexo 4.18.0-4\nexpat 2.6.4-1\nfaad2 2.11.1-1\nfakeroot 1.36-1\nffcall 2.4-3\nffmpeg 2:7.1-4\nffmpeg4.4 4.4.5-1\nfftw 3.3.10-7\nfile 5.46-2\nfilesystem 2024.11.21-1\nfindutils 4.10.0-2\nflac 1.4.3-2\nflex 2.6.4-5\nfmt 11.0.2-1\nfontconfig 2:2.15.0-2\nframeworkintegration 6.9.0-1\nfreerdp2 2.11.7-2\nfreetype2 2.13.3-1\nfribidi 1.0.16-1\nfuse-common 3.16.2-1\nfuse2 2.9.9-5\nfuse3 3.16.2-1\nfzf 0.56.3-1\ngawk 5.3.1-1\ngc 8.2.8-2\ngcc 14.2.1+r134+gab884fffe3fc-1\ngcc-libs 14.2.1+r134+gab884fffe3fc-1\ngcr 3.41.2-2\ngcr-4 4.3.0-1\ngd 2.3.3-8\ngdb 15.2-3\ngdb-common 15.2-3\ngdbm 1.24-1\ngdk-pixbuf2 2.42.12-2\ngeoclue 2.7.2-1\ngettext 0.22.5-2\nghostscript 10.04.0-1\ngiflib 5.2.2-1\ngirara 0.4.4-1\nglib-networking 1:2.80.0-3\nglib2 2.82.4-1\nglib2-devel 2.82.4-1\nglibc 2.40+r16+gaa533d58ff-2\nglibmm 2.66.7-1\nglslang 15.0.0-2\nglu 9.0.3-2\ngmp 6.3.0-2\ngnome-desktop 1:44.1-1\ngnome-desktop-common 1:44.1-1\ngnu-free-fonts 20120503-8\ngnupg 2.4.7-1\ngnutls 3.8.8-1\ngo 2:1.23.4-1\ngobject-introspection 1.82.0-1\ngobject-introspection-runtime 1.82.0-1\ngperftools 2.16-1\ngpgme 1.24.1-1\ngpm 1.20.7.r38.ge82d1a6-6\ngraphene 1.10.8-2\ngraphite 1:1.3.14-4\ngraphviz 12.1.2-1\ngrep 3.11-1\ngroff 1.23.0-7\ngsettings-desktop-schemas 47.1-1\ngsettings-system-schemas 47.1-1\ngsfonts 20200910-4\ngsm 1.0.22-2\ngspell 1.14.0-1\ngssdp 1.6.3-2\ngst-plugins-bad-libs 1.24.10-1\ngst-plugins-base 1.24.10-1\ngst-plugins-base-libs 1.24.10-1\ngstreamer 1.24.10-1\ngtest 1.15.2-1\ngtk-layer-shell 0.9.0-1\ngtk-update-icon-cache 1:4.16.7-1\ngtk2 2.24.33-5\ngtk3 1:3.24.43-4\ngtk4 1:4.16.7-1\ngtkmm3 3.24.9-1\ngts 0.7.6.121130-3\nguile 3.0.10-1\ngupnp 1:1.6.7-1\ngupnp-igd 1.6.0-1\ngvfs-goa 1.56.1-1\ngzip 1.13-4\nharfbuzz 10.1.0-1\nharfbuzz-icu 10.1.0-1\nhdf5 1.14.5-1\nhicolor-icon-theme 0.18-1\nhidapi 0.14.0-3\nhighway 1.2.0-1\nhunspell 1.7.2-2\nhwdata 0.390-1\nhwloc 2.11.2-1\nhyphen 2.8.8-6\nhyprcursor 0.1.10-1\nhyprgraphics 0.1.1-1\nhyprlang 0.6.0-1\nhyprutils 0.2.6-1\nhyprwayland-scanner 0.4.2-1\ni2c-tools 4.4-1\niana-etc 20241206-1\nicu 75.1-1\nijs 0.35-6\nimath 3.1.12-1\nintel-gmmlib 22.5.4-1\nintel-oneapi-common 2024.1.0-1\nintel-oneapi-compiler-shared-runtime-libs 2024.1.0-1\nintel-oneapi-openmp 2024.1.0-1\nintel-oneapi-tcm 1.0.1-3\niproute2 6.12.0-1\niptables 1:1.8.10-2\niputils 20240905-1\niso-codes 4.17.0-1\niw 6.9-1\njansson 2.14-4\njava-environment-common 3-5\njava-runtime-common 3-5\njbig2dec 0.20-1\njbigkit 2.1-8\njson-c 0.18-1\njson-glib 1.10.6-1\njsoncpp 1.9.6-3\nkaccounts-integration 24.12.0-1\nkactivitymanagerd 6.2.4-1\nkarchive 6.9.0-1\nkauth 6.9.0-1\nkbd 2.7.1-1\nkbookmarks 6.9.0-1\nkcmutils 6.9.0-1\nkcodecs 6.9.0-1\nkcolorscheme 6.9.0-1\nkcompletion 6.9.0-1\nkconfig 6.9.0-1\nkconfigwidgets 6.9.0-1\nkcoreaddons 6.9.0-2\nkcrash 6.9.0-1\nkdbusaddons 6.9.0-1\nkde-cli-tools 6.2.4-1\nkde-gtk-config 6.2.4-1\nkdeclarative 6.9.0-1\nkdecoration 6.2.4-1\nkded 6.9.0-1\nkdeplasma-addons 6.2.4-1\nkdesu 6.9.0-1\nkdnssd 6.9.0-1\nkdsoap-qt6 2.2.0-1\nkdsoap-ws-discovery-client 0.4.0-1\nkeyutils 1.6.3-3\nkfilemetadata 6.9.0-1\nkgamma 6.2.4-1\nkglobalaccel 6.9.0-1\nkglobalacceld 6.2.4-1\nkguiaddons 6.9.0-2\nkholidays 1:6.9.0-1\nki18n 6.9.0-1\nkiconthemes 6.9.0-1\nkidletime 6.9.0-1\nkinfocenter 6.2.4-1\nkio 6.9.0-1\nkio-extras 24.12.0-1\nkio-fuse 5.1.0-3\nkirigami 6.9.0-1\nkirigami-addons 1.6.0-1\nkitemmodels 6.9.0-1\nkitemviews 6.9.0-1\nkitty-shell-integration 0.37.0-1\nkitty-terminfo 0.37.0-1\nkjobwidgets 6.9.0-1\nkmenuedit 6.2.4-1\nkmod 33-3\nknewstuff 6.9.0-1\nknotifications 6.9.0-2\nknotifyconfig 6.9.0-1\nkpackage 6.9.0-1\nkparts 6.9.0-1\nkpipewire 6.2.4-1\nkpty 6.9.0-1\nkquickcharts 6.9.0-1\nkrb5 1.21.3-1\nkrdp 6.2.4-1\nkrunner 6.9.0-1\nkscreen 6.2.4-1\nkscreenlocker 6.2.4-1\nkservice 6.9.0-1\nksshaskpass 6.2.4-1\nkstatusnotifieritem 6.9.0-1\nksvg 6.9.0-1\nksystemstats 6.2.4-1\nktexteditor 6.9.0-1\nktextwidgets 6.9.0-1\nkunitconversion 6.9.0-2\nkuserfeedback 6.9.0-1\nkwallet 6.9.0-1\nkwallet-pam 6.2.4-1\nkwayland 6.2.4-1\nkwidgetsaddons 6.9.0-2\nkwin 6.2.4-2\nkwindowsystem 6.9.0-1\nkwrited 6.2.4-1\nkxmlgui 6.9.0-2\nl-smash 2.14.5-4\nlame 3.100-5\nlapack 3.12.0-5\nlayer-shell-qt 6.2.4-2\nlcms2 2.16-1\nldb 2:4.21.2-1\nlevel-zero-loader 1.18.5-2\nlibabw 0.1.3-4\nlibaccounts-glib 1.27-2\nlibaccounts-qt 1.17-1\nlibadwaita 1:1.6.2-1\nlibaec 1.1.3-1\nlibappindicator-gtk3 12.10.0.r298-4\nlibarchive 3.7.7-1\nlibass 0.17.3-1\nlibassuan 3.0.0-1\nlibasyncns 1:0.8+r3+g68cd5af-3\nlibatasmart 0.19-6\nlibatomic_ops 7.8.2-1\nlibavc1394 0.5.4-6\nlibavif 1.1.1-2\nlibayatana-appindicator 0.5.93-1\nlibayatana-indicator 0.9.4-1\nlibb2 0.98.1-3\nlibblockdev 3.2.1-2\nlibblockdev-crypto 3.2.1-2\nlibblockdev-fs 3.2.1-2\nlibblockdev-loop 3.2.1-2\nlibblockdev-mdraid 3.2.1-2\nlibblockdev-nvme 3.2.1-2\nlibblockdev-part 3.2.1-2\nlibblockdev-swap 3.2.1-2\nlibbluray 1.3.4-2\nlibbpf 1.5.0-1\nlibbs2b 3.1.0-9\nlibbsd 0.12.2-2\nlibbytesize 2.8-3\nlibcanberra 1:0.30+r2+gc0620e4-4\nlibcap 2.71-1\nlibcap-ng 0.8.5-2\nlibcdio 2.1.0-4\nlibcdio-paranoia 10.2+2.0.2-1\nlibcdr 0.1.8-1\nlibcloudproviders 0.3.6-1\nlibcmis 0.6.2-2\nlibcolord 1.4.7-2\nlibcups 2:2.4.11-1\nlibcurl-gnutls 8.11.1-3\nlibdaemon 0.14-6\nlibdatrie 0.2.13-4\nlibdbusmenu-glib 16.04.0.r498-2\nlibdbusmenu-gtk3 16.04.0.r498-2\nlibdc1394 2.2.7-1\nlibdca 0.0.7-2\nlibde265 1.0.15-3\nlibdecor 0.2.2-1\nlibdeflate 1.22-1\nlibdisplay-info 0.2.0-2\nlibdmtx 0.7.7-2\nlibdovi 3.3.1-1\nlibdrm 2.4.124-1\nlibdvbpsi 1:1.3.3-3\nlibdvdnav 6.1.1-2\nlibdvdread 6.1.3-2\nlibe-book 0.1.3-16\nlibebml 1.4.5-1\nlibedit 20240517_3.1-1\nlibei 1.3.0-1\nlibelf 0.192-2\nlibepoxy 1.5.10-3\nlibepubgen 0.1.1-5\nlibetonyek 0.1.12-1\nlibevdev 1.13.3-1\nlibevent 2.1.12-4\nlibexif 0.6.24-3\nlibexttextcat 3.4.7-1\nlibfdk-aac 2.0.3-1\nlibffi 3.4.6-1\nlibfontenc 1.1.8-1\nlibfreeaptx 0.1.1-2\nlibfreehand 0.1.2-5\nlibgcrypt 1.11.0-2\nlibgdata 0.18.1-3\nlibgirepository 1.82.0-1\nlibglvnd 1.7.0-1\nlibgoa 3.52.2-1\nlibgpg-error 1.51-1\nlibgtop 2.41.3-2\nlibgudev 238-1\nlibgxps 0.3.2-5\nlibhandy 1.8.3-2\nlibheif 1.19.5-1\nlibice 1.1.2-1\nlibidn 1.42-1\nlibidn2 2.3.7-1\nlibiec61883 1.2.0-8\nlibimagequant 4.3.3-1\nlibimobiledevice 1.3.0-14\nlibimobiledevice-glue 1.3.1-1\nlibinih 58-1\nlibinput 1.27.0-1\nlibisl 0.27-1\nlibixion 0.19.0-3\nlibjpeg-turbo 3.0.4-1\nlibjxl 0.11.1-1\nlibkexiv2 24.12.0-1\nlibksba 1.6.7-1\nlibkscreen 6.2.4-1\nlibksysguard 6.2.4-1\nliblangtag 0.6.7-1\nliblc3 1.1.1-1\nlibldac 2.0.2.3-2\nlibldap 2.6.9-1\nlibliftoff 0.5.0-1\nlibluv 1.48.0_2-1\nliblzf 3.6-5\nlibmad 0.15.1b-10\nlibmalcontent 0.13.0-1\nlibmatroska 1.7.1-2\nlibmbim 1.30.0-1\nlibmd 1.1.0-2\nlibmfx 23.2.2-3\nlibmm-glib 1.22.0-1\nlibmnl 1.0.5-2\nlibmodplug 0.8.9.0-6\nlibmpc 1.3.1-2\nlibmpcdec 1:0.1+r475-6\nlibmpdclient 2.22-1\nlibmpeg2 0.5.1-10\nlibmspub 0.1.4-16\nlibmtp 1.1.22-1\nlibmwaw 0.3.22-3\nlibmysofa 1.3.3-1\nlibndp 1.9-1\nlibnet 2:1.3-1\nlibnetfilter_conntrack 1.0.9-2\nlibnewt 0.52.24-2\nlibnfnetlink 1.0.2-2\nlibnftnl 1.2.8-1\nlibnghttp2 1.64.0-1\nlibnghttp3 1.6.0-1\nlibngtcp2 1.10.0-1\nlibnice 0.1.22-1\nlibnl 3.11.0-1\nlibnm 1.50.0-1\nlibnma 1.10.6-3\nlibnma-common 1.10.6-3\nlibnotify 0.8.3-1\nlibnsl 2.0.1-1\nlibnumbertext 1.0.11-2\nlibnvme 1.11.1-1\nlibodfgen 0.1.8-3\nlibogg 1.3.5-2\nlibomxil-bellagio 0.9.3-5\nlibopenmpt 0.7.11-1\nliborcus 0.19.2-3\nlibp11-kit 0.25.5-1\nlibpagemaker 0.0.4-4\nlibpaper 2.2.5-1\nlibpcap 1.10.5-2\nlibpciaccess 0.18.1-2\nlibpeas 1.36.0-5\nlibpgm 5.3.128-3\nlibpipewire 1:1.2.7-1\nlibplacebo 7.349.0-3\nlibplasma 6.2.4-1\nlibplist 2.6.0-1\nlibpng 1.6.44-1\nlibproxy 0.5.9-1\nlibpsl 0.21.5-2\nlibqaccessibilityclient-qt6 0.6.0-1\nlibqalculate 5.4.0.1-1\nlibqmi 1.34.0-2\nlibqrtr-glib 1.2.2-3\nlibqxp 0.0.2-12\nlibraqm 0.10.2-1\nlibraw1394 2.1.2-4\nlibrevenge 0.0.5-3\nlibrsvg 2:2.59.2-1\nlibrsync 1:2.3.4-2\nlibsamplerate 0.2.2-3\nlibsasl 2.1.28-5\nlibseccomp 2.5.5-3\nlibsecret 0.21.4-1\nlibsigc++ 2.12.1-1\nlibsigsegv 2.14-3\nlibsixel 1.10.3-7\nlibsm 1.2.5-1\nlibsndfile 1.2.2-2\nlibsodium 1.0.20-1\nlibsoup 2.74.3-1\nlibsoup3 3.6.1-1\nlibsoxr 0.1.3-4\nlibspectre 0.2.12-2\nlibssh 0.11.1-1\nlibssh2 1.11.0-1\nlibstaroffice 0.0.7-3\nlibstemmer 2.2.0-2\nlibsynctex 2024.2-5\nlibsysprof-capture 47.2-1\nlibtar 1.2.20-7\nlibtasn1 4.19.0-2\nlibteam 1.32-2\nlibthai 0.1.29-3\nlibtheora 1.1.1-6\nlibtiff 4.7.0-1\nlibtirpc 1.3.6-1\nlibtommath 1.3.0-1\nlibtool 2.5.4+r1+gbaa1fe41-1\nlibtpms 0.10.0-1\nlibunibreak 6.1-1\nlibunistring 1.2-1\nlibunwind 1.8.1-3\nlibupnp 1.14.20-1\nliburing 2.8-1\nlibusb 1.0.27-1\nlibusbmuxd 2.1.0-1\nlibutempter 1.2.1-4\nlibutf8proc 2.9.0-1\nlibuv 1.49.2-1\nlibva 2.22.0-1\nlibvdpau 1.5-3\nlibverto 0.3.2-5\nlibvisio 0.1.8-1\nlibvlc 3.0.21-9\nlibvorbis 1.3.7-3\nlibvpl 2.14.0-1\nlibvpx 1.15.0-1\nlibvterm 0.3.3-2\nlibwacom 2.14.0-1\nlibwbclient 2:4.21.2-1\nlibwebp 1.4.0-3\nlibwireplumber 0.5.7-1\nlibwpd 0.10.3-5\nlibwps 0.4.14-2\nlibx11 1.8.10-1\nlibxau 1.0.12-1\nlibxaw 1.0.16-1\nlibxcb 1.17.0-1\nlibxcomposite 0.4.6-2\nlibxcrypt 4.4.36-2\nlibxcursor 1.2.3-1\nlibxcvt 0.1.3-1\nlibxdamage 1.1.6-2\nlibxdg-basedir 1.2.3-2\nlibxdmcp 1.1.5-1\nlibxext 1.3.6-1\nlibxfce4ui 4.18.6-3\nlibxfce4util 4.18.2-3\nlibxfixes 6.0.1-2\nlibxfont2 2.0.7-1\nlibxft 2.3.8-2\nlibxi 1.8.2-1\nlibxinerama 1.1.5-2\nlibxkbcommon 1.7.0-2\nlibxkbcommon-x11 1.7.0-2\nlibxkbfile 1.1.3-1\nlibxml2 2.13.5-1\nlibxmlb 0.3.21-1\nlibxmu 1.2.1-1\nlibxpm 3.5.17-2\nlibxpresent 1.0.1-2\nlibxrandr 1.5.4-1\nlibxrender 0.9.12-1\nlibxshmfence 1.3.3-1\nlibxslt 1.1.42-1\nlibxss 1.2.4-2\nlibxt 1.3.1-1\nlibxtst 1.2.5-1\nlibxv 1.0.13-1\nlibxxf86vm 1.1.6-1\nlibyaml 0.2.5-3\nlibyuv r2426+464c51a03-1\nlibzip 1.11.2-1\nlibzmf 0.0.2-16\nlicenses 20240728-1\nlilv 0.24.24-2\nlinux-api-headers 6.10-1\nlinux-firmware-whence 20241210.b00a7f7e-1\nllvm-libs 18.1.8-4\nlm_sensors 1:3.6.0.r41.g31d1f125-3\nlmdb 0.9.33-1\nlpsolve 5.5.2.11-3\nlsof 4.99.4-1\nlua 5.4.7-1\nlua51-lpeg 1.1.0-3\nluajit 2.1.1731601260-1\nlv2 1.18.10-1\nlz4 1:1.10.0-2\nlzo 2.10-5\nm4 1.4.19-3\nmailcap 2.1.54-2\nmake 4.4.1-2\nmd4c 0.5.2-1\nmdadm 4.3-2\nmedia-player-info 26-1\nmesa 1:24.3.1-3\nmesa-utils 9.0.0-5\nmeson 1.6.0-4\nmilou 6.2.4-1\nminizip 1:1.3.1-2\nmkinitcpio 39.2-3\nmkinitcpio-busybox 1.36.1-1\nmobile-broadband-provider-info 20240407-1\nmodemmanager 1.22.0-1\nmodemmanager-qt 6.9.0-1\nmpdecimal 4.0.0-2\nmpfr 4.2.1-4\nmpg123 1.32.9-1\nmpv 1:0.39.0-4\nmsgpack-c 6.1.0-2\nmtdev 1.1.7-1\nmujs 1.3.5-1\nncurses 6.5-3\nneon 0.34.0-1\nnet-tools 2.10-3\nnetpbm 10.86.43-1\nnettle 3.10-1\nnetworkmanager-qt 6.9.0-1\nninja 1.12.1-1\nnm-connection-editor 1.36.0-1\nnode-gyp 11.0.0-1\nnodejs-nopt 7.2.1-1\nnoto-fonts 1:2024.12.01-1\nnoto-fonts-emoji 1:2.047-1\nnpth 1.8-1\nnspr 4.36-1\nnss 3.107-1\nocean-sound-theme 6.2.4-1\nocl-icd 2.3.2-2\nonetbb 2022.0.0-1\noniguruma 6.9.9-1\nopenal 1.24.1-1\nopencore-amr 0.1.6-2\nopenexr 3.3.2-1\nopenjpeg2 2.5.3-1\nopenssh 9.9p1-2\nopenssl 3.4.0-1\nopenxr 1.1.38-2\nopus 1.5.2-1\norc 0.4.40-1\nostree 2024.10-1\noxygen 6.2.4-1\noxygen-sounds 6.2.4-1\np11-kit 0.25.5-1\npacman 7.0.0.r6.gc685ae6-1\npacman-mirrorlist 20240717-1\npahole 1:1.27-2\npam 1.7.0-1\npambase 20230918-2\npango 1:1.54.0-1\npangomm 2.46.4-1\nparted 3.6-2\npatch 2.7.6-10\npciutils 3.13.0-2\npcre 8.45-4\npcre2 10.44-1\npcsclite 2.3.0-1\nperl 5.40.0-1\nperl-error 0.17029-7\nperl-mailtools 2.22-1\nperl-timedate 2.33-7\nphonon-qt6 4.12.0-4\nphonon-qt6-vlc 0.12.0-2\npinentry 1.3.1-5\npipewire-audio 1:1.2.7-1\npixman 0.44.2-1\npkgconf 2.3.0-1\nplasma-activities 6.2.4-1\nplasma-activities-stats 6.2.4-1\nplasma-browser-integration 6.2.4-1\nplasma-desktop 6.2.4-1\nplasma-disks 6.2.4-1\nplasma-firewall 6.2.4-1\nplasma-integration 6.2.4-1\nplasma-nm 6.2.4-1\nplasma-pa 6.2.4-1\nplasma-systemmonitor 6.2.4-1\nplasma-thunderbolt 6.2.4-1\nplasma-vault 6.2.4-1\nplasma-welcome 6.2.4-1\nplasma-workspace-wallpapers 6.2.4-1\nplasma5support 6.2.4-1\nplayerctl 2.4.1-4\npolkit 125-1\npolkit-qt6 0.200.0-1\npoppler 24.11.0-2\npoppler-data 0.4.12-2\npoppler-glib 24.11.0-2\npoppler-qt6 24.11.0-2\npopt 1.19-2\nportaudio 1:19.7.0-3\npotrace 1.16-3\npowerdevil 6.2.4-1\nppp 2.5.1-1\nprint-manager 1:6.2.4-1\nprison 6.9.0-1\nprocps-ng 4.0.4-3\nprotobuf 28.3-1\npsmisc 23.7-1\npugixml 1.14-1\npulseaudio-qt 1.6.1-1\npurpose 6.9.0-1\npybind11 2.13.6-1\npython 3.12.7-1\npython-absl 2.1.0-2\npython-argcomplete 3.4.0-1\npython-astor 0.8.1-7\npython-astunparse 1.6.3-8\npython-attrs 23.2.0-3\npython-autocommand 2.2.2-6\npython-bleach 6.1.0-2\npython-cachetools 5.5.0-1\npython-cairo 1.27.0-1\npython-certifi 2024.08.30-1\npython-cffi 1.17.1-1\npython-charset-normalizer 3.4.0-1\npython-click 8.1.7-3\npython-colorama 0.4.6-3\npython-contourpy 1.3.1-1\npython-cryptography 43.0.3-1\npython-cycler 0.12.1-2\npython-dateutil 2.9.0-5\npython-distro 1.9.0-2\npython-docopt 0.6.2-13\npython-fastjsonschema 2.20.0-1\npython-filelock 3.16.1-1\npython-flatbuffers 24.3.25-2\npython-fonttools 4.55.3-1\npython-gast 0.6.0-1\npython-gobject 3.50.0-1\npython-google-auth 2.36.1-1\npython-google-auth-oauthlib 1.2.1-1\npython-grpcio 1.67.1-1\npython-grpcio-tools 1.67.1-1\npython-h11 0.14.0-4\npython-h5py 3.12.1-1\npython-html5lib 1.1-14\npython-httplib2 0.22.0-7\npython-idna 3.10-1\npython-isodate 0.6.1-4\npython-jaraco.collections 5.0.1-1\npython-jaraco.context 5.3.0-1\npython-jaraco.functools 4.0.2-1\npython-jaraco.text 4.0.0-1\npython-joblib 1.4.2-1\npython-keras 3.4.1-1\npython-keyutils 0.6-10\npython-kiwisolver 1.4.5-3\npython-lxml 5.3.0-1\npython-mako 1.3.6-1\npython-markdown 3.7-1\npython-markdown-it-py 3.0.0-2\npython-markupsafe 2.1.5-2\npython-mdurl 0.1.2-5\npython-ml-dtypes 0.5.0-1\npython-more-itertools 10.3.0-1\npython-numpy 2.2.0-1\npython-oauth2client 4.1.3-10\npython-oauthlib 3.2.2-3\npython-opt_einsum 3.3.0-8\npython-optree 0.13.1-1\npython-ordered-set 4.1.0-5\npython-outcome 1.3.0.post0-4\npython-packaging 24.2-1\npython-pasta 0.2.0-8\npython-pillow 11.0.0-1\npython-platformdirs 4.3.6-1\npython-pooch 1.8.2-3\npython-protobuf 28.3-1\npython-psutil 6.1.0-1\npython-pyasn1 0.6.0-1\npython-pyasn1-modules 0.4.0-1\npython-pycountry 24.6.1-2\npython-pycparser 2.22-2\npython-pycryptodome 3.21.0-1\npython-pydot 3.0.1-2\npython-pygdbmi 0.11.0.0-4\npython-pygments 2.18.0-1\npython-pyparsing 3.1.2-2\npython-pysocks 1.7.1-9\npython-pytz 2024.2-1\npython-requests 2.32.3-1\npython-requests-oauthlib 1.3.1-4\npython-rich 13.9.4-1\npython-rsa 4.9-4\npython-scipy 1.14.1-1\npython-sentry_sdk 2.19.2-1\npython-setuptools 1:75.2.0-2\npython-six 1.16.0-9\npython-sniffio 1.3.1-3\npython-sortedcontainers 2.4.0-6\npython-tensorboard_plugin_wit 1.8.1-8\npython-tensorflow-estimator 2.15.0-2\npython-termcolor 2.4.0-2\npython-threadpoolctl 3.5.0-1\npython-tomli 2.0.1-4\npython-tqdm 4.67.1-1\npython-trio 0.26.2-1\npython-trio-websocket 0.11.1-3\npython-trove-classifiers 2024.10.21.16-1\npython-typing_extensions 4.12.2-1\npython-uc-micro-py 1.0.3-2\npython-urllib3 1.26.20-3\npython-userpath 1.9.2-2\npython-validate-pyproject 0.22-1\npython-webencodings 0.5.1-11\npython-websocket-client 1.8.0-1\npython-werkzeug 3.0.4-1\npython-wheel 0.45.0-1\npython-wrapt 1.16.0-3\npython-wsproto 1.2.0-4\npython-yaml 6.0.2-1\nqca-qt6 2.3.9-3\nqcoro 0.11.0-1\nqhull 2020.2-5\nqqc2-breeze-style 6.2.4-1\nqqc2-desktop-style 6.9.0-1\nqrencode 4.1.1-3\nqt5-base 5.15.16+kde+r130-3\nqt5-declarative 5.15.16+kde+r22-3\nqt5-svg 5.15.16+kde+r5-3\nqt5-translations 5.15.16-3\nqt5-x11extras 5.15.16-3\nqt6-5compat 6.8.1-1\nqt6-base 6.8.1-1\nqt6-declarative 6.8.1-1\nqt6-multimedia 6.8.1-2\nqt6-multimedia-ffmpeg 6.8.1-2\nqt6-positioning 6.8.1-1\nqt6-quick3d 6.8.1-1\nqt6-quicktimeline 6.8.1-1\nqt6-scxml 6.8.1-1\nqt6-sensors 6.8.1-1\nqt6-shadertools 6.8.1-1\nqt6-speech 6.8.1-1\nqt6-svg 6.8.1-1\nqt6-tools 6.8.1-1\nqt6-translations 6.8.1-1\nqt6-virtualkeyboard 6.8.1-1\nqt6-webchannel 6.8.1-1\nqt6-webengine 6.8.1-1\nqt6-websockets 6.8.1-1\nqt6-webview 6.8.1-1\nqtkeychain-qt6 0.14.3-1\nraptor 2.0.16-5\nrasqal 1:0.9.33-7\nrav1e 0.7.1-1\nre2 1:20240702-2\nreadline 8.2.013-1\nredland 1:1.0.17-9\nrhash 1.4.4-1\nripgrep-all 0.10.6-3\nrofi-lbonn-wayland-git-debug 1.7.5.wayland3.r52.g0abd8878-1\nrtkit 0.13-3\nrubberband 4.0.0-1\nrust 1:1.83.0-1\nsamba 2:4.21.2-1\nsbc 2.0-2\nscdoc 1.11.3-1\nsdbus-cpp 2.1.0-2\nsddm 0.21.0-4\nsddm-kcm 6.2.4-1\nsdl12-compat 1.2.68-2\nsdl2 2.30.10-1\nseatd 0.9.1-1\nsed 4.9-3\nsemver 7.6.3-1\nserd 0.32.2-1\nserf 1.3.10-2\nshaderc 2024.3-2\nshadow 4.16.0-1\nshared-mime-info 2.4-1\nsignon-kwallet-extension 24.12.0-1\nsignon-plugin-oauth2 0.25-3\nsignon-ui 0.17+20231016-3\nsignond 8.61-3\nslang 2.3.3-3\nsmbclient 2:4.21.2-1\nsnappy 1.2.1-2\nsndio 1.10.0-1\nsocat 1.8.0.1-1\nsolid 6.9.1-2\nsonnet 6.9.0-1\nsord 0.16.16-1\nsound-theme-freedesktop 0.8-6\nsource-highlight 3.1.9-13\nspdlog 1.15.0-1\nspeex 1.2.1-2\nspeexdsp 1.2.1-2\nspirv-tools 2024.4.rc1-1\nsqlite 3.47.2-1\nsratom 0.6.16-1\nsrt 1.5.4-1\nstartup-notification 0.12-8\nsubversion 1.14.4-1\nsudo 1.9.16.p2-1\nsvt-av1 2.2.1-1\nsyndication 6.9.0-1\nsyntax-highlighting 6.9.0-1\nsystemd 257-1\nsystemd-libs 257-1\nsystemd-sysvcompat 257-1\nsystemsettings 6.2.4-1\ntaglib 2.0.2-1\ntalloc 2.4.2-3\ntar 1.35-2\ntdb 1.4.12-1\ntensorboard 2.18.0-1\ntensorflow 2.18.0-3\ntevent 1:0.16.1-3\ntexinfo 7.1.1-1\ntexlive-bibtexextra 2024.2-3\ntexlive-bin 2024.2-5\ntexlive-binextra 2024.2-3\ntexlive-context 2024.2-3\ntexlive-fontsrecommended 2024.2-3\ntexlive-fontutils 2024.2-3\ntexlive-formatsextra 2024.2-3\ntexlive-games 2024.2-3\ntexlive-humanities 2024.2-3\ntexlive-latex 2024.2-3\ntexlive-latexrecommended 2024.2-3\ntexlive-luatex 2024.2-3\ntexlive-mathscience 2024.2-3\ntexlive-metapost 2024.2-3\ntexlive-music 2024.2-3\ntexlive-pictures 2024.2-3\ntexlive-plaingeneric 2024.2-3\ntexlive-pstricks 2024.2-3\ntexlive-publishers 2024.2-3\ntexlive-xetex 2024.2-3\ntinysparql 3.8.2-2\ntomlplusplus 3.4.0-1\ntpm2-tss 4.1.3-1\ntree-sitter 0.24.3-1\ntree-sitter-c 0.23.1-1\ntree-sitter-lua 0.2.0-1\ntree-sitter-markdown 0.3.1-1\ntree-sitter-query 0.4.0-1\ntree-sitter-vim 0.4.0-1\ntree-sitter-vimdoc 3.0.0-1\ntslib 1.23-1\nttf-hack 3.003-7\ntzdata 2024b-2\nuchardet 0.0.8-3\nudisks2 2.10.1-5\nunibilium 2.1.1-2\nupower 1.90.6-1\nusbmuxd 1.1.1-4\nusbutils 018-1\nutil-linux 2.40.2-1\nutil-linux-libs 2.40.2-1\nv4l-utils 1.28.1-1\nvapoursynth R70-1\nverdict 1.4.2-1\nvid.stab 1.1.1-2\nvim-runtime 9.1.0866-1\nvirtualbox-host-dkms 7.1.4-2\nvisual-studio-code-bin-debug 1.95.3-1\nvlc 3.0.21-9\nvmaf 3.0.0-1\nvolume_key 0.3.12-9\nvulkan-headers 1:1.4.303-1\nvulkan-icd-loader 1.4.303-1\nvulkan-tools 1.4.303-2\nvulkan-validation-layers 1.3.296.0-1\nwaybar-git-debug r3722.20ca48c3-1\nwayland 1.23.1-1\nwayland-protocols 1.39-1\nwayland-utils 1.2.0-2\nwebp-pixbuf-loader 0.2.7-1\nwebrtc-audio-processing-1 1.3-3\nwhich 2.21-6\nwlogout-debug 1.2.2-0\nwlroots0.17 0.17.4-3\nwoff2 1.0.2-5\nwpa_supplicant 2:2.11-2\nx264 3:0.164.r3108.31e19f9-2\nx265 4.0-1\nxcb-proto 1.17.0-2\nxcb-util 0.4.1-2\nxcb-util-cursor 0.1.5-1\nxcb-util-errors 1.0.1-2\nxcb-util-image 0.4.1-3\nxcb-util-keysyms 0.4.1-5\nxcb-util-renderutil 0.3.10-2\nxcb-util-wm 0.4.2-2\nxcb-util-xrm 1.3-3\nxdg-dbus-proxy 0.1.6-1\nxdg-desktop-portal 1.18.4-2\nxdg-desktop-portal-kde 6.2.4-1\nxdg-user-dirs 0.18-2\nxf86-input-libinput 1.5.0-1\nxfconf 4.18.3-3\nxkeyboard-config 2.43-1\nxmlsec 1.3.6-1\nxorg-fonts-encodings 1.1.0-1\nxorg-server 21.1.15-1\nxorg-server-common 21.1.15-1\nxorg-setxkbmap 1.3.4-2\nxorg-xauth 1.1.3-1\nxorg-xdpyinfo 1.3.4-2\nxorg-xinput 1.6.4-2\nxorg-xkbcomp 1.4.7-1\nxorg-xmessage 1.0.7-1\nxorg-xmodmap 1.0.11-2\nxorg-xprop 1.2.8-1\nxorg-xrandr 1.5.3-1\nxorg-xrdb 1.2.2-2\nxorg-xset 1.2.5-2\nxorg-xsetroot 1.1.3-2\nxorg-xwayland 24.1.4-1\nxorgproto 2024.1-2\nxsettingsd 1.0.2-2\nxvidcore 1.3.7-3\nxxhash 0.8.2-1\nxz 5.6.3-1\nzeromq 4.3.5-2\nzimg 3.0.5-1\nzix 0.4.2-2\nzlib 1:1.3.1-2\nzstd 1.5.6-1\nzxing-cpp 2.2.1-1\nzziplib 0.13.78-1","recorded":"2024-12-23 15:40:16.882334890","filePath":"null","pinned":false},{"value":"paru -Qm\n","recorded":"2024-12-23 15:13:58.157562141","filePath":"null","pinned":false},{"value":"[godzeus@arch ~]$ pacman -Qd","recorded":"2024-12-23 15:12:17.300566323","filePath":"null","pinned":false},{"value":"a52dec 0.8.0-2\nabseil-cpp 20240722.0-1\naccounts-qml-module 0.7-6\naccountsservice 23.13.9-2\nacl 2.3.2-1\nadobe-source-code-pro-fonts 2.042u+1.062i+1.026vf-2\nadwaita-cursors 47.0-1\nadwaita-icon-theme 47.0-1\nadwaita-icon-theme-legacy 46.2-3\naha 0.5.1-3\nalsa-card-profiles 1:1.2.7-1\nalsa-lib 1.2.13-1\nalsa-topology-conf 1.2.5.1-4\nalsa-ucm-conf 1.2.13-2\naom 3.11.0-1\nappstream 1.0.4-1\nappstream-qt 1.0.4-1\napr 1.7.5-3\napr-util 1.6.3-2\naquamarine 0.5.1-1\narchlinux-appstream-data 20241214-1\narchlinux-keyring 20241203-1\nargon2 20190702-6\naria2 1.37.0-1\naribb24 1.0.3-4\nat-spi2-core 2.54.0-2\natkmm 2.28.4-1\nattica 6.9.0-1\nattr 2.5.2-1\naudit 4.0.2-2\nautoconf 2.72-1\nautomake 1.17-1\navahi 1:0.8+r194+g3f79789-2\nayatana-ido 0.10.4-1\nbaloo 6.9.0-1\nbaloo-widgets 24.12.0-1\nbash 5.2.037-1\nbinutils 2.43+r4+g7999dae6961-1\nbison 3.8.2-8\nblas 3.12.0-5\nbluedevil 1:6.2.4-1\nbluetui-debug 0.5.1-1\nbluez 5.79-1\nbluez-libs 5.79-1\nbluez-qt 6.9.0-1\nbolt 0.9.8-1\nboost-libs 1.86.0-3\nbreeze 6.2.4-1\nbreeze-gtk 6.2.4-1\nbreeze-icons 6.9.0-1\nbrotli 1.1.0-2\nbubblewrap 0.11.0-1\nbzip2 1.0.8-6\nc-ares 1.34.3-1\nca-certificates 20240618-1\nca-certificates-mozilla 3.107-1\nca-certificates-utils 20240618-1\ncairo 1.18.2-2\ncairomm 1.14.5-1\ncantarell-fonts 1:0.303.1-2\ncatch2 3.7.1-1\ncblas 3.12.0-5\ncdparanoia 10.2-9\ncheck 0.15.2-2\nchrono-date 3.0.3-1\ncifs-utils 7.1-1\nclinfo 3.0.23.01.25-1\nclucene 2.3.3.4-15\ncmake 3.31.3-1\ncomposefs 1.0.4-1\nconvertlit 1.8-12\ncoreutils 9.5-2\ncppdap 1.58.0-2\ncrunch-debug 3.6-1\ncryptsetup 2.7.5-1\ncurl 8.11.1-3\ndav1d 1.5.0-1\ndb5.3 5.3.28-5\ndbus 1.14.10-2\ndbus-broker 36-4\ndbus-broker-units 36-4\ndbus-glib 0.112-4\ndbus-units 36-4\ndconf 0.40.0-3\nddcutil 2.1.4-2\ndebugedit 5.1-1\ndefault-cursors 3-1\ndesktop-file-utils 0.28-1\ndevice-mapper 2.03.29-1\ndiffutils 3.10-1\ndiscount 3.0.0.d-1\ndiscover 6.2.4-1\ndjvulibre 3.5.28-6\ndkms 3.1.3-1\ndmidecode 3.6-1\ndouble-conversion 3.3.0-2\ndrkonqi 6.2.4-1\nduktape 2.7.0-7\ndvisvgm 3.4.2-1\ne2fsprogs 1.47.1-4\nebook-tools 0.2.2-8\neditorconfig-core-c 0.12.9-1\nefivar 39-1\neglexternalplatform 1.2-2\nell 0.71-1\nenchant 2.8.2-1\nethtool 1:6.9-1\nexempi 2.6.5-1\nexiv2 0.28.3-1\nexo 4.18.0-4\nexpat 2.6.4-1\nfaad2 2.11.1-1\nfakeroot 1.36-1\nffcall 2.4-3\nffmpeg 2:7.1-4\nffmpeg4.4 4.4.5-1\nfftw 3.3.10-7\nfile 5.46-2\nfilesystem 2024.11.21-1\nfindutils 4.10.0-2\nflac 1.4.3-2\nflex 2.6.4-5\nfmt 11.0.2-1\nfontconfig 2:2.15.0-2\nframeworkintegration 6.9.0-1\nfreerdp2 2.11.7-2\nfreetype2 2.13.3-1\nfribidi 1.0.16-1\nfuse-common 3.16.2-1\nfuse2 2.9.9-5\nfuse3 3.16.2-1\nfzf 0.56.3-1\ngawk 5.3.1-1\ngc 8.2.8-2\ngcc 14.2.1+r134+gab884fffe3fc-1\ngcc-libs 14.2.1+r134+gab884fffe3fc-1\ngcr 3.41.2-2\ngcr-4 4.3.0-1\ngd 2.3.3-8\ngdb 15.2-3\ngdb-common 15.2-3\ngdbm 1.24-1\ngdk-pixbuf2 2.42.12-2\ngeoclue 2.7.2-1\ngettext 0.22.5-2\nghostscript 10.04.0-1\ngiflib 5.2.2-1\ngirara 0.4.4-1\nglib-networking 1:2.80.0-3\nglib2 2.82.4-1\nglib2-devel 2.82.4-1\nglibc 2.40+r16+gaa533d58ff-2\nglibmm 2.66.7-1\nglslang 15.0.0-2\nglu 9.0.3-2\ngmp 6.3.0-2\ngnome-desktop 1:44.1-1\ngnome-desktop-common 1:44.1-1\ngnu-free-fonts 20120503-8\ngnupg 2.4.7-1\ngnutls 3.8.8-1\ngo 2:1.23.4-1\ngobject-introspection 1.82.0-1\ngobject-introspection-runtime 1.82.0-1\ngperftools 2.16-1\ngpgme 1.24.1-1\ngpm 1.20.7.r38.ge82d1a6-6\ngraphene 1.10.8-2\ngraphite 1:1.3.14-4\ngraphviz 12.1.2-1\ngrep 3.11-1\ngroff 1.23.0-7\ngsettings-desktop-schemas 47.1-1\ngsettings-system-schemas 47.1-1\ngsfonts 20200910-4\ngsm 1.0.22-2\ngspell 1.14.0-1\ngssdp 1.6.3-2\ngst-plugins-bad-libs 1.24.10-1\ngst-plugins-base 1.24.10-1\ngst-plugins-base-libs 1.24.10-1\ngstreamer 1.24.10-1\ngtest 1.15.2-1\ngtk-layer-shell 0.9.0-1\ngtk-update-icon-cache 1:4.16.7-1\ngtk2 2.24.33-5\ngtk3 1:3.24.43-4\ngtk4 1:4.16.7-1\ngtkmm3 3.24.9-1\ngts 0.7.6.121130-3\nguile 3.0.10-1\ngupnp 1:1.6.7-1\ngupnp-igd 1.6.0-1\ngvfs-goa 1.56.1-1\ngzip 1.13-4\nharfbuzz 10.1.0-1\nharfbuzz-icu 10.1.0-1\nhdf5 1.14.5-1\nhicolor-icon-theme 0.18-1\nhidapi 0.14.0-3\nhighway 1.2.0-1\nhunspell 1.7.2-2\nhwdata 0.390-1\nhwloc 2.11.2-1\nhyphen 2.8.8-6\nhyprcursor 0.1.10-1\nhyprgraphics 0.1.1-1\nhyprlang 0.6.0-1\nhyprutils 0.2.6-1\nhyprwayland-scanner 0.4.2-1\ni2c-tools 4.4-1\niana-etc 20241206-1\nicu 75.1-1\nijs 0.35-6\nimath 3.1.12-1\nintel-gmmlib 22.5.4-1\nintel-oneapi-common 2024.1.0-1\nintel-oneapi-compiler-shared-runtime-libs 2024.1.0-1\nintel-oneapi-openmp 2024.1.0-1\nintel-oneapi-tcm 1.0.1-3\niproute2 6.12.0-1\niptables 1:1.8.10-2\niputils 20240905-1\niso-codes 4.17.0-1\niw 6.9-1\njansson 2.14-4\njava-environment-common 3-5\njava-runtime-common 3-5\njbig2dec 0.20-1\njbigkit 2.1-8\njson-c 0.18-1\njson-glib 1.10.6-1\njsoncpp 1.9.6-3\nkaccounts-integration 24.12.0-1\nkactivitymanagerd 6.2.4-1\nkarchive 6.9.0-1\nkauth 6.9.0-1\nkbd 2.7.1-1\nkbookmarks 6.9.0-1\nkcmutils 6.9.0-1\nkcodecs 6.9.0-1\nkcolorscheme 6.9.0-1\nkcompletion 6.9.0-1\nkconfig 6.9.0-1\nkconfigwidgets 6.9.0-1\nkcoreaddons 6.9.0-2\nkcrash 6.9.0-1\nkdbusaddons 6.9.0-1\nkde-cli-tools 6.2.4-1\nkde-gtk-config 6.2.4-1\nkdeclarative 6.9.0-1\nkdecoration 6.2.4-1\nkded 6.9.0-1\nkdeplasma-addons 6.2.4-1\nkdesu 6.9.0-1\nkdnssd 6.9.0-1\nkdsoap-qt6 2.2.0-1\nkdsoap-ws-discovery-client 0.4.0-1\nkeyutils 1.6.3-3\nkfilemetadata 6.9.0-1\nkgamma 6.2.4-1\nkglobalaccel 6.9.0-1\nkglobalacceld 6.2.4-1\nkguiaddons 6.9.0-2\nkholidays 1:6.9.0-1\nki18n 6.9.0-1\nkiconthemes 6.9.0-1\nkidletime 6.9.0-1\nkinfocenter 6.2.4-1\nkio 6.9.0-1\nkio-extras 24.12.0-1\nkio-fuse 5.1.0-3\nkirigami 6.9.0-1\nkirigami-addons 1.6.0-1\nkitemmodels 6.9.0-1\nkitemviews 6.9.0-1\nkitty-shell-integration 0.37.0-1\nkitty-terminfo 0.37.0-1\nkjobwidgets 6.9.0-1\nkmenuedit 6.2.4-1\nkmod 33-3\nknewstuff 6.9.0-1\nknotifications 6.9.0-2\nknotifyconfig 6.9.0-1\nkpackage 6.9.0-1\nkparts 6.9.0-1\nkpipewire 6.2.4-1\nkpty 6.9.0-1\nkquickcharts 6.9.0-1\nkrb5 1.21.3-1\nkrdp 6.2.4-1\nkrunner 6.9.0-1\nkscreen 6.2.4-1\nkscreenlocker 6.2.4-1\nkservice 6.9.0-1\nksshaskpass 6.2.4-1\nkstatusnotifieritem 6.9.0-1\nksvg 6.9.0-1\nksystemstats 6.2.4-1\nktexteditor 6.9.0-1\nktextwidgets 6.9.0-1\nkunitconversion 6.9.0-2\nkuserfeedback 6.9.0-1\nkwallet 6.9.0-1\nkwallet-pam 6.2.4-1\nkwayland 6.2.4-1\nkwidgetsaddons 6.9.0-2\nkwin 6.2.4-2\nkwindowsystem 6.9.0-1\nkwrited 6.2.4-1\nkxmlgui 6.9.0-2\nl-smash 2.14.5-4\nlame 3.100-5\nlapack 3.12.0-5\nlayer-shell-qt 6.2.4-2\nlcms2 2.16-1\nldb 2:4.21.2-1\nlevel-zero-loader 1.18.5-2\nlibabw 0.1.3-4\nlibaccounts-glib 1.27-2\nlibaccounts-qt 1.17-1\nlibadwaita 1:1.6.2-1\nlibaec 1.1.3-1\nlibappindicator-gtk3 12.10.0.r298-4\nlibarchive 3.7.7-1\nlibass 0.17.3-1\nlibassuan 3.0.0-1\nlibasyncns 1:0.8+r3+g68cd5af-3\nlibatasmart 0.19-6\nlibatomic_ops 7.8.2-1\nlibavc1394 0.5.4-6\nlibavif 1.1.1-2\nlibayatana-appindicator 0.5.93-1\nlibayatana-indicator 0.9.4-1\nlibb2 0.98.1-3\nlibblockdev 3.2.1-2\nlibblockdev-crypto 3.2.1-2\nlibblockdev-fs 3.2.1-2\nlibblockdev-loop 3.2.1-2\nlibblockdev-mdraid 3.2.1-2\nlibblockdev-nvme 3.2.1-2\nlibblockdev-part 3.2.1-2\nlibblockdev-swap 3.2.1-2\nlibbluray 1.3.4-2\nlibbpf 1.5.0-1\nlibbs2b 3.1.0-9\nlibbsd 0.12.2-2\nlibbytesize 2.8-3\nlibcanberra 1:0.30+r2+gc0620e4-4\nlibcap 2.71-1\nlibcap-ng 0.8.5-2\nlibcdio 2.1.0-4\nlibcdio-paranoia 10.2+2.0.2-1\nlibcdr 0.1.8-1\nlibcloudproviders 0.3.6-1\nlibcmis 0.6.2-2\nlibcolord 1.4.7-2\nlibcups 2:2.4.11-1\nlibcurl-gnutls 8.11.1-3\nlibdaemon 0.14-6\nlibdatrie 0.2.13-4\nlibdbusmenu-glib 16.04.0.r498-2\nlibdbusmenu-gtk3 16.04.0.r498-2\nlibdc1394 2.2.7-1\nlibdca 0.0.7-2\nlibde265 1.0.15-3\nlibdecor 0.2.2-1\nlibdeflate 1.22-1\nlibdisplay-info 0.2.0-2\nlibdmtx 0.7.7-2\nlibdovi 3.3.1-1\nlibdrm 2.4.124-1\nlibdvbpsi 1:1.3.3-3\nlibdvdnav 6.1.1-2\nlibdvdread 6.1.3-2\nlibe-book 0.1.3-16\nlibebml 1.4.5-1\nlibedit 20240517_3.1-1\nlibei 1.3.0-1\nlibelf 0.192-2\nlibepoxy 1.5.10-3\nlibepubgen 0.1.1-5\nlibetonyek 0.1.12-1\nlibevdev 1.13.3-1\nlibevent 2.1.12-4\nlibexif 0.6.24-3\nlibexttextcat 3.4.7-1\nlibfdk-aac 2.0.3-1\nlibffi 3.4.6-1\nlibfontenc 1.1.8-1\nlibfreeaptx 0.1.1-2\nlibfreehand 0.1.2-5\nlibgcrypt 1.11.0-2\nlibgdata 0.18.1-3\nlibgirepository 1.82.0-1\nlibglvnd 1.7.0-1\nlibgoa 3.52.2-1\nlibgpg-error 1.51-1\nlibgtop 2.41.3-2\nlibgudev 238-1\nlibgxps 0.3.2-5\nlibhandy 1.8.3-2\nlibheif 1.19.5-1\nlibice 1.1.2-1\nlibidn 1.42-1\nlibidn2 2.3.7-1\nlibiec61883 1.2.0-8\nlibimagequant 4.3.3-1\nlibimobiledevice 1.3.0-14\nlibimobiledevice-glue 1.3.1-1\nlibinih 58-1\nlibinput 1.27.0-1\nlibisl 0.27-1\nlibixion 0.19.0-3\nlibjpeg-turbo 3.0.4-1\nlibjxl 0.11.1-1\nlibkexiv2 24.12.0-1\nlibksba 1.6.7-1\nlibkscreen 6.2.4-1\nlibksysguard 6.2.4-1\nliblangtag 0.6.7-1\nliblc3 1.1.1-1\nlibldac 2.0.2.3-2\nlibldap 2.6.9-1\nlibliftoff 0.5.0-1\nlibluv 1.48.0_2-1\nliblzf 3.6-5\nlibmad 0.15.1b-10\nlibmalcontent 0.13.0-1\nlibmatroska 1.7.1-2\nlibmbim 1.30.0-1\nlibmd 1.1.0-2\nlibmfx 23.2.2-3\nlibmm-glib 1.22.0-1\nlibmnl 1.0.5-2\nlibmodplug 0.8.9.0-6\nlibmpc 1.3.1-2\nlibmpcdec 1:0.1+r475-6\nlibmpdclient 2.22-1\nlibmpeg2 0.5.1-10\nlibmspub 0.1.4-16\nlibmtp 1.1.22-1\nlibmwaw 0.3.22-3\nlibmysofa 1.3.3-1\nlibndp 1.9-1\nlibnet 2:1.3-1\nlibnetfilter_conntrack 1.0.9-2\nlibnewt 0.52.24-2\nlibnfnetlink 1.0.2-2\nlibnftnl 1.2.8-1\nlibnghttp2 1.64.0-1\nlibnghttp3 1.6.0-1\nlibngtcp2 1.10.0-1\nlibnice 0.1.22-1\nlibnl 3.11.0-1\nlibnm 1.50.0-1\nlibnma 1.10.6-3\nlibnma-common 1.10.6-3\nlibnotify 0.8.3-1\nlibnsl 2.0.1-1\nlibnumbertext 1.0.11-2\nlibnvme 1.11.1-1\nlibodfgen 0.1.8-3\nlibogg 1.3.5-2\nlibomxil-bellagio 0.9.3-5\nlibopenmpt 0.7.11-1\nliborcus 0.19.2-3\nlibp11-kit 0.25.5-1\nlibpagemaker 0.0.4-4\nlibpaper 2.2.5-1\nlibpcap 1.10.5-2\nlibpciaccess 0.18.1-2\nlibpeas 1.36.0-5\nlibpgm 5.3.128-3\nlibpipewire 1:1.2.7-1\nlibplacebo 7.349.0-3\nlibplasma 6.2.4-1\nlibplist 2.6.0-1\nlibpng 1.6.44-1\nlibproxy 0.5.9-1\nlibpsl 0.21.5-2\nlibqaccessibilityclient-qt6 0.6.0-1\nlibqalculate 5.4.0.1-1\nlibqmi 1.34.0-2\nlibqrtr-glib 1.2.2-3\nlibqxp 0.0.2-12\nlibraqm 0.10.2-1\nlibraw1394 2.1.2-4\nlibrevenge 0.0.5-3\nlibrsvg 2:2.59.2-1\nlibrsync 1:2.3.4-2\nlibsamplerate 0.2.2-3\nlibsasl 2.1.28-5\nlibseccomp 2.5.5-3\nlibsecret 0.21.4-1\nlibsigc++ 2.12.1-1\nlibsigsegv 2.14-3\nlibsixel 1.10.3-7\nlibsm 1.2.5-1\nlibsndfile 1.2.2-2\nlibsodium 1.0.20-1\nlibsoup 2.74.3-1\nlibsoup3 3.6.1-1\nlibsoxr 0.1.3-4\nlibspectre 0.2.12-2\nlibssh 0.11.1-1\nlibssh2 1.11.0-1\nlibstaroffice 0.0.7-3\nlibstemmer 2.2.0-2\nlibsynctex 2024.2-5\nlibsysprof-capture 47.2-1\nlibtar 1.2.20-7\nlibtasn1 4.19.0-2\nlibteam 1.32-2\nlibthai 0.1.29-3\nlibtheora 1.1.1-6\nlibtiff 4.7.0-1\nlibtirpc 1.3.6-1\nlibtommath 1.3.0-1\nlibtool 2.5.4+r1+gbaa1fe41-1\nlibtpms 0.10.0-1\nlibunibreak 6.1-1\nlibunistring 1.2-1\nlibunwind 1.8.1-3\nlibupnp 1.14.20-1\nliburing 2.8-1\nlibusb 1.0.27-1\nlibusbmuxd 2.1.0-1\nlibutempter 1.2.1-4\nlibutf8proc 2.9.0-1\nlibuv 1.49.2-1\nlibva 2.22.0-1\nlibvdpau 1.5-3\nlibverto 0.3.2-5\nlibvisio 0.1.8-1\nlibvlc 3.0.21-9\nlibvorbis 1.3.7-3\nlibvpl 2.14.0-1\nlibvpx 1.15.0-1\nlibvterm 0.3.3-2\nlibwacom 2.14.0-1\nlibwbclient 2:4.21.2-1\nlibwebp 1.4.0-3\nlibwireplumber 0.5.7-1\nlibwpd 0.10.3-5\nlibwps 0.4.14-2\nlibx11 1.8.10-1\nlibxau 1.0.12-1\nlibxaw 1.0.16-1\nlibxcb 1.17.0-1\nlibxcomposite 0.4.6-2\nlibxcrypt 4.4.36-2\nlibxcursor 1.2.3-1\nlibxcvt 0.1.3-1\nlibxdamage 1.1.6-2\nlibxdg-basedir 1.2.3-2\nlibxdmcp 1.1.5-1\nlibxext 1.3.6-1\nlibxfce4ui 4.18.6-3\nlibxfce4util 4.18.2-3\nlibxfixes 6.0.1-2\nlibxfont2 2.0.7-1\nlibxft 2.3.8-2\nlibxi 1.8.2-1\nlibxinerama 1.1.5-2\nlibxkbcommon 1.7.0-2\nlibxkbcommon-x11 1.7.0-2\nlibxkbfile 1.1.3-1\nlibxml2 2.13.5-1\nlibxmlb 0.3.21-1\nlibxmu 1.2.1-1\nlibxpm 3.5.17-2\nlibxpresent 1.0.1-2\nlibxrandr 1.5.4-1\nlibxrender 0.9.12-1\nlibxshmfence 1.3.3-1\nlibxslt 1.1.42-1\nlibxss 1.2.4-2\nlibxt 1.3.1-1\nlibxtst 1.2.5-1\nlibxv 1.0.13-1\nlibxxf86vm 1.1.6-1\nlibyaml 0.2.5-3\nlibyuv r2426+464c51a03-1\nlibzip 1.11.2-1\nlibzmf 0.0.2-16\nlicenses 20240728-1\nlilv 0.24.24-2\nlinux-api-headers 6.10-1\nlinux-firmware-whence 20241210.b00a7f7e-1\nllvm-libs 18.1.8-4\nlm_sensors 1:3.6.0.r41.g31d1f125-3\nlmdb 0.9.33-1\nlpsolve 5.5.2.11-3\nlsof 4.99.4-1\nlua 5.4.7-1\nlua51-lpeg 1.1.0-3\nluajit 2.1.1731601260-1\nlv2 1.18.10-1\nlz4 1:1.10.0-2\nlzo 2.10-5\nm4 1.4.19-3\nmailcap 2.1.54-2\nmake 4.4.1-2\nmd4c 0.5.2-1\nmdadm 4.3-2\nmedia-player-info 26-1\nmesa 1:24.3.1-3\nmesa-utils 9.0.0-5\nmeson 1.6.0-4\nmilou 6.2.4-1\nminizip 1:1.3.1-2\nmkinitcpio 39.2-3\nmkinitcpio-busybox 1.36.1-1\nmobile-broadband-provider-info 20240407-1\nmodemmanager 1.22.0-1\nmodemmanager-qt 6.9.0-1\nmpdecimal 4.0.0-2\nmpfr 4.2.1-4\nmpg123 1.32.9-1\nmpv 1:0.39.0-4\nmsgpack-c 6.1.0-2\nmtdev 1.1.7-1\nmujs 1.3.5-1\nncurses 6.5-3\nneon 0.34.0-1\nnet-tools 2.10-3\nnetpbm 10.86.43-1\nnettle 3.10-1\nnetworkmanager-qt 6.9.0-1\nninja 1.12.1-1\nnm-connection-editor 1.36.0-1\nnode-gyp 11.0.0-1\nnodejs-nopt 7.2.1-1\nnoto-fonts 1:2024.12.01-1\nnoto-fonts-emoji 1:2.047-1\nnpth 1.8-1\nnspr 4.36-1\nnss 3.107-1\nocean-sound-theme 6.2.4-1\nocl-icd 2.3.2-2\nonetbb 2022.0.0-1\noniguruma 6.9.9-1\nopenal 1.24.1-1\nopencore-amr 0.1.6-2\nopenexr 3.3.2-1\nopenjpeg2 2.5.3-1\nopenssh 9.9p1-2\nopenssl 3.4.0-1\nopenxr 1.1.38-2\nopus 1.5.2-1\norc 0.4.40-1\nostree 2024.10-1\noxygen 6.2.4-1\noxygen-sounds 6.2.4-1\np11-kit 0.25.5-1\npacman 7.0.0.r6.gc685ae6-1\npacman-mirrorlist 20240717-1\npahole 1:1.27-2\npam 1.7.0-1\npambase 20230918-2\npango 1:1.54.0-1\npangomm 2.46.4-1\nparted 3.6-2\npatch 2.7.6-10\npciutils 3.13.0-2\npcre 8.45-4\npcre2 10.44-1\npcsclite 2.3.0-1\nperl 5.40.0-1\nperl-error 0.17029-7\nperl-mailtools 2.22-1\nperl-timedate 2.33-7\nphonon-qt6 4.12.0-4\nphonon-qt6-vlc 0.12.0-2\npinentry 1.3.1-5\npipewire-audio 1:1.2.7-1\npixman 0.44.2-1\npkgconf 2.3.0-1\nplasma-activities 6.2.4-1\nplasma-activities-stats 6.2.4-1\nplasma-browser-integration 6.2.4-1\nplasma-desktop 6.2.4-1\nplasma-disks 6.2.4-1\nplasma-firewall 6.2.4-1\nplasma-integration 6.2.4-1\nplasma-nm 6.2.4-1\nplasma-pa 6.2.4-1\nplasma-systemmonitor 6.2.4-1\nplasma-thunderbolt 6.2.4-1\nplasma-vault 6.2.4-1\nplasma-welcome 6.2.4-1\nplasma-workspace-wallpapers 6.2.4-1\nplasma5support 6.2.4-1\nplayerctl 2.4.1-4\npolkit 125-1\npolkit-qt6 0.200.0-1\npoppler 24.11.0-2\npoppler-data 0.4.12-2\npoppler-glib 24.11.0-2\npoppler-qt6 24.11.0-2\npopt 1.19-2\nportaudio 1:19.7.0-3\npotrace 1.16-3\npowerdevil 6.2.4-1\nppp 2.5.1-1\nprint-manager 1:6.2.4-1\nprison 6.9.0-1\nprocps-ng 4.0.4-3\nprotobuf 28.3-1\npsmisc 23.7-1\npugixml 1.14-1\npulseaudio-qt 1.6.1-1\npurpose 6.9.0-1\npybind11 2.13.6-1\npython 3.12.7-1\npython-absl 2.1.0-2\npython-argcomplete 3.4.0-1\npython-astor 0.8.1-7\npython-astunparse 1.6.3-8\npython-attrs 23.2.0-3\npython-autocommand 2.2.2-6\npython-bleach 6.1.0-2\npython-cachetools 5.5.0-1\npython-cairo 1.27.0-1\npython-certifi 2024.08.30-1\npython-cffi 1.17.1-1\npython-charset-normalizer 3.4.0-1\npython-click 8.1.7-3\npython-colorama 0.4.6-3\npython-contourpy 1.3.1-1\npython-cryptography 43.0.3-1\npython-cycler 0.12.1-2\npython-dateutil 2.9.0-5\npython-distro 1.9.0-2\npython-docopt 0.6.2-13\npython-fastjsonschema 2.20.0-1\npython-filelock 3.16.1-1\npython-flatbuffers 24.3.25-2\npython-fonttools 4.55.3-1\npython-gast 0.6.0-1\npython-gobject 3.50.0-1\npython-google-auth 2.36.1-1\npython-google-auth-oauthlib 1.2.1-1\npython-grpcio 1.67.1-1\npython-grpcio-tools 1.67.1-1\npython-h11 0.14.0-4\npython-h5py 3.12.1-1\npython-html5lib 1.1-14\npython-httplib2 0.22.0-7\npython-idna 3.10-1\npython-isodate 0.6.1-4\npython-jaraco.collections 5.0.1-1\npython-jaraco.context 5.3.0-1\npython-jaraco.functools 4.0.2-1\npython-jaraco.text 4.0.0-1\npython-joblib 1.4.2-1\npython-keras 3.4.1-1\npython-keyutils 0.6-10\npython-kiwisolver 1.4.5-3\npython-lxml 5.3.0-1\npython-mako 1.3.6-1\npython-markdown 3.7-1\npython-markdown-it-py 3.0.0-2\npython-markupsafe 2.1.5-2\npython-mdurl 0.1.2-5\npython-ml-dtypes 0.5.0-1\npython-more-itertools 10.3.0-1\npython-numpy 2.2.0-1\npython-oauth2client 4.1.3-10\npython-oauthlib 3.2.2-3\npython-opt_einsum 3.3.0-8\npython-optree 0.13.1-1\npython-ordered-set 4.1.0-5\npython-outcome 1.3.0.post0-4\npython-packaging 24.2-1\npython-pasta 0.2.0-8\npython-pillow 11.0.0-1\npython-platformdirs 4.3.6-1\npython-pooch 1.8.2-3\npython-protobuf 28.3-1\npython-psutil 6.1.0-1\npython-pyasn1 0.6.0-1\npython-pyasn1-modules 0.4.0-1\npython-pycountry 24.6.1-2\npython-pycparser 2.22-2\npython-pycryptodome 3.21.0-1\npython-pydot 3.0.1-2\npython-pygdbmi 0.11.0.0-4\npython-pygments 2.18.0-1\npython-pyparsing 3.1.2-2\npython-pysocks 1.7.1-9\npython-pytz 2024.2-1\npython-requests 2.32.3-1\npython-requests-oauthlib 1.3.1-4\npython-rich 13.9.4-1\npython-rsa 4.9-4\npython-scipy 1.14.1-1\npython-sentry_sdk 2.19.2-1\npython-setuptools 1:75.2.0-2\npython-six 1.16.0-9\npython-sniffio 1.3.1-3\npython-sortedcontainers 2.4.0-6\npython-tensorboard_plugin_wit 1.8.1-8\npython-tensorflow-estimator 2.15.0-2\npython-termcolor 2.4.0-2\npython-threadpoolctl 3.5.0-1\npython-tomli 2.0.1-4\npython-tqdm 4.67.1-1\npython-trio 0.26.2-1\npython-trio-websocket 0.11.1-3\npython-trove-classifiers 2024.10.21.16-1\npython-typing_extensions 4.12.2-1\npython-uc-micro-py 1.0.3-2\npython-urllib3 1.26.20-3\npython-userpath 1.9.2-2\npython-validate-pyproject 0.22-1\npython-webencodings 0.5.1-11\npython-websocket-client 1.8.0-1\npython-werkzeug 3.0.4-1\npython-wheel 0.45.0-1\npython-wrapt 1.16.0-3\npython-wsproto 1.2.0-4\npython-yaml 6.0.2-1\nqca-qt6 2.3.9-3\nqcoro 0.11.0-1\nqhull 2020.2-5\nqqc2-breeze-style 6.2.4-1\nqqc2-desktop-style 6.9.0-1\nqrencode 4.1.1-3\nqt5-base 5.15.16+kde+r130-3\nqt5-declarative 5.15.16+kde+r22-3\nqt5-svg 5.15.16+kde+r5-3\nqt5-translations 5.15.16-3\nqt5-x11extras 5.15.16-3\nqt6-5compat 6.8.1-1\nqt6-base 6.8.1-1\nqt6-declarative 6.8.1-1\nqt6-multimedia 6.8.1-2\nqt6-multimedia-ffmpeg 6.8.1-2\nqt6-positioning 6.8.1-1\nqt6-quick3d 6.8.1-1\nqt6-quicktimeline 6.8.1-1\nqt6-scxml 6.8.1-1\nqt6-sensors 6.8.1-1\nqt6-shadertools 6.8.1-1\nqt6-speech 6.8.1-1\nqt6-svg 6.8.1-1\nqt6-tools 6.8.1-1\nqt6-translations 6.8.1-1\nqt6-virtualkeyboard 6.8.1-1\nqt6-webchannel 6.8.1-1\nqt6-webengine 6.8.1-1\nqt6-websockets 6.8.1-1\nqt6-webview 6.8.1-1\nqtkeychain-qt6 0.14.3-1\nraptor 2.0.16-5\nrasqal 1:0.9.33-7\nrav1e 0.7.1-1\nre2 1:20240702-2\nreadline 8.2.013-1\nredland 1:1.0.17-9\nrhash 1.4.4-1\nripgrep-all 0.10.6-3\nrofi-lbonn-wayland-git-debug 1.7.5.wayland3.r52.g0abd8878-1\nrtkit 0.13-3\nrubberband 4.0.0-1\nrust 1:1.83.0-1\nsamba 2:4.21.2-1\nsbc 2.0-2\nscdoc 1.11.3-1\nsdbus-cpp 2.1.0-2\nsddm 0.21.0-4\nsddm-kcm 6.2.4-1\nsdl12-compat 1.2.68-2\nsdl2 2.30.10-1\nseatd 0.9.1-1\nsed 4.9-3\nsemver 7.6.3-1\nserd 0.32.2-1\nserf 1.3.10-2\nshaderc 2024.3-2\nshadow 4.16.0-1\nshared-mime-info 2.4-1\nsignon-kwallet-extension 24.12.0-1\nsignon-plugin-oauth2 0.25-3\nsignon-ui 0.17+20231016-3\nsignond 8.61-3\nslang 2.3.3-3\nsmbclient 2:4.21.2-1\nsnappy 1.2.1-2\nsndio 1.10.0-1\nsocat 1.8.0.1-1\nsolid 6.9.1-2\nsonnet 6.9.0-1\nsord 0.16.16-1\nsound-theme-freedesktop 0.8-6\nsource-highlight 3.1.9-13\nspdlog 1.15.0-1\nspeex 1.2.1-2\nspeexdsp 1.2.1-2\nspirv-tools 2024.4.rc1-1\nsqlite 3.47.2-1\nsratom 0.6.16-1\nsrt 1.5.4-1\nstartup-notification 0.12-8\nsubversion 1.14.4-1\nsudo 1.9.16.p2-1\nsvt-av1 2.2.1-1\nsyndication 6.9.0-1\nsyntax-highlighting 6.9.0-1\nsystemd 257-1\nsystemd-libs 257-1\nsystemd-sysvcompat 257-1\nsystemsettings 6.2.4-1\ntaglib 2.0.2-1\ntalloc 2.4.2-3\ntar 1.35-2\ntdb 1.4.12-1\ntensorboard 2.18.0-1\ntensorflow 2.18.0-3\ntevent 1:0.16.1-3\ntexinfo 7.1.1-1\ntexlive-bibtexextra 2024.2-3\ntexlive-bin 2024.2-5\ntexlive-binextra 2024.2-3\ntexlive-context 2024.2-3\ntexlive-fontsrecommended 2024.2-3\ntexlive-fontutils 2024.2-3\ntexlive-formatsextra 2024.2-3\ntexlive-games 2024.2-3\ntexlive-humanities 2024.2-3\ntexlive-latex 2024.2-3\ntexlive-latexrecommended 2024.2-3\ntexlive-luatex 2024.2-3\ntexlive-mathscience 2024.2-3\ntexlive-metapost 2024.2-3\ntexlive-music 2024.2-3\ntexlive-pictures 2024.2-3\ntexlive-plaingeneric 2024.2-3\ntexlive-pstricks 2024.2-3\ntexlive-publishers 2024.2-3\ntexlive-xetex 2024.2-3\ntinysparql 3.8.2-2\ntomlplusplus 3.4.0-1\ntpm2-tss 4.1.3-1\ntree-sitter 0.24.3-1\ntree-sitter-c 0.23.1-1\ntree-sitter-lua 0.2.0-1\ntree-sitter-markdown 0.3.1-1\ntree-sitter-query 0.4.0-1\ntree-sitter-vim 0.4.0-1\ntree-sitter-vimdoc 3.0.0-1\ntslib 1.23-1\nttf-hack 3.003-7\ntzdata 2024b-2\nuchardet 0.0.8-3\nudisks2 2.10.1-5\nunibilium 2.1.1-2\nupower 1.90.6-1\nusbmuxd 1.1.1-4\nusbutils 018-1\nutil-linux 2.40.2-1\nutil-linux-libs 2.40.2-1\nv4l-utils 1.28.1-1\nvapoursynth R70-1\nverdict 1.4.2-1\nvid.stab 1.1.1-2\nvim-runtime 9.1.0866-1\nvirtualbox-host-dkms 7.1.4-2\nvisual-studio-code-bin-debug 1.95.3-1\nvlc 3.0.21-9\nvmaf 3.0.0-1\nvolume_key 0.3.12-9\nvulkan-headers 1:1.4.303-1\nvulkan-icd-loader 1.4.303-1\nvulkan-tools 1.4.303-2\nvulkan-validation-layers 1.3.296.0-1\nwaybar-git-debug r3722.20ca48c3-1\nwayland 1.23.1-1\nwayland-protocols 1.39-1\nwayland-utils 1.2.0-2\nwebp-pixbuf-loader 0.2.7-1\nwebrtc-audio-processing-1 1.3-3\nwhich 2.21-6\nwlogout-debug 1.2.2-0\nwlroots0.17 0.17.4-3\nwoff2 1.0.2-5\nwpa_supplicant 2:2.11-2\nx264 3:0.164.r3108.31e19f9-2\nx265 4.0-1\nxcb-proto 1.17.0-2\nxcb-util 0.4.1-2\nxcb-util-cursor 0.1.5-1\nxcb-util-errors 1.0.1-2\nxcb-util-image 0.4.1-3\nxcb-util-keysyms 0.4.1-5\nxcb-util-renderutil 0.3.10-2\nxcb-util-wm 0.4.2-2\nxcb-util-xrm 1.3-3\nxdg-dbus-proxy 0.1.6-1\nxdg-desktop-portal 1.18.4-2\nxdg-desktop-portal-kde 6.2.4-1\nxdg-user-dirs 0.18-2\nxf86-input-libinput 1.5.0-1\nxfconf 4.18.3-3\nxkeyboard-config 2.43-1\nxmlsec 1.3.6-1\nxorg-fonts-encodings 1.1.0-1\nxorg-server 21.1.15-1\nxorg-server-common 21.1.15-1\nxorg-setxkbmap 1.3.4-2\nxorg-xauth 1.1.3-1\nxorg-xdpyinfo 1.3.4-2\nxorg-xinput 1.6.4-2\nxorg-xkbcomp 1.4.7-1\nxorg-xmessage 1.0.7-1\nxorg-xmodmap 1.0.11-2\nxorg-xprop 1.2.8-1\nxorg-xrandr 1.5.3-1\nxorg-xrdb 1.2.2-2\nxorg-xset 1.2.5-2\nxorg-xsetroot 1.1.3-2\nxorg-xwayland 24.1.4-1\nxorgproto 2024.1-2\nxsettingsd 1.0.2-2\nxvidcore 1.3.7-3\nxxhash 0.8.2-1\nxz 5.6.3-1\nzeromq 4.3.5-2\nzimg 3.0.5-1\nzix 0.4.2-2\nzlib 1:1.3.1-2\nzstd 1.5.6-1\nzxing-cpp 2.2.1-1\nzziplib 0.13.78-1\n","recorded":"2024-12-23 15:11:56.275814934","filePath":"null","pinned":false},{"value":"[godzeus@arch ~]$ pacman -Qq\na52dec\nabseil-cpp\naccounts-qml-module\naccountsservice\nacl\nadobe-source-code-pro-fonts\nadwaita-cursors\nadwaita-icon-theme\nadwaita-icon-theme-legacy\naha\naircrack-ng\nalsa-card-profiles\nalsa-lib\nalsa-topology-conf\nalsa-ucm-conf\nalsa-utils\nani-cli\naom\nappstream\nappstream-qt\napr\napr-util\naquamarine\narchlinux-appstream-data\narchlinux-keyring\nargon2\naria2\naribb24\nark\nat-spi2-core\natkmm\nattica\nattr\naudit\nautoconf\nautomake\navahi\nayatana-ido\nbaloo\nbaloo-widgets\nbaobab\nbase\nbase-devel\nbash\nbash-completion\nbinutils\nbison\nblas\nbluedevil\nblueman\nbluetui\nbluetui-debug\nbluez\nbluez-libs\nbluez-qt\nbluez-utils\nbolt\nboost-libs\nbreeze\nbreeze-gtk\nbreeze-icons\nbrightnessctl\nbrotli\nbtop\nbubblewrap\nbzip2\nc-ares\nca-certificates\nca-certificates-mozilla\nca-certificates-utils\ncairo\ncairomm\ncantarell-fonts\ncatch2\ncblas\ncdparanoia\nchafa\ncheck\nchrono-date\ncifs-utils\nclinfo\nclipse-bin\nclucene\ncmake\ncomposefs\nconvertlit\ncoreutils\ncppdap\ncrunch\ncrunch-debug\ncryptsetup\ncurl\ndav1d\ndb5.3\ndbus\ndbus-broker\ndbus-broker-units\ndbus-glib\ndbus-units\ndconf\nddcutil\ndebugedit\ndefault-cursors\ndesktop-file-utils\ndevice-mapper\ndiffutils\ndiscord\ndiscount\ndiscover\ndjvulibre\ndkms\ndmidecode\ndolphin\ndouble-conversion\ndrkonqi\nduktape\ndunst\ndvisvgm\ne2fsprogs\nebook-tools\neditorconfig-core-c\nefibootmgr\nefivar\negl-wayland\neglexternalplatform\nell\nenchant\neog\nethtool\nevince\nexempi\nexiv2\nexo\nexpat\nfaad2\nfakeroot\nfd\nffcall\nffmpeg\nffmpeg4.4\nfftw\nfile\nfilesystem\nfindutils\nfirefox\nfish\nflac\nflatpak\nflex\nfmt\nfontconfig\nframeworkintegration\nfreerdp2\nfreetype2\nfribidi\nfuse-common\nfuse2\nfuse3\nfzf\ngawk\ngc\ngcc\ngcc-libs\ngcr\ngcr-4\ngd\ngdb\ngdb-common\ngdbm\ngdk-pixbuf2\ngeoclue\ngettext\nghostscript\ngiflib\ngirara\ngit\ngithub-desktop-bin\nglib-networking\nglib2\nglib2-devel\nglibc\nglibmm\nglslang\nglu\ngmp\ngnome-desktop\ngnome-desktop-common\ngnome-keyring\ngnu-free-fonts\ngnupg\ngnutls\ngo\ngobject-introspection\ngobject-introspection-runtime\ngperftools\ngpgme\ngpm\ngraphene\ngraphite\ngraphviz\ngrep\ngrim\ngroff\ngrub\ngruvbox-dark-gtk\ngsettings-desktop-schemas\ngsettings-system-schemas\ngsfonts\ngsm\ngspell\ngssdp\ngst-plugin-pipewire\ngst-plugins-bad-libs\ngst-plugins-base\ngst-plugins-base-libs\ngstreamer\ngtest\ngtk-engine-murrine\ngtk-layer-shell\ngtk-update-icon-cache\ngtk2\ngtk3\ngtk4\ngtkmm3\ngts\nguile\ngupnp\ngupnp-igd\ngvfs\ngvfs-afc\ngvfs-goa\ngvfs-google\ngzip\nharfbuzz\nharfbuzz-icu\nhashcat\nhcxtools\nhdf5\nhicolor-icon-theme\nhidapi\nhighway\nhtop\nhunspell\nhwdata\nhwloc\nhyphen\nhyprcursor\nhyprgraphics\nhypridle\nhyprland\nhyprland-qtutils\nhyprlang\nhyprlock\nhyprpaper\nhyprpicker\nhyprutils\nhyprwayland-scanner\ni2c-tools\niana-etc\nicu\nijs\nimath\nintel-gmmlib\nintel-media-driver\nintel-oneapi-common\nintel-oneapi-compiler-shared-runtime-libs\nintel-oneapi-openmp\nintel-oneapi-tcm\nintel-ucode\niproute2\niptables\niputils\niso-codes\niw\niwd\njansson\njava-environment-common\njava-runtime-common\njbig2dec\njbigkit\njdk-openjdk\njq\njson-c\njson-glib\njsoncpp\nkaccounts-integration\nkactivitymanagerd\nkarchive\nkate\nkauth\nkbd\nkbookmarks\nkcmutils\nkcodecs\nkcolorscheme\nkcompletion\nkconfig\nkconfigwidgets\nkcoreaddons\nkcrash\nkdbusaddons\nkde-cli-tools\nkde-gtk-config\nkdeclarative\nkdecoration\nkded\nkdeplasma-addons\nkdesu\nkdnssd\nkdsoap-qt6\nkdsoap-ws-discovery-client\nkeyutils\nkfilemetadata\nkgamma\nkglobalaccel\nkglobalacceld\nkguiaddons\nkholidays\nki18n\nkiconthemes\nkidletime\nkinfocenter\nkio\nkio-extras\nkio-fuse\nkirigami\nkirigami-addons\nkitemmodels\nkitemviews\nkitty\nkitty-shell-integration\nkitty-terminfo\nkjobwidgets\nkmenuedit\nkmod\nknewstuff\nknotifications\nknotifyconfig\nkonsole\nkpackage\nkparts\nkpipewire\nkpty\nkquickcharts\nkrb5\nkrdp\nkrunner\nkscreen\nkscreenlocker\nkservice\nksshaskpass\nkstatusnotifieritem\nksvg\nksystemstats\nktexteditor\nktextwidgets\nkunitconversion\nkuserfeedback\nkvantum\nkvantum-theme-otto-git\nkwallet\nkwallet-pam\nkwayland\nkwidgetsaddons\nkwin\nkwindowsystem\nkwrited\nkxmlgui\nl-smash\nlame\nlapack\nlayer-shell-qt\nlcms2\nldb\nlevel-zero-loader\nlibabw\nlibaccounts-glib\nlibaccounts-qt\nlibadwaita\nlibaec\nlibappindicator-gtk3\nlibarchive\nlibass\nlibassuan\nlibasyncns\nlibatasmart\nlibatomic_ops\nlibavc1394\nlibavif\nlibayatana-appindicator\nlibayatana-indicator\nlibb2\nlibblockdev\nlibblockdev-crypto\nlibblockdev-fs\nlibblockdev-loop\nlibblockdev-mdraid\nlibblockdev-nvme\nlibblockdev-part\nlibblockdev-swap\nlibbluray\nlibbpf\nlibbs2b\nlibbsd\nlibbytesize\nlibcanberra\nlibcap\nlibcap-ng\nlibcdio\nlibcdio-paranoia\nlibcdr\nlibcloudproviders\nlibcmis\nlibcolord\nlibcups\nlibcurl-gnutls\nlibdaemon\nlibdatrie\nlibdbusmenu-glib\nlibdbusmenu-gtk3\nlibdc1394\nlibdca\nlibde265\nlibdecor\nlibdeflate\nlibdisplay-info\nlibdmtx\nlibdovi\nlibdrm\nlibdvbpsi\nlibdvdnav\nlibdvdread\nlibe-book\nlibebml\nlibedit\nlibei\nlibelf\nlibepoxy\nlibepubgen\nlibetonyek\nlibevdev\nlibevent\nlibexif\nlibexttextcat\nlibfdk-aac\nlibffi\nlibfontenc\nlibfreeaptx\nlibfreehand\nlibgcrypt\nlibgdata\nlibgirepository\nlibglvnd\nlibgoa\nlibgpg-error\nlibgtop\nlibgudev\nlibgxps\nlibhandy\nlibheif\nlibice\nlibidn\nlibidn2\nlibiec61883\nlibimagequant\nlibimobiledevice\nlibimobiledevice-glue\nlibinih\nlibinput\nlibisl\nlibixion\nlibjpeg-turbo\nlibjxl\nlibkexiv2\nlibksba\nlibkscreen\nlibksysguard\nliblangtag\nliblc3\nlibldac\nlibldap\nlibliftoff\nlibluv\nliblzf\nlibmad\nlibmalcontent\nlibmatroska\nlibmbim\nlibmd\nlibmfx\nlibmm-glib\nlibmnl\nlibmodplug\nlibmpc\nlibmpcdec\nlibmpdclient\nlibmpeg2\nlibmspub\nlibmtp\nlibmwaw\nlibmysofa\nlibndp\nlibnet\nlibnetfilter_conntrack\nlibnewt\nlibnfnetlink\nlibnftnl\nlibnghttp2\nlibnghttp3\nlibngtcp2\nlibnice\nlibnl\nlibnm\nlibnma\nlibnma-common\nlibnotify\nlibnsl\nlibnumbertext\nlibnvme\nlibodfgen\nlibogg\nlibomxil-bellagio\nlibopenmpt\nliborcus\nlibp11-kit\nlibpagemaker\nlibpaper\nlibpcap\nlibpciaccess\nlibpeas\nlibpgm\nlibpipewire\nlibplacebo\nlibplasma\nlibplist\nlibpng\nlibproxy\nlibpsl\nlibpulse\nlibqaccessibilityclient-qt6\nlibqalculate\nlibqmi\nlibqrtr-glib\nlibqxp\nlibraqm\nlibraw1394\nlibreoffice-fresh\nlibrevenge\nlibrsvg\nlibrsync\nlibsamplerate\nlibsasl\nlibseccomp\nlibsecret\nlibsigc++\nlibsigsegv\nlibsixel\nlibsm\nlibsndfile\nlibsodium\nlibsoup\nlibsoup3\nlibsoxr\nlibspectre\nlibssh\nlibssh2\nlibstaroffice\nlibstemmer\nlibsynctex\nlibsysprof-capture\nlibtar\nlibtasn1\nlibteam\nlibthai\nlibtheora\nlibtiff\nlibtirpc\nlibtommath\nlibtool\nlibtpms\nlibunibreak\nlibunistring\nlibunwind\nlibupnp\nliburing\nlibusb\nlibusbmuxd\nlibutempter\nlibutf8proc\nlibuv\nlibva\nlibva-intel-driver\nlibvdpau\nlibverto\nlibvisio\nlibvlc\nlibvorbis\nlibvpl\nlibvpx\nlibvterm\nlibwacom\nlibwbclient\nlibwebp\nlibwireplumber\nlibwpd\nlibwps\nlibx11\nlibxau\nlibxaw\nlibxcb\nlibxcomposite\nlibxcrypt\nlibxcursor\nlibxcvt\nlibxdamage\nlibxdg-basedir\nlibxdmcp\nlibxext\nlibxfce4ui\nlibxfce4util\nlibxfixes\nlibxfont2\nlibxft\nlibxi\nlibxinerama\nlibxkbcommon\nlibxkbcommon-x11\nlibxkbfile\nlibxml2\nlibxmlb\nlibxmu\nlibxpm\nlibxpresent\nlibxrandr\nlibxrender\nlibxshmfence\nlibxslt\nlibxss\nlibxt\nlibxtst\nlibxv\nlibxxf86vm\nlibyaml\nlibyuv\nlibzip\nlibzmf\nlicenses\nlilv\nlinux\nlinux-api-headers\nlinux-firmware\nlinux-firmware-whence\nlinux-headers\nlinux-lts\nlinux-lts-headers\nllvm-libs\nlm_sensors\nlmdb\nlpsolve\nlsof\nlua\nlua51-lpeg\nluajit\nlv2\nlxappearance\nlz4\nlzo\nm4\nmailcap\nmake\nmd4c\nmdadm\nmedia-player-info\nmesa\nmesa-utils\nmeson\nmilou\nminizip\nmkinitcpio\nmkinitcpio-busybox\nmobile-broadband-provider-info\nmodemmanager\nmodemmanager-qt\nmpdecimal\nmpfr\nmpg123\nmpv\nmsgpack-c\nmtdev\nmujs\nnano\nncurses\nneofetch\nneon\nneovim\nnet-tools\nnetpbm\nnettle\nnetwork-manager-applet\nnetworkmanager\nnetworkmanager-qt\nninja\nnm-connection-editor\nnode-gyp\nnodejs\nnodejs-nopt\nnoto-fonts\nnoto-fonts-emoji\nnpm\nnpth\nnspr\nnss\nntfs-3g\nocean-sound-theme\nocl-icd\nonetbb\noniguruma\nopenal\nopencore-amr\nopencv\nopenexr\nopenjpeg2\nopenssh\nopenssl\nopenxr\nopus\norc\nos-prober\nostree\noxygen\noxygen-sounds\np11-kit\npacman\npacman-contrib\npacman-mirrorlist\npahole\npam\npambase\npango\npangomm\nparted\nparu\nparu-debug\npatch\npciutils\npcre\npcre2\npcsclite\nperl\nperl-error\nperl-mailtools\nperl-timedate\nphonon-qt6\nphonon-qt6-vlc\npinentry\npipewire\npipewire-alsa\npipewire-audio\npipewire-jack\npipewire-pulse\npixman\npkgconf\nplasma-activities\nplasma-activities-stats\nplasma-browser-integration\nplasma-desktop\nplasma-disks\nplasma-firewall\nplasma-integration\nplasma-meta\nplasma-nm\nplasma-pa\nplasma-systemmonitor\nplasma-thunderbolt\nplasma-vault\nplasma-welcome\nplasma-workspace\nplasma-workspace-wallpapers\nplasma5support\nplayerctl\nplymouth\nplymouth-kcm\npolkit\npolkit-kde-agent\npolkit-qt6\npoppler\npoppler-data\npoppler-glib\npoppler-qt6\npopt\nportaudio\npotrace\npowerdevil\nppp\nprint-manager\nprison\nprocps-ng\nprotobuf\npsmisc\npugixml\npulseaudio-qt\npurpose\npybind11\npython\npython-absl\npython-argcomplete\npython-astor\npython-astunparse\npython-attrs\npython-autocommand\npython-bleach\npython-cachetools\npython-cairo\npython-certifi\npython-cffi\npython-charset-normalizer\npython-click\npython-colorama\npython-contourpy\npython-cryptography\npython-cycler\npython-dateutil\npython-distro\npython-docopt\npython-fastjsonschema\npython-filelock\npython-flatbuffers\npython-fonttools\npython-gast\npython-gobject\npython-google-auth\npython-google-auth-oauthlib\npython-grpcio\npython-grpcio-tools\npython-h11\npython-h5py\npython-html5lib\npython-httplib2\npython-idna\npython-isodate\npython-jaraco.collections\npython-jaraco.context\npython-jaraco.functools\npython-jaraco.text\npython-joblib\npython-keras\npython-keyutils\npython-kiwisolver\npython-lxml\npython-mako\npython-markdown\npython-markdown-it-py\npython-markupsafe\npython-matplotlib\npython-mdurl\npython-ml-dtypes\npython-more-itertools\npython-numpy\npython-oauth2client\npython-oauthlib\npython-opt_einsum\npython-optree\npython-ordered-set\npython-outcome\npython-packaging\npython-pandas\npython-pasta\npython-pillow\npython-pip\npython-pipx\npython-platformdirs\npython-pooch\npython-protobuf\npython-psutil\npython-pyasn1\npython-pyasn1-modules\npython-pycountry\npython-pycparser\npython-pycryptodome\npython-pydot\npython-pygdbmi\npython-pygments\npython-pyparsing\npython-pysocks\npython-pytz\npython-requests\npython-requests-oauthlib\npython-rich\npython-rsa\npython-scikit-learn\npython-scipy\npython-sentry_sdk\npython-setuptools\npython-six\npython-sniffio\npython-sortedcontainers\npython-tensorboard_plugin_wit\npython-tensorflow\npython-tensorflow-estimator\npython-termcolor\npython-threadpoolctl\npython-tomli\npython-tqdm\npython-trio\npython-trio-websocket\npython-trove-classifiers\npython-typing_extensions\npython-uc-micro-py\npython-urllib3\npython-userpath\npython-validate-pyproject\npython-webencodings\npython-websocket-client\npython-werkzeug\npython-wheel\npython-wrapt\npython-wsproto\npython-yaml\nqca-qt6\nqcoro\nqhull\nqqc2-breeze-style\nqqc2-desktop-style\nqrencode\nqt5-base\nqt5-declarative\nqt5-svg\nqt5-translations\nqt5-wayland\nqt5-x11extras\nqt6-5compat\nqt6-base\nqt6-declarative\nqt6-multimedia\nqt6-multimedia-ffmpeg\nqt6-positioning\nqt6-quick3d\nqt6-quicktimeline\nqt6-scxml\nqt6-sensors\nqt6-shadertools\nqt6-speech\nqt6-svg\nqt6-tools\nqt6-translations\nqt6-virtualkeyboard\nqt6-wayland\nqt6-webchannel\nqt6-webengine\nqt6-websockets\nqt6-webview\nqt6ct\nqtkeychain-qt6\nranger\nraptor\nrasqal\nrav1e\nre2\nreadline\nredland\nreflector\nrhash\nripgrep\nripgrep-all\nrofi-lbonn-wayland-git\nrofi-lbonn-wayland-git-debug\nrtkit\nrubberband\nrust\nsamba\nsbc\nscdoc\nsdbus-cpp\nsddm\nsddm-kcm\nsdl12-compat\nsdl2\nseatd\nsed\nsemver\nserd\nserf\nshaderc\nshadow\nshared-mime-info\nsignon-kwallet-extension\nsignon-plugin-oauth2\nsignon-ui\nsignond\nslang\nslurp\nsmartmontools\nsmbclient\nsnappy\nsndio\nsocat\nsof-firmware\nsolid\nsonnet\nsord\nsound-theme-freedesktop\nsource-highlight\nspdlog\nspeex\nspeexdsp\nspirv-tools\nspotify\nsqlite\nsratom\nsrt\nstartup-notification\nstow\nstreamlink\nsubversion\nsudo\nsvt-av1\nsyndication\nsyntax-highlighting\nsystemd\nsystemd-libs\nsystemd-sysvcompat\nsystemsettings\ntaglib\ntalloc\ntar\ntdb\ntensorboard\ntensorflow\ntevent\ntexinfo\ntexlive-basic\ntexlive-bibtexextra\ntexlive-bin\ntexlive-binextra\ntexlive-context\ntexlive-fontsextra\ntexlive-fontsrecommended\ntexlive-fontutils\ntexlive-formatsextra\ntexlive-games\ntexlive-humanities\ntexlive-latex\ntexlive-latexextra\ntexlive-latexrecommended\ntexlive-luatex\ntexlive-mathscience\ntexlive-meta\ntexlive-metapost\ntexlive-music\ntexlive-pictures\ntexlive-plaingeneric\ntexlive-pstricks\ntexlive-publishers\ntexlive-xetex\nthunar\nthunar-archive-plugin\nthunar-media-tags-plugin\nthunar-shares-plugin\nthunar-vcs-plugin\nthunar-volman\ntinysparql\ntomlplusplus\ntpm2-tss\ntree-sitter\ntree-sitter-c\ntree-sitter-lua\ntree-sitter-markdown\ntree-sitter-query\ntree-sitter-vim\ntree-sitter-vimdoc\ntslib\nttf-dejavu\nttf-hack\nttf-jetbrains-mono-nerd\ntzdata\nuchardet\nudiskie\nudisks2\nunibilium\nunzip\nupower\nusbmuxd\nusbutils\nutil-linux\nutil-linux-libs\nv4l-utils\nvapoursynth\nverdict\nvid.stab\nvim\nvim-runtime\nvirtualbox\nvirtualbox-host-dkms\nvisual-studio-code-bin\nvisual-studio-code-bin-debug\nvlc\nvmaf\nvolume_key\nvulkan-headers\nvulkan-icd-loader\nvulkan-intel\nvulkan-radeon\nvulkan-tools\nvulkan-validation-layers\nwaybar\nwaybar-git-debug\nwayland\nwayland-protocols\nwayland-utils\nwebp-pixbuf-loader\nwebrtc-audio-processing-1\nwf-recorder\nwget\nwhich\nwhitesur-cursor-theme-git\nwhitesur-icon-theme\nwireless_tools\nwireplumber\nwl-clipboard\nwlogout\nwlogout-debug\nwlroots0.17\nwoff2\nwofi\nwpa_supplicant\nx264\nx265\nxcb-proto\nxcb-util\nxcb-util-cursor\nxcb-util-errors\nxcb-util-image\nxcb-util-keysyms\nxcb-util-renderutil\nxcb-util-wm\nxcb-util-xrm\nxdg-dbus-proxy\nxdg-desktop-portal\nxdg-desktop-portal-hyprland\n[godzeus@arch ~]$ pacman -Qe\naircrack-ng 1.7-4\nalsa-utils 1.2.13-2\nani-cli 4.9-1\nark 24.12.0-1\nbaobab 47.0-1\nbase 3-2\nbase-devel 1-2\nbash-completion 2.15.0-1\nblueman 2.4.3-1\nbluetui 0.6-1\nbluez-utils 5.79-1\nbrightnessctl 0.5.1-3\nbtop 1.4.0-4\nchafa 1.14.5-1\nclipse-bin 1.1.0-1\ncrunch 3.6-1\ndiscord 0.0.78-1\ndolphin 24.12.0.1-1\ndunst 1.12.0-1\nefibootmgr 18-3\negl-wayland 4:1.1.17-1\neog 47.0-1\nevince 46.3.1-2\nfd 10.2.0-1\nfirefox 133.0.3-2\nfish 3.7.1-2\nflatpak 1:1.15.12-1\ngit 2.47.1-1\ngithub-desktop-bin 3.4.8_linux1-1\ngnome-keyring 1:46.2-1\ngrim 1.4.1-2\ngrub 2:2.12-3\ngruvbox-dark-gtk 1.0.2-1\ngst-plugin-pipewire 1:1.2.7-1\ngtk-engine-murrine 0.98.2-4\ngvfs 1.56.1-1\ngvfs-afc 1.56.1-1\ngvfs-google 1.56.1-1\nhashcat 1:6.2.6-2\nhcxtools 6.3.4-1\nhtop 3.3.0-3\nhypridle 0.1.5-1\nhyprland 0.46.2-1\nhyprland-qtutils 0.1.1-3\nhyprlock 0.6.0-1\nhyprpaper 0.7.3-2\nhyprpicker 0.4.1-1\nintel-media-driver 24.4.4-1\nintel-ucode 20241112-1\niwd 3.3-1\njdk-openjdk 23.0.1.u0-1\njq 1.7.1-2\nkate 24.12.0-1\nkitty 0.37.0-1\nkonsole 24.12.0-1\nkvantum 1.1.3-1\nkvantum-theme-otto-git r25.f50ea46-1\nlibpulse 17.0+r43+g3e2bb8a1e-1\nlibreoffice-fresh 24.8.3-1\nlibva-intel-driver 2.4.1-3\nlinux 6.12.4.arch1-1\nlinux-firmware 20241210.b00a7f7e-1\nlinux-headers 6.12.4.arch1-1\nlinux-lts 6.6.65-1\nlinux-lts-headers 6.6.65-1\nlxappearance 0.6.3-5\nnano 8.2-1\nneofetch 7.1.0-2\nneovim 0.10.3-1\nnetwork-manager-applet 1.36.0-1\nnetworkmanager 1.50.0-1\nnodejs 23.4.0-1\nnpm 10.9.2-1\nntfs-3g 2022.10.3-1\nopencv 4.10.0-15\nos-prober 1.81-2\npacman-contrib 1.10.6-2\nparu 2.0.4-1\nparu-debug 2.0.4-1\npipewire 1:1.2.7-1\npipewire-alsa 1:1.2.7-1\npipewire-jack 1:1.2.7-1\npipewire-pulse 1:1.2.7-1\nplasma-meta 6.1-1\nplasma-workspace 6.2.4-1\nplymouth 24.004.60-9\nplymouth-kcm 6.2.4-1\npolkit-kde-agent 6.2.4-1\npython-matplotlib 3.9.3-1\npython-pandas 2.2.2-3\npython-pip 24.3.1-1\npython-pipx 1.7.1-1\npython-scikit-learn 1.6.0-1\npython-tensorflow 2.18.0-3\nqt5-wayland 5.15.16+kde+r59-3\nqt6-wayland 6.8.1-1\nqt6ct 0.9-12\nranger 1.9.4-1\nreflector 2023-2\nripgrep 14.1.1-1\nrofi-lbonn-wayland-git 1.7.5.wayland3.r52.g0abd8878-1\nslurp 1.5.0-1\nsmartmontools 7.4-2\nsof-firmware 2024.09.2-1\nspotify 1:1.2.50.335-1\nstow 2.4.1-1\nstreamlink 7.0.0-1\ntexlive-basic 2024.2-3\ntexlive-fontsextra 2024.2-3\ntexlive-latexextra 2024.2-3\ntexlive-meta 2024.2-3\nthunar 4.18.11-3\nthunar-archive-plugin 0.5.2-3\nthunar-media-tags-plugin 0.4.0-4\nthunar-shares-plugin 1:0.3.2-3\nthunar-vcs-plugin 0.2.0-3\nthunar-volman 4.18.0-4\nttf-dejavu 2.37+18+g9b5d1b2f-7\nttf-jetbrains-mono-nerd 3.3.0-1\nudiskie 2.5.3-1\nunzip 6.0-21\nvim 9.1.0866-1\nvirtualbox 7.1.4-2\nvisual-studio-code-bin 1.95.3-1\nvulkan-intel 1:24.3.1-3\nvulkan-radeon 1:24.3.1-3\nwaybar 0.11.0-4\nwf-recorder 0.5.0-2\nwget 1.25.0-1\nwhitesur-cursor-theme-git r11.63d04b8-1\nwhitesur-icon-theme 2024.09.07-1\nwireless_tools 30.pre9-4\nwireplumber 0.5.7-1\nwl-clipboard 1:2.2.1-2\nwlogout 1.2.2-0\nwofi 1.4.1-1\nxdg-desktop-portal-hyprland 1.3.9-1\nxdg-utils 1.2.1-1\nxf86-video-amdgpu 23.0.0-2\nxf86-video-ati 1:22.0.0-2\nxf86-video-nouveau 1.0.18-1\nxf86-video-vmware 13.4.0-3\nxorg-xinit 1.4.2-2\nyay 12.4.2-1\nyay-debug 12.4.2-1\nzathura 0.5.8-1\nzathura-pdf-poppler 0.3.3-1\nzen-browser-bin 1.0.2.b.0-1\nzram-generator 1.2.1-1\nzsh 5.9-5","recorded":"2024-12-23 15:04:48.460417868","filePath":"null","pinned":false},{"value":"[godzeus@arch ~]$ pacman -Qq\na52dec\nabseil-cpp\naccounts-qml-module\naccountsservice\nacl\nadobe-source-code-pro-fonts\nadwaita-cursors\nadwaita-icon-theme\nadwaita-icon-theme-legacy\naha\naircrack-ng\nalsa-card-profiles\nalsa-lib\nalsa-topology-conf\nalsa-ucm-conf\nalsa-utils\nani-cli\naom\nappstream\nappstream-qt\napr\napr-util\naquamarine\narchlinux-appstream-data\narchlinux-keyring\nargon2\naria2\naribb24\nark\nat-spi2-core\natkmm\nattica\nattr\naudit\nautoconf\nautomake\navahi\nayatana-ido\nbaloo\nbaloo-widgets\nbaobab\nbase\nbase-devel\nbash\nbash-completion\nbinutils\nbison\nblas\nbluedevil\nblueman\nbluetui\nbluetui-debug\nbluez\nbluez-libs\nbluez-qt\nbluez-utils\nbolt\nboost-libs\nbreeze\nbreeze-gtk\nbreeze-icons\nbrightnessctl\nbrotli\nbtop\nbubblewrap\nbzip2\nc-ares\nca-certificates\nca-certificates-mozilla\nca-certificates-utils\ncairo\ncairomm\ncantarell-fonts\ncatch2\ncblas\ncdparanoia\nchafa\ncheck\nchrono-date\ncifs-utils\nclinfo\nclipse-bin\nclucene\ncmake\ncomposefs\nconvertlit\ncoreutils\ncppdap\ncrunch\ncrunch-debug\ncryptsetup\ncurl\ndav1d\ndb5.3\ndbus\ndbus-broker\ndbus-broker-units\ndbus-glib\ndbus-units\ndconf\nddcutil\ndebugedit\ndefault-cursors\ndesktop-file-utils\ndevice-mapper\ndiffutils\ndiscord\ndiscount\ndiscover\ndjvulibre\ndkms\ndmidecode\ndolphin\ndouble-conversion\ndrkonqi\nduktape\ndunst\ndvisvgm\ne2fsprogs\nebook-tools\neditorconfig-core-c\nefibootmgr\nefivar\negl-wayland\neglexternalplatform\nell\nenchant\neog\nethtool\nevince\nexempi\nexiv2\nexo\nexpat\nfaad2\nfakeroot\nfd\nffcall\nffmpeg\nffmpeg4.4\nfftw\nfile\nfilesystem\nfindutils\nfirefox\nfish\nflac\nflatpak\nflex\nfmt\nfontconfig\nframeworkintegration\nfreerdp2\nfreetype2\nfribidi\nfuse-common\nfuse2\nfuse3\nfzf\ngawk\ngc\ngcc\ngcc-libs\ngcr\ngcr-4\ngd\ngdb\ngdb-common\ngdbm\ngdk-pixbuf2\ngeoclue\ngettext\nghostscript\ngiflib\ngirara\ngit\ngithub-desktop-bin\nglib-networking\nglib2\nglib2-devel\nglibc\nglibmm\nglslang\nglu\ngmp\ngnome-desktop\ngnome-desktop-common\ngnome-keyring\ngnu-free-fonts\ngnupg\ngnutls\ngo\ngobject-introspection\ngobject-introspection-runtime\ngperftools\ngpgme\ngpm\ngraphene\ngraphite\ngraphviz\ngrep\ngrim\ngroff\ngrub\ngruvbox-dark-gtk\ngsettings-desktop-schemas\ngsettings-system-schemas\ngsfonts\ngsm\ngspell\ngssdp\ngst-plugin-pipewire\ngst-plugins-bad-libs\ngst-plugins-base\ngst-plugins-base-libs\ngstreamer\ngtest\ngtk-engine-murrine\ngtk-layer-shell\ngtk-update-icon-cache\ngtk2\ngtk3\ngtk4\ngtkmm3\ngts\nguile\ngupnp\ngupnp-igd\ngvfs\ngvfs-afc\ngvfs-goa\ngvfs-google\ngzip\nharfbuzz\nharfbuzz-icu\nhashcat\nhcxtools\nhdf5\nhicolor-icon-theme\nhidapi\nhighway\nhtop\nhunspell\nhwdata\nhwloc\nhyphen\nhyprcursor\nhyprgraphics\nhypridle\nhyprland\nhyprland-qtutils\nhyprlang\nhyprlock\nhyprpaper\nhyprpicker\nhyprutils\nhyprwayland-scanner\ni2c-tools\niana-etc\nicu\nijs\nimath\nintel-gmmlib\nintel-media-driver\nintel-oneapi-common\nintel-oneapi-compiler-shared-runtime-libs\nintel-oneapi-openmp\nintel-oneapi-tcm\nintel-ucode\niproute2\niptables\niputils\niso-codes\niw\niwd\njansson\njava-environment-common\njava-runtime-common\njbig2dec\njbigkit\njdk-openjdk\njq\njson-c\njson-glib\njsoncpp\nkaccounts-integration\nkactivitymanagerd\nkarchive\nkate\nkauth\nkbd\nkbookmarks\nkcmutils\nkcodecs\nkcolorscheme\nkcompletion\nkconfig\nkconfigwidgets\nkcoreaddons\nkcrash\nkdbusaddons\nkde-cli-tools\nkde-gtk-config\nkdeclarative\nkdecoration\nkded\nkdeplasma-addons\nkdesu\nkdnssd\nkdsoap-qt6\nkdsoap-ws-discovery-client\nkeyutils\nkfilemetadata\nkgamma\nkglobalaccel\nkglobalacceld\nkguiaddons\nkholidays\nki18n\nkiconthemes\nkidletime\nkinfocenter\nkio\nkio-extras\nkio-fuse\nkirigami\nkirigami-addons\nkitemmodels\nkitemviews\nkitty\nkitty-shell-integration\nkitty-terminfo\nkjobwidgets\nkmenuedit\nkmod\nknewstuff\nknotifications\nknotifyconfig\nkonsole\nkpackage\nkparts\nkpipewire\nkpty\nkquickcharts\nkrb5\nkrdp\nkrunner\nkscreen\nkscreenlocker\nkservice\nksshaskpass\nkstatusnotifieritem\nksvg\nksystemstats\nktexteditor\nktextwidgets\nkunitconversion\nkuserfeedback\nkvantum\nkvantum-theme-otto-git\nkwallet\nkwallet-pam\nkwayland\nkwidgetsaddons\nkwin\nkwindowsystem\nkwrited\nkxmlgui\nl-smash\nlame\nlapack\nlayer-shell-qt\nlcms2\nldb\nlevel-zero-loader\nlibabw\nlibaccounts-glib\nlibaccounts-qt\nlibadwaita\nlibaec\nlibappindicator-gtk3\nlibarchive\nlibass\nlibassuan\nlibasyncns\nlibatasmart\nlibatomic_ops\nlibavc1394\nlibavif\nlibayatana-appindicator\nlibayatana-indicator\nlibb2\nlibblockdev\nlibblockdev-crypto\nlibblockdev-fs\nlibblockdev-loop\nlibblockdev-mdraid\nlibblockdev-nvme\nlibblockdev-part\nlibblockdev-swap\nlibbluray\nlibbpf\nlibbs2b\nlibbsd\nlibbytesize\nlibcanberra\nlibcap\nlibcap-ng\nlibcdio\nlibcdio-paranoia\nlibcdr\nlibcloudproviders\nlibcmis\nlibcolord\nlibcups\nlibcurl-gnutls\nlibdaemon\nlibdatrie\nlibdbusmenu-glib\nlibdbusmenu-gtk3\nlibdc1394\nlibdca\nlibde265\nlibdecor\nlibdeflate\nlibdisplay-info\nlibdmtx\nlibdovi\nlibdrm\nlibdvbpsi\nlibdvdnav\nlibdvdread\nlibe-book\nlibebml\nlibedit\nlibei\nlibelf\nlibepoxy\nlibepubgen\nlibetonyek\nlibevdev\nlibevent\nlibexif\nlibexttextcat\nlibfdk-aac\nlibffi\nlibfontenc\nlibfreeaptx\nlibfreehand\nlibgcrypt\nlibgdata\nlibgirepository\nlibglvnd\nlibgoa\nlibgpg-error\nlibgtop\nlibgudev\nlibgxps\nlibhandy\nlibheif\nlibice\nlibidn\nlibidn2\nlibiec61883\nlibimagequant\nlibimobiledevice\nlibimobiledevice-glue\nlibinih\nlibinput\nlibisl\nlibixion\nlibjpeg-turbo\nlibjxl\nlibkexiv2\nlibksba\nlibkscreen\nlibksysguard\nliblangtag\nliblc3\nlibldac\nlibldap\nlibliftoff\nlibluv\nliblzf\nlibmad\nlibmalcontent\nlibmatroska\nlibmbim\nlibmd\nlibmfx\nlibmm-glib\nlibmnl\nlibmodplug\nlibmpc\nlibmpcdec\nlibmpdclient\nlibmpeg2\nlibmspub\nlibmtp\nlibmwaw\nlibmysofa\nlibndp\nlibnet\nlibnetfilter_conntrack\nlibnewt\nlibnfnetlink\nlibnftnl\nlibnghttp2\nlibnghttp3\nlibngtcp2\nlibnice\nlibnl\nlibnm\nlibnma\nlibnma-common\nlibnotify\nlibnsl\nlibnumbertext\nlibnvme\nlibodfgen\nlibogg\nlibomxil-bellagio\nlibopenmpt\nliborcus\nlibp11-kit\nlibpagemaker\nlibpaper\nlibpcap\nlibpciaccess\nlibpeas\nlibpgm\nlibpipewire\nlibplacebo\nlibplasma\nlibplist\nlibpng\nlibproxy\nlibpsl\nlibpulse\nlibqaccessibilityclient-qt6\nlibqalculate\nlibqmi\nlibqrtr-glib\nlibqxp\nlibraqm\nlibraw1394\nlibreoffice-fresh\nlibrevenge\nlibrsvg\nlibrsync\nlibsamplerate\nlibsasl\nlibseccomp\nlibsecret\nlibsigc++\nlibsigsegv\nlibsixel\nlibsm\nlibsndfile\nlibsodium\nlibsoup\nlibsoup3\nlibsoxr\nlibspectre\nlibssh\nlibssh2\nlibstaroffice\nlibstemmer\nlibsynctex\nlibsysprof-capture\nlibtar\nlibtasn1\nlibteam\nlibthai\nlibtheora\nlibtiff\nlibtirpc\nlibtommath\nlibtool\nlibtpms\nlibunibreak\nlibunistring\nlibunwind\nlibupnp\nliburing\nlibusb\nlibusbmuxd\nlibutempter\nlibutf8proc\nlibuv\nlibva\nlibva-intel-driver\nlibvdpau\nlibverto\nlibvisio\nlibvlc\nlibvorbis\nlibvpl\nlibvpx\nlibvterm\nlibwacom\nlibwbclient\nlibwebp\nlibwireplumber\nlibwpd\nlibwps\nlibx11\nlibxau\nlibxaw\nlibxcb\nlibxcomposite\nlibxcrypt\nlibxcursor\nlibxcvt\nlibxdamage\nlibxdg-basedir\nlibxdmcp\nlibxext\nlibxfce4ui\nlibxfce4util\nlibxfixes\nlibxfont2\nlibxft\nlibxi\nlibxinerama\nlibxkbcommon\nlibxkbcommon-x11\nlibxkbfile\nlibxml2\nlibxmlb\nlibxmu\nlibxpm\nlibxpresent\nlibxrandr\nlibxrender\nlibxshmfence\nlibxslt\nlibxss\nlibxt\nlibxtst\nlibxv\nlibxxf86vm\nlibyaml\nlibyuv\nlibzip\nlibzmf\nlicenses\nlilv\nlinux\nlinux-api-headers\nlinux-firmware\nlinux-firmware-whence\nlinux-headers\nlinux-lts\nlinux-lts-headers\nllvm-libs\nlm_sensors\nlmdb\nlpsolve\nlsof\nlua\nlua51-lpeg\nluajit\nlv2\nlxappearance\nlz4\nlzo\nm4\nmailcap\nmake\nmd4c\nmdadm\nmedia-player-info\nmesa\nmesa-utils\nmeson\nmilou\nminizip\nmkinitcpio\nmkinitcpio-busybox\nmobile-broadband-provider-info\nmodemmanager\nmodemmanager-qt\nmpdecimal\nmpfr\nmpg123\nmpv\nmsgpack-c\nmtdev\nmujs\nnano\nncurses\nneofetch\nneon\nneovim\nnet-tools\nnetpbm\nnettle\nnetwork-manager-applet\nnetworkmanager\nnetworkmanager-qt\nninja\nnm-connection-editor\nnode-gyp\nnodejs\nnodejs-nopt\nnoto-fonts\nnoto-fonts-emoji\nnpm\nnpth\nnspr\nnss\nntfs-3g\nocean-sound-theme\nocl-icd\nonetbb\noniguruma\nopenal\nopencore-amr\nopencv\nopenexr\nopenjpeg2\nopenssh\nopenssl\nopenxr\nopus\norc\nos-prober\nostree\noxygen\noxygen-sounds\np11-kit\npacman\npacman-contrib\npacman-mirrorlist\npahole\npam\npambase\npango\npangomm\nparted\nparu\nparu-debug\npatch\npciutils\npcre\npcre2\npcsclite\nperl\nperl-error\nperl-mailtools\nperl-timedate\nphonon-qt6\nphonon-qt6-vlc\npinentry\npipewire\npipewire-alsa\npipewire-audio\npipewire-jack\npipewire-pulse\npixman\npkgconf\nplasma-activities\nplasma-activities-stats\nplasma-browser-integration\nplasma-desktop\nplasma-disks\nplasma-firewall\nplasma-integration\nplasma-meta\nplasma-nm\nplasma-pa\nplasma-systemmonitor\nplasma-thunderbolt\nplasma-vault\nplasma-welcome\nplasma-workspace\nplasma-workspace-wallpapers\nplasma5support\nplayerctl\nplymouth\nplymouth-kcm\npolkit\npolkit-kde-agent\npolkit-qt6\npoppler\npoppler-data\npoppler-glib\npoppler-qt6\npopt\nportaudio\npotrace\npowerdevil\nppp\nprint-manager\nprison\nprocps-ng\nprotobuf\npsmisc\npugixml\npulseaudio-qt\npurpose\npybind11\npython\npython-absl\npython-argcomplete\npython-astor\npython-astunparse\npython-attrs\npython-autocommand\npython-bleach\npython-cachetools\npython-cairo\npython-certifi\npython-cffi\npython-charset-normalizer\npython-click\npython-colorama\npython-contourpy\npython-cryptography\npython-cycler\npython-dateutil\npython-distro\npython-docopt\npython-fastjsonschema\npython-filelock\npython-flatbuffers\npython-fonttools\npython-gast\npython-gobject\npython-google-auth\npython-google-auth-oauthlib\npython-grpcio\npython-grpcio-tools\npython-h11\npython-h5py\npython-html5lib\npython-httplib2\npython-idna\npython-isodate\npython-jaraco.collections\npython-jaraco.context\npython-jaraco.functools\npython-jaraco.text\npython-joblib\npython-keras\npython-keyutils\npython-kiwisolver\npython-lxml\npython-mako\npython-markdown\npython-markdown-it-py\npython-markupsafe\npython-matplotlib\npython-mdurl\npython-ml-dtypes\npython-more-itertools\npython-numpy\npython-oauth2client\npython-oauthlib\npython-opt_einsum\npython-optree\npython-ordered-set\npython-outcome\npython-packaging\npython-pandas\npython-pasta\npython-pillow\npython-pip\npython-pipx\npython-platformdirs\npython-pooch\npython-protobuf\npython-psutil\npython-pyasn1\npython-pyasn1-modules\npython-pycountry\npython-pycparser\npython-pycryptodome\npython-pydot\npython-pygdbmi\npython-pygments\npython-pyparsing\npython-pysocks\npython-pytz\npython-requests\npython-requests-oauthlib\npython-rich\npython-rsa\npython-scikit-learn\npython-scipy\npython-sentry_sdk\npython-setuptools\npython-six\npython-sniffio\npython-sortedcontainers\npython-tensorboard_plugin_wit\npython-tensorflow\npython-tensorflow-estimator\npython-termcolor\npython-threadpoolctl\npython-tomli\npython-tqdm\npython-trio\npython-trio-websocket\npython-trove-classifiers\npython-typing_extensions\npython-uc-micro-py\npython-urllib3\npython-userpath\npython-validate-pyproject\npython-webencodings\npython-websocket-client\npython-werkzeug\npython-wheel\npython-wrapt\npython-wsproto\npython-yaml\nqca-qt6\nqcoro\nqhull\nqqc2-breeze-style\nqqc2-desktop-style\nqrencode\nqt5-base\nqt5-declarative\nqt5-svg\nqt5-translations\nqt5-wayland\nqt5-x11extras\nqt6-5compat\nqt6-base\nqt6-declarative\nqt6-multimedia\nqt6-multimedia-ffmpeg\nqt6-positioning\nqt6-quick3d\nqt6-quicktimeline\nqt6-scxml\nqt6-sensors\nqt6-shadertools\nqt6-speech\nqt6-svg\nqt6-tools\nqt6-translations\nqt6-virtualkeyboard\nqt6-wayland\nqt6-webchannel\nqt6-webengine\nqt6-websockets\nqt6-webview\nqt6ct\nqtkeychain-qt6\nranger\nraptor\nrasqal\nrav1e\nre2\nreadline\nredland\nreflector\nrhash\nripgrep\nripgrep-all\nrofi-lbonn-wayland-git\nrofi-lbonn-wayland-git-debug\nrtkit\nrubberband\nrust\nsamba\nsbc\nscdoc\nsdbus-cpp\nsddm\nsddm-kcm\nsdl12-compat\nsdl2\nseatd\nsed\nsemver\nserd\nserf\nshaderc\nshadow\nshared-mime-info\nsignon-kwallet-extension\nsignon-plugin-oauth2\nsignon-ui\nsignond\nslang\nslurp\nsmartmontools\nsmbclient\nsnappy\nsndio\nsocat\nsof-firmware\nsolid\nsonnet\nsord\nsound-theme-freedesktop\nsource-highlight\nspdlog\nspeex\nspeexdsp\nspirv-tools\nspotify\nsqlite\nsratom\nsrt\nstartup-notification\nstow\nstreamlink\nsubversion\nsudo\nsvt-av1\nsyndication\nsyntax-highlighting\nsystemd\nsystemd-libs\nsystemd-sysvcompat\nsystemsettings\ntaglib\ntalloc\ntar\ntdb\ntensorboard\ntensorflow\ntevent\ntexinfo\ntexlive-basic\ntexlive-bibtexextra\ntexlive-bin\ntexlive-binextra\ntexlive-context\ntexlive-fontsextra\ntexlive-fontsrecommended\ntexlive-fontutils\ntexlive-formatsextra\ntexlive-games\ntexlive-humanities\ntexlive-latex\ntexlive-latexextra\ntexlive-latexrecommended\ntexlive-luatex\ntexlive-mathscience\ntexlive-meta\ntexlive-metapost\ntexlive-music\ntexlive-pictures\ntexlive-plaingeneric\ntexlive-pstricks\ntexlive-publishers\ntexlive-xetex\nthunar\nthunar-archive-plugin\nthunar-media-tags-plugin\nthunar-shares-plugin\nthunar-vcs-plugin\nthunar-volman\ntinysparql\ntomlplusplus\ntpm2-tss\ntree-sitter\ntree-sitter-c\ntree-sitter-lua\ntree-sitter-markdown\ntree-sitter-query\ntree-sitter-vim\ntree-sitter-vimdoc\ntslib\nttf-dejavu\nttf-hack\nttf-jetbrains-mono-nerd\ntzdata\nuchardet\nudiskie\nudisks2\nunibilium\nunzip\nupower\nusbmuxd\nusbutils\nutil-linux\nutil-linux-libs\nv4l-utils\nvapoursynth\nverdict\nvid.stab\nvim\nvim-runtime\nvirtualbox\nvirtualbox-host-dkms\nvisual-studio-code-bin\nvisual-studio-code-bin-debug\nvlc\nvmaf\nvolume_key\nvulkan-headers\nvulkan-icd-loader\nvulkan-intel\nvulkan-radeon\nvulkan-tools\nvulkan-validation-layers\nwaybar\nwaybar-git-debug\nwayland\nwayland-protocols\nwayland-utils\nwebp-pixbuf-loader\nwebrtc-audio-processing-1\nwf-recorder\nwget\nwhich\nwhitesur-cursor-theme-git\nwhitesur-icon-theme\nwireless_tools\nwireplumber\nwl-clipboard\nwlogout\nwlogout-debug\nwlroots0.17\nwoff2\nwofi\nwpa_supplicant\nx264\nx265\nxcb-proto\nxcb-util\nxcb-util-cursor\nxcb-util-errors\nxcb-util-image\nxcb-util-keysyms\nxcb-util-renderutil\nxcb-util-wm\nxcb-util-xrm\nxdg-dbus-proxy\nxdg-desktop-portal\nxdg-desktop-portal-hyprland\nxdg-desktop-portal-kde\nxdg-user-dirs\nxdg-utils\nxf86-input-libinput\nxf86-video-amdgpu\nxf86-video-ati\nxf86-video-nouveau\nxf86-video-vmware\nxfconf\nxkeyboard-config\nxmlsec\nxorg-fonts-encodings\nxorg-server\nxorg-server-common\nxorg-setxkbmap\nxorg-xauth\nxorg-xdpyinfo\nxorg-xinit\nxorg-xinput\nxorg-xkbcomp\nxorg-xmessage\nxorg-xmodmap\nxorg-xprop\nxorg-xrandr\nxorg-xrdb\nxorg-xset\nxorg-xsetroot\nxorg-xwayland\nxorgproto\nxsettingsd\nxvidcore\nxxhash\nxz\nyay\nyay-debug\nzathura\nzathura-pdf-poppler\nzen-browser-bin\nzeromq\nzimg\nzix\nzlib\nzram-generator\nzsh\nzstd\nzxing-cpp\nzziplib","recorded":"2024-12-23 15:00:41.421544242","filePath":"null","pinned":false},{"value":"Now lets get into keywords. Tell me the required changes given by the reviewers.","recorded":"2024-12-23 14:56:22.570065731","filePath":"null","pinned":false},{"value":"The judicial system is fundamental to democratic governance, adjudicating conflicts, resolving disputes, and upholding justice. This research study presents a Court Judgment Prediction and Recommendation (CJPR) system, a novel artificial intelligence-based framework aimed at mitigating inefficiencies in judicial decision-making, decreasing case backlogs, and improving judgment quality. The system utilizes a dataset of Supreme Court cases in India from 1947 to 2023, employing a comprehensive methodological framework that includes data preprocessing, feature extraction, and evaluation through advanced machine learning models, such as transformers. The results demonstrate that the RoBERTa model attained a model accuracy of 77% and an F1-score of 0.765, surpassing traditional models like logistic regression and random forests. The system exhibited an 82% prediction accuracy on 1,000 new test cases, highlighting its robustness in real-world applications. Significant advancements comprise an integrated recommendation engine that use cosine similarity to suggest relevant historical cases, with the utilization of GPU and TPU platforms for scalable model training and inference. This research establishes a new standard for legal analytics by incorporating advanced approaches in natural language processing and predictive analytics, significantly enhancing court efficiency and decision-making processes.","recorded":"2024-12-23 14:33:38.216140502","filePath":"null","pinned":false},{"value":"The judicial system is a cornerstone of democratic governance, mediating conflicts, resolving disputes, and ensuring justice. This study presents a Court Judgment Prediction and Recommendation (CJPR) system, a novel artificial intelligence-driven framework designed to address inefficiencies in judicial decision-making, reduce case backlogs, and enhance decision quality. Leveraging a dataset of Supreme Court cases in India spanning from 1947 to 2023, the system employs a rigorous methodological framework encompassing data preprocessing, feature extraction, and evaluation using advanced machine learning models, including transformers. Results indicate that the RoBERTa model achieved a 77% model accuracy and an F1-score of 0.765, outperforming classical models such as logistic regression and random forests. Additionally, the system demonstrated an 82% prediction accuracy on 1,000 new test cases, underscoring its robustness in practical scenarios. Key innovations include an integrated recommendation engine, which utilizes cosine similarity to recommend relevant historical cases, and the use of GPU and TPU platforms for scalable model training and inference. By integrating state-of-the-art techniques in natural language processing and predictive analytics, this research sets a new benchmark for legal analytics, with significant implications for improving judicial efficiency and decision-making processes.","recorded":"2024-12-23 14:26:16.369957179","filePath":"null","pinned":false},{"value":"https://github.com/Chaganti-Reddy/CJPR","recorded":"2024-12-23 14:20:55.382254446","filePath":"null","pinned":false},{"value":"The judicial system is fundamental to democratic governance, serving as a mediator, conflict resolver, and guardian of societal fairness. This study introduces a Court Judgment Prediction and Recommendation (CJPR) system utilizing artificial intelligence to augment judicial efficiency, alleviate case backlogs, and boost decision quality. The CJPR system analyzes Supreme Court cases in India from 1947 to 2023, utilizing sophisticated machine learning models, including transformers, on GPU and TPU platforms. The methodological approach includes data preparation, feature extraction, and model evaluation utilizing measures such as accuracy, precision, and F1-score. The system attained an 82% prediction accuracy on 1,000 new test cases and a model accuracy of 77%, with an F1-score of 0.765 utilizing the RoBERTa model, markedly surpassing traditional models such as logistic regression and random forests. The CJPR algorithm predicts case outcomes and proposes pertinent past instances, offering legal practitioners practical information. This research introduces innovative features, including a recommendation engine and scalable transformer-based structures, marking a significant development in legal analytics that could improve judicial decision-making","recorded":"2024-12-23 14:16:22.645816403","filePath":"null","pinned":false},{"value":"The judicial system is fundamental to democratic governance, serving as a mediator, conflict resolver, and guardian of societal fairness. This study introduces a Court Judgment Prediction and Recommendation (CJPR) system utilizing artificial intelligence to augment judicial efficiency, alleviate case backlogs, and boost decision quality. The CJPR system analyzes Supreme Court cases in India from 1947 to 2023, utilizing sophisticated machine learning models, including transformers, on GPU and TPU platforms. The methodological approach includes data preparation, feature extraction, and model evaluation utilizing measures such as accuracy, precision, and F1-score. The system attained an 82% prediction accuracy on 1,000 new test cases and a model accuracy of 77%, with an F1-score of 0.765 utilizing the RoBERTa model, markedly surpassing traditional models such as logistic regression and random forests. The CJPR algorithm predicts case outcomes and proposes pertinent past instances, offering legal practitioners practical information. This research introduces innovative features, including a recommendation engine and scalable transformer-based structures, marking a significant development in legal analytics that could improve judicial decision-making.a","recorded":"2024-12-23 14:16:17.614983277","filePath":"null","pinned":false},{"value":"The judicial system is a cornerstone of democratic governance, acting as a mediator, conflict resolver, and protector of societal justice. This study proposes a Court Judgment Prediction and Recommendation (CJPR) system leveraging artificial intelligence to enhance judicial efficiency, reduce case backlogs, and improve decision quality. The CJPR system processes Supreme Court cases in India from 1947 to 2023, employing advanced machine learning models, including transformers, on GPU and TPU platforms. The methodological framework incorporates data preprocessing, feature extraction, and model evaluation using metrics such as accuracy, precision, and F1-score. The system achieved an 82% prediction accuracy on 1,000 new test cases and a model accuracy of 77%, with an F1-score of 0.765 using the RoBERTa model, significantly outperforming classical models like logistic regression and random forests. The CJPR system not only predicts case outcomes but also recommends relevant historical cases, providing legal professionals with actionable insights. By introducing novel features, such as a recommendation engine and scalable transformer-based architectures, this research represents a critical advancement in legal analytics with the potential to revolutionize judicial decision-making.","recorded":"2024-12-23 14:13:30.898419983","filePath":"null","pinned":false},{"value":":: removing hyprutils breaks dependency 'libhyprutils.so=1-64' required by hyprgraphics","recorded":"2024-12-23 13:17:47.759784071","filePath":"null","pinned":false},{"value":"dependency","recorded":"2024-12-23 13:17:47.580663827","filePath":"null","pinned":false},{"value":"ROG Zephyrus G16","recorded":"2024-12-22 17:05:04.909389148","filePath":"null","pinned":false},{"value":"Write a python code for changing from 1st person plural to 3rd person singular or using impersonal tone to maintain objectivity and professionalism. The input would be a paragraph.","recorded":"2024-12-22 15:58:52.126948233","filePath":"null","pinned":false},{"value":"adobe-source-code-pro-fonts noto-fonts noto-fonts-cjk noto-fonts-emoji ttf-hack ttf-jetbrains-mono ttf-ubuntu-font-family ttf-ubuntu-mono-nerd ttf-ubuntu-nerd ttf-opensans gnu-free-fonts --noconfirm \u0026\u0026 paru -S ttf-ms-fonts qt6ct-kde --noconfirm\n","recorded":"2024-12-19 01:10:17.948743551","filePath":"null","pinned":false},{"value":"mpv mpc mpd ncmpcpp mplayer poppler poppler-glib --noconfirm ","recorded":"2024-12-19 01:08:48.372727090","filePath":"null","pinned":false},{"value":"baobab gnome-disk-utility flameshot bc docker docker-compose docker-scan gparted libreoffice-fresh pavucontrol qutebrowser ranger yad timeshift --noconfirm\n\n","recorded":"2024-12-19 01:05:13.926076007","filePath":"null","pinned":false},{"value":"zen-browser-bin ccrypt didyoumean-git github-desktop-bin visual-studio-code-bin preload peerflix webtorrent-cli webtorrent-mpv-hook git-remote-gcrypt --noconfirm\n\n","recorded":"2024-12-19 01:03:27.334117715","filePath":"null","pinned":false},{"value":"base-devel intel-ucode git vim zsh zsh-completions zsh-autosuggestions zsh-syntax-highlighting bash-completion openssh wget curl btop fastfetch bat exa fd ripgrep fzf stow stylua tar tree time acpilight aria2 unrar unzip bluez bluez-utils brightnessctl xfsprogs ntfs-3g clang gcc clipmenu clipnotify inotify-tools psutils dunst e2fsprogs gvfs gvfs-afc gvfs-google gvfs-goa gvfs-gphoto2 gvfs-mtp gvfs-nfs gvfs-onedrive gvfs-smb efibootmgr zoxide gc git-lfs gnome-keyring polkit-gnome pass udiskie gstreamer jq xdotool screenkey xorg-xprop lazygit lolcat sxiv shellcheck net-tools numlockx prettier progress zip rsync trash-cli tlp tlp-rdw neovim xorg-xinput xclip xcompmgr xorg-xrandr xorg-xsetroot xsel xwallpaper pandoc starship python-pywal glow xarchiver xfce4-clipman-plugin qemu-full libguestfs xorg-xman man-db man-pages ncdu python-adblock dnsmasq python-pip nwg-look python-prctl vscode-css-languageserver ffmpegthumbnailer virt-manager spice-vdagent lua-language-server pass pinentry gnupg pass-otp zbar xorg-xlsclients xscreensaver os-prober qt5ct pamixer qt5-wayland qt6-wayland parallel shfmt tesseract html-xml-utils --noconfirm\n","recorded":"2024-12-19 00:59:28.066954979","filePath":"null","pinned":false},{"value":"# lines_to_append=\"ILoveCandy\\nParallelDownloads=10\\nColor\"\n# sudo sed -i '/^\\[options\\]/a '\"$lines_to_append\" /etc/pacman.conf\n\n# mkdir Downloads Documents Music Videos Pictures Desktop Git\n# Update the system\n# sudo pacman -Syu archlinux-keyring --noconfirm\n\n# 1. Install essential packages\n# sudo pacman -S base-devel intel-ucode git vim zsh zsh-completions zsh-autosuggestions zsh-syntax-highlighting bash-completion openssh wget curl btop fastfetch bat exa fd ripgrep fzf stow stylua tar tree time acpilight aria2 unrar unzip bluez bluez-utils brightnessctl xfsprogs ntfs-3g clang gcc clipmenu clipnotify inotify-tools psutils dunst e2fsprogs gvfs gvfs-afc gvfs-google gvfs-goa gvfs-gphoto2 gvfs-mtp gvfs-nfs gvfs-onedrive gvfs-smb efibootmgr zoxide gc git-lfs gnome-keyring polkit-gnome pass udiskie gstreamer jq xdotool screenkey xorg-xprop lazygit lolcat sxiv shellcheck net-tools numlockx prettier progress zip rsync trash-cli tlp tlp-rdw neovim xorg-xinput xclip xcompmgr xorg-xrandr xorg-xsetroot xsel xwallpaper pandoc starship python-pywal glow xarchiver xfce4-clipman-plugin qemu-full libguestfs xorg-xman man-db man-pages ncdu python-adblock dnsmasq python-pip nwg-look python-prctl vscode-css-languageserver ffmpegthumbnailer virt-manager spice-vdagent lua-language-server pass pinentry gnupg pass-otp zbar xorg-xlsclients xscreensaver os-prober qt5ct pamixer qt5-wayland qt6-wayland parallel shfmt tesseract html-xml-utils --noconfirm\n\n# Using XFCE4-CLIPMAN for clipboard manager\n\n# 2. Install yay\n# git clone https://aur.archlinux.org/paru.git\n# cd paru\n# makepkg -si\n# cd ..\n# rm -rf paru\n\n# 3. Install AUR packages\n# paru -S zen-browser-bin ccrypt didyoumean-git github-desktop-bin visual-studio-code-bin preload peerflix webtorrent-cli webtorrent-mpv-hook git-remote-gcrypt --noconfirm\n\n# 4. Install GUI packages\n# sudo pacman -S baobab gnome-disk-utility flameshot bc docker docker-compose docker-scan gparted libreoffice-fresh pavucontrol qutebrowser ranger yad timeshift --noconfirm\n\n# 5. Install multimedia packages\n# sudo pacman -S mpv mpc mpd ncmpcpp mplayer poppler poppler-glib --noconfirm \u0026\u0026 \n# yay -S ferdium-bin --noconfirm\n# sudo pacman -Rns kate\n\n# 6. Install fonts\n# sudo pacman -S adobe-source-code-pro-fonts noto-fonts noto-fonts-cjk noto-fonts-emoji ttf-hack ttf-jetbrains-mono ttf-ubuntu-font-family ttf-ubuntu-mono-nerd ttf-ubuntu-nerd ttf-opensans gnu-free-fonts --noconfirm \u0026\u0026 paru -S ttf-ms-fonts qt6ct-kde --noconfirm\n\n# 7. Install external packages\n# paru -S ani-cli-git arch-wiki-docs ytfzf-git  --noconfirm\n# yay -S walogram-git docker-desktop # Optional\n# sudo pacman -S yt-dlp go hugo hunspell hunspell-en_us imagemagick ueberzug luacheck mlocate newsboat nodejs npm texlive-bin texlive-meta texlive-latex texlive-basic translate-shell --noconfirm\n\n# 8. Mariadb setup\n# sudo pacman -S mariadb --noconfirm\n# sudo mysql_install_db --user=mysql --basedir=/usr --datadir=/var/lib/mysql\n# sudo systemctl enable --now mariadb\n# sudo mariadb-secure-installation\n\n# 9. Enable services\n# sudo updatedb\n# sudo mandb\n# sudo systemctl enable --now tlp\n# sudo systemctl enable --now bluetooth.service\n# sudo systemctl enable lightdm.service\n# sudo systemctl enable --now libvirtd\n\n# 10. Permissions\n# sudo usermod -aG docker $USER\n# sudo usermod -aG video $USER\n# sudo usermod -aG libvirt $USER\n# sudo virsh net-start default\n# For VM sharing https://docs.getutm.app/guest-support/linux/\n\n# 11. Setup git\n#git config --global user.name \"Chaganti-Reddy\"\n#git config --global user.email \"chagantivenkataramireddy4@gmail.com\"\n\n# 12. Alternatives \u0026 Optionals:\n# 1. Install Java\n#sudo pacman -S jdk-openjdk openjdk-doc openjdk-src --noconfirm\n# 2. Install qbit torrent\n# sudo pacman -S qbittorrent --noconfirm\n# 4. Install Teamviewer\n# paru -S teamviewer --noconfirm\n# 5. Install Zathura\n# sudo pacman -S zathura zathura-pdf-mupdf zathura-djvu zathura-ps zathura-cb --noconfirm \u0026\u0026 yay -S zathura-pywal-git --noconfirm\n# also install pywal zathura in ~/dotfiles/Extras/Extras/Zathura-Pywal-master/\n# 8. Install Thunar\n# sudo pacman -S thunar thunar-archive-plugin thunar-volman thunar-media-tags-plugin --noconfirm\n# 8. Install GTK theme and QT theme\n# yay -S elementary-icon-theme --noconfirm # Previously used icons\n# paru -S  gruvbox-dark-gtk whitesur-icon-theme whitesur-gtk-theme-git kvantum-theme-whitesur-git whitesur-cursor-theme-git kvantum kvantum-theme-otto-git \u0026\u0026 sudo pacman -S gtk-engine-murrine --noconfirm\n# 15. Install MINICONDA\n# wget https://repo.anaconda.com/miniconda/Miniconda3-py310_24.3.0-0-Linux-x86_64.sh\n# sh Miniconda3-py310_24.3.0-0-Linux-x86_64.sh\n# rm Miniconda3-py310_24.3.0-0-Linux-x86_64.sh\n# 9. Install anipy-cli\n# pip install anipy-cli\n# 10. Install Doom Emacs\n# sudo pacman -S emacs --noconfirm \u0026\u0026 git clone --depth 1 https://github.com/doomemacs/doomemacs ~/.config/emacs \u0026\u0026 ~/.config/emacs/bin/doom install # Later run doom sync\n# After that run ---- M-x nerd-icons-install-fonts\n# 11. Insatll waldl from Extras folder of dotfiles\n# cd  ~/dotfiles/Extras/Extras/waldl-master/ \u0026\u0026 sudo make install \u0026\u0026 cd ~/dotfiles\n# 12. Install ollama from Extras folder of dotfiles\n# sh ~/dotfiles/Extras/Extras/ollama.sh\n# ollama serve\n# ollama pull mistral\n# ollama pull gemma:7b\n# 13. Install brave Extensions\n# brave://extensions/ ---\u003e Install Comp Companion, uBlock Origin, GFG to Github, Google Translate, LeetHub, User-Agent switcher\n# 14. Install Bash Language Server\n# sudo npm i -g bash-language-server\n# 15. Setup zsh shell as default\n# chsh -s /bin/zsh\n# 16. Install oh-my-zsh\n# sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n# exit # exit from zsh\n# 17. Install floorp instead of firefox\n# paru -S zen-browser-bin --noconfirm\n# paru -S python-pywalfox --noconfirm\n# 18. Use dmenu for network manager (Optional)\n# yay -S networkmanager-dmenu-git\n# 19. Install Blender for Video Editing\n# sudo pacman -S blender --noconfirm\n# 20. Install OBS Studio for Screen Recording\n# sudo pacman -S obs-studio --noconfirm\n# 21. Install GIMP for Image Editing\n# sudo pacman -S gimp --noconfirm\n# 22. Install Inkscape for Vector Graphics\n# sudo pacman -S inkscape --noconfirm\n# 23. Install Octave for Numerical Computing\n# sudo pacman -S octave --noconfirm\n# 24. Install blueman if needed for bluetooth manager\n# sudo pacman -S blueman --noconfirm\n# sudo pacman -S discord --noconfirm\n\n# 13. Move Respective files to root directory\n# sudo mkdir /usr/share/xsessions/\n# sudo cp ~/dotfiles/Extras/Extras/usr/share/xsessions/dwm.desktop /usr/share/xsessions\n# # sudo cp -r ~/dotfiles/Extras/Extras/boot/grub/themes/mocha /boot/grub/themes/\n# sudo cp -r ~/dotfiles/Extras/Extras/boot/grub/themes/tartarus/ /boot/grub/themes/\n# Now edit the grub config file\n# sudo cp ~/dotfiles/Extras/Extras/etc/bash.bashrc /etc/\n# sudo cp ~/dotfiles/Extras/Extras/etc/DIR_COLORS /etc/\n# sudo cp ~/dotfiles/Extras/Extras/etc/mpd.conf /etc/\n# sudo cp ~/dotfiles/Extras/Extras/etc/nanorc /etc/\n# sudo cp ~/dotfiles/Extras/Extras/etc/environment /etc/\n# # sudo cp -r ~/dotfiles/Extras/Extras/etc/lightdm/ /etc/\n# cp ~/dotfiles/Extras/Extras/alanpeabody.zsh-theme ~/.oh-my-zsh/themes/\n# mkdir ~/.icons \u0026\u0026 cp -r ~/dotfiles/Extras/Extras/.icons/Capitaine/ ~/.icons/\n\n# 16. Install Fonts\n# mv my-fonts ~/.local/share/fonts/\n\n# 17. Install/stow dotfiles\n# First check for conflicts\n#rm -rf ~/.config/doom\n# rm -rf ~/.config/gtk-3.0\n# rm ~/.bashrc\n# rm ~/.zshrc\n# cd ~/dotfiles\n# stow */\n\n# 18. Install DWM\n# cd ~/.config/dwm \u0026\u0026 sudo make clean install \u0026\u0026 cd\n# cd ~/.config/slstatus \u0026\u0026 sudo make clean install \u0026\u0026 cd\n# cd ~/.config/st \u0026\u0026 sudo make install \u0026\u0026 cd\n# cd ~/.config/dmenu \u0026\u0026 sudo make install \u0026\u0026 cd\n\n# 19. Install Stockfish\n# wget https://github.com/official-stockfish/Stockfish/releases/latest/download/stockfish-ubuntu-x86-64-avx2.tar\n# tar -xvf stockfish-ubuntu-x86-64-avx2.tar\n# rm stockfish-ubuntu-x86-64-avx2.tar\n# mv stockfish ~/\n\n# Install python packages\n# pip install pynvim numpy pandas matplotlib seaborn scikit-learn jupyterlab ipykernel ipywidgets tensorflow python-prctl inotify-simple psutil opencv-python keras mov-cli-youtube mov-cli mov-cli-test otaku-watcher film-central daemon\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu # pytorch cpu version\n\n### HYPRLAND\n\n# sudo pacman -S kitty hyprland system-config-printer hyprpicker hyprlock chafa hypridle waybar wl-clipboard speech-dispatcher hyprpaper brightnessctl cmake meson cpio grim slurp rofi rofi-emoji rofi-calc wtype wf-recorder swaync zenity\n# paru -S wlrobs-hg  clipse-bin hyde-cli-git wlogout hyprshot-git \n# sudo pacman -S wofi\n# git clone https://github.com/dracula/wofi.git\n# sudo pacman -S thunar thunar-archive-plugin thunar-media-tags-plugin thunar-volman thunar-vcs-plugin\n#\n#\n#### SDDM \n# sudo pacman -S qt6-5compat qt6-declarative qt6-svg sddm \n\n# Video Download Helper \n# curl -sSLf https://github.com/aclap-dev/vdhcoapp/releases/latest/download/install.sh | bash\n#\n# Open in neovim from Thunar \n# sudo cp ~/dotfiles/Extras/Extras/nvim.desktop /usr/share/applications/\n#\n# Compiler.nvim for neovim compilation --\u003e Choose what you need \n# paru -S --needed \"gcc\" \"binutils\" \"dotnet-runtime\" \"dotnet-sdk\" \"aspnet-runtime\" \"mono\" \"jdk-openjdk\" \"dart\" \"kotlin\" \"elixir\" \"npm\" \"nodejs\" \"typescript\" \"make\" \"go\" \"nasm\" \"r\" \"nuitka\" \"python\" \"ruby\" \"perl\" \"lua\" \"pyinstaller\" \"swift-language\" \"flutter-bin\" \"gcc-fortran\" \"fortran-fpm-bin\"","recorded":"2024-12-19 00:56:32.890750219","filePath":"null","pinned":false},{"value":"/home/godzeus/scripts","recorded":"2024-12-19 00:53:17.608076133","filePath":"null","pinned":false},{"value":"/home/godzeus/dotfiles/install.sh","recorded":"2024-12-19 00:51:54.890574809","filePath":"null","pinned":false},{"value":"[godzeus@arch dotfiles]$ stow install.sh \nstow: ERROR: The stow directory dotfiles does not contain package install.sh","recorded":"2024-12-19 00:49:52.746557281","filePath":"null","pinned":false},{"value":"go","recorded":"2024-12-19 00:49:46.935033742","filePath":"null","pinned":false},{"value":"/home/godzeus/install.sh","recorded":"2024-12-19 00:46:14.395849776","filePath":"null","pinned":false},{"value":"Eavesdropping attacks","recorded":"2024-12-18 13:10:36.404580627","filePath":"null","pinned":false},{"value":"Taxonamy Of Threats to IOT Networks","recorded":"2024-12-18 12:25:51.964472652","filePath":"null","pinned":false},{"value":"Layered IoT Architecture","recorded":"2024-12-18 12:10:59.807552358","filePath":"null","pinned":false},{"value":"Cellular","recorded":"2024-12-18 11:35:12.220004468","filePath":"null","pinned":false},{"value":"Near Field Communication (NFC)","recorded":"2024-12-18 11:31:57.252946551","filePath":"null","pinned":false},{"value":"Zigbee","recorded":"2024-12-18 11:27:40.015125107","filePath":"null","pinned":false},{"value":"Bluetooth","recorded":"2024-12-18 11:23:10.726323389","filePath":"null","pinned":false},{"value":"Wi-Fi","recorded":"2024-12-18 11:17:06.675787988","filePath":"null","pinned":false},{"value":"FUNDAMENTALS OF IoT SECURITY","recorded":"2024-12-18 11:11:03.921743967","filePath":"null","pinned":false},{"value":"IoT Frameworks and Complexity","recorded":"2024-12-18 11:02:27.824978601","filePath":"null","pinned":false},{"value":"Trusted IoT Networks and the Network Edge","recorded":"2024-12-18 10:54:57.603296341","filePath":"null","pinned":false},{"value":"Security Challenges in Constrained Environments","recorded":"2024-12-18 10:53:48.221716700","filePath":"null","pinned":false},{"value":"Moore's Law and Constrained Computing","recorded":"2024-12-18 10:50:06.109068056","filePath":"null","pinned":false},{"value":"The IoT Pyramid and Attack Surface","recorded":"2024-12-18 10:45:30.807864575","filePath":"null","pinned":false},{"value":"IoT Layering and Security Challenges","recorded":"2024-12-18 10:44:44.037298484","filePath":"null","pinned":false},{"value":"Designing Safe and Secure Cyber-Physical Systems","recorded":"2024-12-18 10:44:05.449282720","filePath":"null","pinned":false},{"value":"Stuxnet","recorded":"2024-12-18 10:30:47.700944931","filePath":"null","pinned":false},{"value":"Air-Gap Security","recorded":"2024-12-18 10:10:32.074885596","filePath":"null","pinned":false},{"value":"The BadUSB Threat","recorded":"2024-12-18 09:59:35.341053122","filePath":"null","pinned":false},{"value":"\"format-icons\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"],","recorded":"2024-12-17 18:21:32.498022268","filePath":"null","pinned":false},{"value":"        \"format-charging\": \" {capacity}%\",\n","recorded":"2024-12-17 18:20:32.095561951","filePath":"null","pinned":false},{"value":"        \"format-critical\": \" {capacity}%\",\n","recorded":"2024-12-17 18:20:28.749875689","filePath":"null","pinned":false},{"value":"        \"format-warning\": \" {capacity}%\",\n","recorded":"2024-12-17 18:20:26.334422396","filePath":"null","pinned":false},{"value":"        \"format-full\": \" {capacity}%\",\n","recorded":"2024-12-17 18:20:25.182841379","filePath":"null","pinned":false},{"value":"        \"tooltip-format-charging\": \"Charging: {timeTo}\",\n","recorded":"2024-12-17 18:19:12.665887968","filePath":"null","pinned":false},{"value":"            \"good\": 99,\n","recorded":"2024-12-17 18:18:45.142111203","filePath":"null","pinned":false},{"value":"            \"full\": 100,\n","recorded":"2024-12-17 18:18:43.931093023","filePath":"null","pinned":false},{"value":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","recorded":"2024-12-17 18:09:30.794607117","filePath":"null","pinned":false},{"value":"/home/godzeus/Downloads/rofi/config.rasi","recorded":"2024-12-17 17:14:22.682233629","filePath":"null","pinned":false},{"value":"/**\n * ROFI Color theme\n * NAME: dt-center.rasi\n * DESCRIPTION: This is a centered prompt.\n * AUTHOR: Derek Taylor (DT)\n */\n\n* {\n    background-color:            #282c34;\n    border-color:                #282c34;\n    text-color:                  #bbc2cf;\n    font:                        \"SauceCodePro Nerd Font Mono 11\";\n    prompt-font:                 \"Ubuntu Bold 9\";\n    prompt-background:           #51afef;\n    prompt-foreground:           #282c34;\n    prompt-padding:              4px;\n    alternate-normal-background: #1c1f24;\n    alternate-normal-foreground: @text-color;\n    selected-normal-background:  #ae3f3e;\n    selected-normal-foreground:  #ffffff;\n    spacing:                     3;\n}\n#window {\n    border:  1;\n    padding: 5;\n}\n#mainbox {\n    border:  0;\n    padding: 0;\n}\n#message {\n    border:       1px dash 0px 0px ;\n    padding:      1px ;\n}\n#listview {\n    fixed-height: 0;\n    border:       2px dash 0px 0px ;\n    spacing:      2px ;\n    scrollbar:    true;\n    padding:      2px 0px 0px ;\n}\n#element {\n    border:  0;\n    padding: 1px ;\n}\n#element.selected.normal {\n    background-color: @selected-normal-background;\n    text-color:       @selected-normal-foreground;\n}\n#element.alternate.normal {\n    background-color: @alternate-normal-background;\n    text-color:       @alternate-normal-foreground;\n}\n#scrollbar {\n    width:        0px ;\n    border:       0;\n    handle-width: 0px ;\n    padding:      0;\n}\n#sidebar {\n    border: 2px dash 0px 0px ;\n}\n#button.selected {\n    background-color: @selected-normal-background;\n    text-color:       @selected-normal-foreground;\n}\n#inputbar {\n    spacing:    0;\n    padding:    1px ;\n}\n#case-indicator {\n    spacing:    0;\n}\n#entry {\n    padding: 4px 4px;\n    expand: false;\n    width: 10em;\n}\n#prompt {\n    padding:          @prompt-padding;\n    background-color: @prompt-background;\n    text-color:       @prompt-foreground;\n    font:             @prompt-font;\n    border-radius:    2px;\n}\n\nelement-text {\n    background-color: inherit;\n    text-color:       inherit;\n}\n\n/* Not actually relevant for this configuration, but it might\nbe useful to someone having issues with their icons' background color\n\nelement-icon {\n    background-color: inherit;\n}\n*/\n","recorded":"2024-12-17 17:14:02.565545700","filePath":"null","pinned":false},{"value":"/home/godzeus/.config/rofi.zip","recorded":"2024-12-17 17:13:02.205466086","filePath":"null","pinned":false},{"value":"git push -u origin main","recorded":"2024-12-17 17:05:09.267840861","filePath":"null","pinned":false},{"value":"git remote add origin git@github.com:reddy-bhavesh/dotfiles.git","recorded":"2024-12-17 17:05:04.102301772","filePath":"null","pinned":false},{"value":"git branch -M main","recorded":"2024-12-17 17:04:59.328818057","filePath":"null","pinned":false},{"value":"git remote add origin https://github.com/reddy-bhavesh/dotfiles.git","recorded":"2024-12-17 17:02:21.252500743","filePath":"null","pinned":false},{"value":"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOZjgp2Q7J7/K096moab2DYHPCKgiAxGHL2AYXpSnBOS bhaveshmandy6321@gmail.com","recorded":"2024-12-17 17:00:30.612108174","filePath":"null","pinned":false},{"value":" bhaveshmandy6321@gmail.com ","recorded":"2024-12-17 17:00:04.051499123","filePath":"null","pinned":false},{"value":"ssh-keygen -t ed25519 -C","recorded":"2024-12-17 16:59:23.599574638","filePath":"null","pinned":false},{"value":"/home/godzeus/Downloads/dunstrc","recorded":"2024-12-17 16:55:41.489961677","filePath":"null","pinned":false},{"value":"/home/karna/dotfiles/Hyprland/.config/clipse\n/home/karna/dotfiles/Hyprland/.config/hypr\n/home/karna/dotfiles/Hyprland/.config/kitty\n/home/karna/dotfiles/Hyprland/.config/waybar\n/home/karna/dotfiles/Hyprland/.config/wlogout","recorded":"2024-10-28 17:43:21.269372785","filePath":"null","pinned":false},{"value":"Savita Shetty et al. \u0026 2022 \u0026 Resnet34 with modified unet for multicalss segmentation was applied \u0026 The developed method provide accuracy of up to 80\\% and F! score of 0.96 \u0026 A better segmentation is required \\\\ \\hline","recorded":"2024-10-28 16:26:27.615481963","filePath":"null","pinned":false},{"value":"Author\nMesfer Al\nDuhayyimal.,\net\nQirui Huang et\nal.,\nHuan Ding et\nal.\nSavitaet al.\nShetty\nYear\n2023\n2023\n2023\n2022\nTechniques\n Findings\n Limitations\nSO with\n Hybrid model\n The dataset\nFusion based\n provided F1\n used was\nClassification\n score of 94.5\n imbalance\n(CADOC-\nSFOFC)\nmodel\nDL algorithm\n Proposed\n complexity of\nbased on\n method\n algorithm was\nmetaheuristic\n provided an\n increased\napproach\n Precision of\n92.66%\nModified\n Model\n SVM could be\nLocust Swarm\n provided\n replaced by a\noptimization\n 92.37%\n better deep\nalgorithm\n specificity and\n learning model\n96.94%\naccuracy rate\nResnet34 with\n The developed\n A better\nModified Unet\n method\n segmentation\nfor Multiclass\n provide\n is required\nsegmentation\n accuracy of up\nwas applied\n to 80%. and\nF1 score of\n0.96","recorded":"2024-10-28 16:16:21.109461820","filePath":"null","pinned":false},{"value":"Mesfer Al\nDuhayyimal.,\net","recorded":"2024-10-28 16:09:45.460351262","filePath":"null","pinned":false},{"value":"https://www.amazon.in/Ant-Esports-GM320-Programmable-Comfortable/dp/B08D64C9FN/ref=sr_1_3?crid=2YOXVOFMYM32A\u0026dib=eyJ2IjoiMSJ9.P9tM3t-kfvm2ttoxInCwWw5tFmryGmXYs_iHbCJSK651F_hjVVvM2AaUvN89mujyvX0Wbfv4SFNd1nr7kEjJ56uWxXL---UZdcTUt4757Vdfz0vulcD2SE3vXppB1V89XopMGjxZx5otZJYidzeYKBvjy58cRoD_Zxo-wefN6Tff9iVS1SW6a5kzs_h6ZZXyqDivrOPC5-SOLpHRxipTTMtfffQM_u6mKC1Pa6qfhis.jYvwtSPHJx-m2MpuW3cGSoNLDPLedXf0eP_MwXi1Sy8\u0026dib_tag=se\u0026keywords=gm320%2Bmouse%2Bant%2Besports\u0026nsdOptOutParam=true\u0026qid=1730111425\u0026sprefix=GM320%2B%2Caps%2C242\u0026sr=8-3\u0026th=1","recorded":"2024-10-28 16:01:19.860492105","filePath":"null","pinned":false},{"value":"https://www.amazon.in/Redgear-MP35-Control-Type-Gaming-Mousepad/dp/B01J1CFO30/ref=sr_1_3?crid=1HVRWVPHT8SWY\u0026dib=eyJ2IjoiMSJ9.PbgGtSoE-fyIRhkldtZn9pB2BSB9adwVMkrJQ5g2joDjX4kz1XeLUyr5HrfWb1EX-KnYAGQjxJFyV-vHXt56YE_aI2yFo5iIxEmVRK1ogz_zVLLynlmDZ7W_yr69CGqkGha2RCWszRQkgsxcUi8tr_37nBtj3FkmU9yk3nOCdIN1UqXjA5NdPyre57M3idcKwed0zC-NmJObF--5YYz1Ho2tyDTr3YfWffL9rjUSNy8.B8AtF1CS6PU1xt9GWS5SOK_DVJc7lyB76ATl8B85RQ8\u0026dib_tag=se\u0026keywords=mousepad+redgear\u0026nsdOptOutParam=true\u0026qid=1730111157\u0026sprefix=mousepad+redgear%2Caps%2C228\u0026sr=8-3","recorded":"2024-10-28 15:56:32.599575444","filePath":"null","pinned":false},{"value":"\\begin{table}[H]\n    \\centering\n    \\caption{Summary of studies on Ensemble Techniques for oral cancer detection} \\label{Table_03}\n    \\addtolength{\\tabcolsep}{1.5pt}\n    \\renewcommand{\\arraystretch}{1.5}\n    \\begin{tabular}{|p{2.5cm}|p{0.8cm}|p{3.8cm}|p{3.5cm}|p{3.5cm}|}\n    \\hline\n    \\textbf{Author} \u0026 \\textbf{Year} \u0026 \\textbf{Techniques} \u0026 \\textbf{Findings} \u0026 \\textbf{Limitations} \\\\ \\hline\n    Heba M. Afify et al. \u0026 2023 \u0026 Deep learning models with the use of GRAD-CAM \u0026 EfficientNet-B0 achieved an accuracy of 95\\% \u0026 Class imbalance and time complexity were not taken into consideration \\\\ \\hline\n    Mathis Ersted Rasmussen et al. \u0026 2023 \u0026 A single-cycle interactive segmentation model \u0026 The medians of the dice rose with single-cycle segmentation in the range of 0.004 to 0.009 \u0026 The CT-only model failed to predict 36 organs-at-risk \\\\ \\hline\n    Natheer Al Rawi et al. \u0026 2022 \u0026 AI-based model with a modified cross-entropy loss function \u0026 The accuracy ranged from 43.5\\% to 100\\%, sensitivity from 94\\% to 100\\%, and specificity from 96\\% to 100\\% for the proposed model \u0026 The presence of modal noise reduced the effectiveness of this technique to detect oral cancer \\\\ \\hline\n    Nanditha B R et al. \u0026 2021 \u0026 An ensemble model with benefits of ResNet50 and VGG-16 \u0026 Ensemble model provided 96.2\\% accuracy, 98.14\\% sensitivity, and 94.23\\% specificity \u0026 The computational time of the model was very high \\\\ \\hline\n    \\end{tabular}\n\\end{table}","recorded":"2024-10-28 15:36:23.011849199","filePath":"null","pinned":false},{"value":"Lin et al. (2021) introduced a robust picture diagnostic method for cellphones via a deep learning system. A centered rule image-capturing method was proposed for acquiring images of the mouth cavity. A medium-sized oral dataset comprising five categories of illnesses was created to mitigate the effects of picture variability from hand-held smartphone cameras. A newly created deep learning network (HRNet) was utilized to evaluate the efficacy of our oral cancer detection technology. The proposed technique achieved an F1 score of 83.6%, a sensitivity of 83.0%, a specificity of 96.6%, and an accuracy of 84.3% on 455 test images.The newly introduced HRNet exhibited somewhat superior performance compared to VGG, ResNet, and DenseNet for sensitivity, specificity, accuracy, and F1 score.","recorded":"2024-10-28 15:35:01.639073703","filePath":"null","pinned":false},{"value":"(Lin et al. 2021) presented a powerful deep learning algorithm-based image diagnosis\napproach for smartphones. For gathering images of the oral cavity, a centred rule image-\ncapturing method was suggested. In order to lessen the impact of image variability from\nhand-held smartphone cameras, a medium-sized oral dataset with five types of disorders\nwas constructed. A recently developed deep learning network (HRNet) was used to assess\nhow well our technique for detecting oral cancer performed. On 455 test photos, the\nproposed technique performed with an F1 of 83.6%, a sensitivity of 83.0%, a specificity\nof 96.6%, and an accuracy of 84.3%.In terms of the parameters of sensitivity, specificity,\naccuracy, and F1, the newly presented HRNet performed marginally better than VGG,\nResNet, and DenseNet.","recorded":"2024-10-28 15:34:51.515777400","filePath":"null","pinned":false},{"value":"Song et al. (2021) suggested a deep learning model utilizing a severely unbalanced dataset. Three thousand eight hundred fifty-one images of the buccal mucosa in polarized white light were obtained using a custom-designed oral cancer screening device. To improve the neural network's efficacy in classifying oral cancer images from imbalanced multi-class datasets obtained from high-risk populations during screenings in low-resource environments, the researchers employed weight balancing, data augmentation, undersampling, focal loss, and ensemble techniques. The detection success rate of the minority classes, which were initially challenging to distinguish, improved with the application of data-level and algorithm-level techniques in deep learning training. The experiment's results demonstrated that rectifying class imbalance enhanced the efficacy of the proposed technique, yielding an AUC of up to 0.93.","recorded":"2024-10-28 15:34:46.070323350","filePath":"null","pinned":false},{"value":"were initially difficult to differentiate, was enhanced by using data-level and algorithm-\nlevel methods to deep learning training. The experiments findings proved that eliminating\nclass imbalance helped make the proposed strategy more effective, providing an AUC of\nup to 0.93.","recorded":"2024-10-28 15:34:35.326158474","filePath":"null","pinned":false},{"value":"(Song et al. 2021) proposed a deep learning model on a dataset which was highly\nimbalanced. 3851 pictures of the cheek mucosa in polarised white light were captured\nby a specially made oral cancer screening tool. In order to enhance the neural networks\nperformance of oral cancer image classification with the imbalanced multi-class datasets\ncollected from high-risk populations during oral cancer screening in low-resource set-\ntings, the researchers used weight balancing, data augmentation, under sampling, focal\nloss, and ensemble methods. The success rate of detection of the minority classes, which","recorded":"2024-10-28 15:34:29.284852159","filePath":"null","pinned":false},{"value":"Al Duhayyim et al. (2023) present the Sailfish Optimisation with Fusion Based Classification (CADOC-SFOFC) model, a distinctive Computer Aided Diagnosis system for the identification of oral cancer. The detection of oral cancer in clinical photographs is facilitated by the proposed CADOC-SFOFC model. This is achieved by utilizing the VGGNet16 and Residual Network (ResNet) models in a fusion-based feature extraction methodology. The integrated feature vectors were subsequently sent to an extreme learning machine for categorization. The SFO approach was selected for parameter optimization of the ELM machine. The proposed strategy, when applied to a publically available dataset, yielded an average F1 score of 94.51 for the categorization of oral cancer images.","recorded":"2024-10-28 15:34:20.484945182","filePath":"null","pinned":false},{"value":"(Al Duhayyim et al. 2023) proposes the Sailfish Optimisation with Fusion Based\nClassification (CADOC-SFOFC) model, it is a unique Computer Aided Diagnosis for\ndetection of oral cancer. The presence of oral cancer on clinical images is determined by\nthe suggested CADOC-SFOFC model. This is accomplished through the employment of\nthe VGGNet16 and Residual Network (ResNet) model in a fusion-based feature extraction\napproach. The fused feature vectors were then provided to an extreme learning machine\nfor classification. SFO method was chosen for parameter selection of ELM machine. The\nproposed method when applied to a publicly available dataset provided an average F1\nscore of 94.51 for classification of oral cancer images.","recorded":"2024-10-28 15:34:12.669688966","filePath":"null","pinned":false},{"value":"Shetty and Patil (2023) suggested a model for detecting oral cancer in a distributed cloud environment with an optimized ensemble. The proposed model employed Improved Linear Discriminant Analysis (ILDA) for feature extraction, which mitigated overfitting and enhanced accuracy, while also decreasing training time. The proposed model integrated a Multi-layer Perceptron (MLP) with a Support Vector Machine (SVM). The model was trained using 1,224 histological images of oral cancer captured at various magnifications. The dataset was subsequently divided into uniformly sized segments, and the model was employed. The developed approach yields an accuracy of up to 80%.","recorded":"2024-10-28 15:33:55.064836058","filePath":"null","pinned":false},{"value":"(Shetty and Patil 2023) proposed a Model for detecting oral cancer in a distributed\ncloud setting using an optimised ensemble. The suggested model was using Improved\nLinear Discriminant Analysis (ILDA) for feature extraction which helped in reducing the\nover fitting and increased the accuracy, also the suggested feature extraction reduced the\ntraining time. The proposed model was a combination of Multi-layer Perceptron (MLP)\nwith Support Vector Machine (SVM). The model was trained on 1224 histological photos\nof oral cancer taken at different magnifications. The dataset was then split into equally\nsized pieces, and the model was used. The developed method provide accuracy of up to\n80%.","recorded":"2024-10-28 15:33:46.216702961","filePath":"null","pinned":false},{"value":"Ding, Huang, and Rodriguez (2023) suggested a system for oral cancer diagnosis that incorporates Reinforcement Learning for image segmentation, the Gabor wavelet technique for feature extraction, and an RBF-kernel-based SVM for classification. Employing an enhanced metaheuristic called the Modified Locus Swarm Optimization (MLSO) method, the optimal attributes were selected. The classification phase similarly employs this methodology to furnish the SVM with the optimal configuration contingent upon the kernel. The \"Oral Cancer images\" dataset is utilized to assess the efficacy of the proposed method. The simulation results indicate that the proposed method achieved a 96.94% accuracy rate, exhibiting the lowest error ratio in comparison to other analogous methods. The results indicate that the proposed method exhibits 92.37% specificity and 93.89% sensitivity.","recorded":"2024-10-28 15:33:30.141395707","filePath":"null","pinned":false},{"value":"Swarm Optimization (MLSO) algorithm, the best characteristics were chosen. The clas-\nsification stage also uses this approach to provide the SVM with the best configuration\npossible based on the kernel. The \"Oral Cancer images\" dataset is used to validate the ef-\nfectiveness of the suggested technique. According to the simulation findings, the proposed\nmethod, which had a 96.94% accuracy rate, had the lowest error ratio when compared to\nother comparable methods. Additionally, the findings show that the suggested approach\nhas 92.37% specificity and 93.89% sensitivity.","recorded":"2024-10-28 15:33:00.827337320","filePath":"null","pinned":false},{"value":"( Ding, Huang, and Rodriguez 2023) proposed a methodology for oral cancer detec-\ntion which include the Reinforcement Learning technique for picture segmentation, Using\nthe Gabor wavelet technique to extract picture features and an RBF-kernel-based SVM to\nclassify the results. Utilising an improved metaheuristic known as the Modified Locus","recorded":"2024-10-28 15:32:54.969348460","filePath":"null","pinned":false},{"value":"Huang, Ding, and Razmjooy (2023) devised an innovative way utilizing a metaheuristic approach and deep learning to create a dependable cancer diagnostic tool. Three preprocessing procedures were employed to enhance the quality and quantity of the raw images, providing sufficient data for the training of convolutional neural networks. A public dataset concerning oral cancer was utilized for model training. The mouth Cancer pictures dataset comprises 131 images of mouth cancer, captured at several ENT institutions and meticulously classified. ISSA was employed for weight optimization. The optimization is derived from the foraging techniques of flying squirrels. The ISSA algorithm, when integrated with CNN, elevates the algorithm's complexity. The proposed strategy achieved a precision of 92.66% in classifying oral cancer images.","recorded":"2024-10-28 15:31:10.298980753","filePath":"null","pinned":false},{"value":"( Huang, Ding, and Razmjooy 2023) developed a novel strategy based on a metaheuristic\napproach and deep learning to make a reliable cancer diagnosis tool. three preprocessing\nstrategies were implemented to improve the quality and quantity of the raw images to\ngive enough data for convolutional neural network training. A public dataset on oral\ncancer was taken for training the model. The Oral Cancer photographs dataset consists of\n131 oral cancer photographs that were photographed at various ENT hospitals and were\nexpertly categorised. ISSA was used for the optimization of the weights. The optimization\nis based on flying squirrels and there foraging techniques. ISSA algorithm combined with\nCNN increases the complexity of algorithm. Proposed method provided an Precision of\n92.66% while classifying oral cancer images.","recorded":"2024-10-28 15:30:55.843717235","filePath":"null","pinned":false},{"value":"Oral Cancer detection using meta heuristic optimizer","recorded":"2024-10-28 15:30:25.897362993","filePath":"null","pinned":false},{"value":"Summary of studies on deep learning techniques for oral cancer detection","recorded":"2024-10-28 15:14:55.033781638","filePath":"null","pinned":false},{"value":"\\begin{tabular}{|p{2.5cm}|p{0.8cm}|p{3.8cm}|p{3.5cm}|p{3.5cm}|}","recorded":"2024-10-28 15:14:21.281636709","filePath":"null","pinned":false},{"value":"\\begin{table}[H]\n    \\centering\n    \\caption{Recent Studies on Deep Learning Techniques for Oral Cancer Detection (Continuation)} \\label{Table_03}\n    \\addtolength{\\tabcolsep}{1.5pt}\n    \\renewcommand{\\arraystretch}{1.5}\n    \\begin{tabular}{|p{4cm}|p{1cm}|p{4cm}|p{4cm}|p{4cm}|}\n    \\hline\n    \\textbf{Author} \u0026 \\textbf{Year} \u0026 \\textbf{Techniques} \u0026 \\textbf{Findings} \u0026 \\textbf{Limitations} \\\\ \\hline\n    Heba M. Afify et al. \u0026 2023 \u0026 Deep learning models with the use of GRAD-CAM \u0026 EfficientNet-B0 achieved an accuracy of 95\\% \u0026 Class imbalance and time complexity were not taken into consideration \\\\ \\hline\n    Mathis Ersted Rasmussen et al. \u0026 2023 \u0026 A single-cycle interactive segmentation model \u0026 The medians of the dice rose with single-cycle segmentation in the range of 0.004 to 0.009 \u0026 The CT-only model failed to predict 36 organs-at-risk \\\\ \\hline\n    Natheer Al Rawi et al. \u0026 2022 \u0026 AI-based model with a modified cross-entropy loss function \u0026 The accuracy ranged from 43.5\\% to 100\\%, sensitivity from 94\\% to 100\\%, and specificity from 96\\% to 100\\% for the proposed model \u0026 The presence of modal noise reduced the effectiveness of this technique to detect oral cancer \\\\ \\hline\n    Nanditha B R et al. \u0026 2021 \u0026 An ensemble model with benefits of ResNet50 and VGG-16 \u0026 Ensemble model provided 96.2\\% accuracy, 98.14\\% sensitivity, and 94.23\\% specificity \u0026 The computational time of the model was very high \\\\ \\hline\n    \\end{tabular}\n\\end{table}\n","recorded":"2024-10-28 15:13:44.708515385","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/102848.png","recorded":"2024-10-28 15:11:55.381553844","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/102806.png","recorded":"2024-10-28 15:11:20.548414152","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/102835.png","recorded":"2024-10-28 15:10:43.748238779","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/102856.png","recorded":"2024-10-28 15:10:29.479990085","filePath":"null","pinned":false},{"value":"Welikala et al. (2020) introduced an ensemble deep learning model that integrates ResNet 101 and R-CNN for the categorization of oral cancer photos. This model was designed for mobile oral screening, capable of capturing real photos and classifying oral lesions accordingly. We employed various metrics, such as accuracy, precision, and F1-score, to assess the established approach. The results indicated that the proposed method achieved an F1 score of 87% for item identification, whereas the score for object detection diminished to just 41.8%.","recorded":"2024-10-28 15:08:54.305597216","filePath":"null","pinned":false},{"value":"(Welikala et al. 2020) proposed an ensemble deep learning model combining resnet\n101 and R cnn for classification of oral cnacer images. This model was developed to\nbe used for mobile mouth screening anywhere, this model take real images and classify\noral lesion from it. We used a variety of metrics, including accuracy, precision, and F1-\nscore, to evaluate the established method. The outcomes showed that the proposed method\nproduced F1 score of 87%, for object identification furthermore for object detection the\nscore was reduced to a mere 41.8%","recorded":"2024-10-28 15:08:45.788498572","filePath":"null","pinned":false},{"value":"Nanditha et al. (2021) established a computerized system for the automatic detection of oral cancer, proposing an ensemble model that combines ResNet50 and VGG-Skip. By integrating these networks, one may distinguish between images utilizing extracted features and a classifier. Images of oral lesions were collected from several colleges and hospitals in Karnataka. A total of 332 photographs of oral lesions were collected, comprising 269 precancerous lesions and 63 benign lesions. The proposed model outperforms other commonly used deep learning models in classifying oral pictures. The ensemble deep learning model achieved an accuracy of 96.2%, sensitivity of 98.14%, and specificity of 94.23%.","recorded":"2024-10-28 15:08:31.227643335","filePath":"null","pinned":false},{"value":"performs better than other widely utilised deep learning models when it comes to clas-\nsifying oral images. Using the ensemble deep learning model, 96.2% accuracy, 98.14%\nsensitivity, and 94.23% specificity were attained.","recorded":"2024-10-28 15:08:21.627449587","filePath":"null","pinned":false},{"value":"(Nanditha et al. 2021) developed a computerised system for automatic detecting oral\ncancer, an ensemble model was suggested with combination of resnet50 and vgg-skip, By\ncombining these networks, it is possible to differentiate between images using extracted\ncharacteristics and a classifier. Images of oral lesions were gathered from several col-\nleges and hospitals in Karnataka. A total of 332 photos of oral lesions wete gathered, of\nwhich 269 were precancerous lesions and 63 were benign lesions. The suggested model","recorded":"2024-10-28 15:08:13.668868723","filePath":"null","pinned":false},{"value":"Al-Rawi et al. (2022) suggested an AI-based model for diagnosing oral cancer, utilizing the ReLU activation function alongside a modified cross-entropy loss function. The model's dataset was sourced from four repositories: PubMed, Scopus, EBSCO, and OVID.\nThe prediction framework was assessed for its applicability and potential bias risk. The collection has a total of 7,245 patients and 69,425 images. The research assessed the efficacy of AI employing ten statistical methods. The accuracy varied between 43.5% and 100%, sensitivity ranged from 94% to 100%, specificity ranged from 96% to 100%, and an AUC of 93% was noted based on the findings of supervised machine learning.","recorded":"2024-10-28 14:54:00.489743217","filePath":"null","pinned":false},{"value":"S. Panigrahi et al. (2023) developed an innovative approach employing transfer learning and a suggested CNN model to concentrate on the binary categorization of oral histopathology images. Improving the pretrained VGG, ResNet, Inception, and MobileNet involves training half of the layers while keeping the remaining layers frozen. The experimental results indicate that ResNet50, with an accuracy of 96.6%, outperforms several fine-tuned DCNN models and the proposed baseline model significantly.","recorded":"2024-10-28 14:53:48.559240104","filePath":"null","pinned":false},{"value":"(Al-Rawi et al. 2022) proposed an AI based model for the diagnosis of oral can-\ncer which used the Relu activation function with a modified cross entropy loss function,\ndataset for the model was taken from 4 datasets(PubMed, Scopus, EBSCO, and OVID).\nUsing the prediction framework as a potential bias assessment tool, the applicability and\nrisk of bias were evaluated. The dataset consist a total of 7245 patients and 69,425 im-\nages. In the included research, the effectiveness of AI was evaluated using ten statistical\ntechniques. The accuracy ranged from 43.5% to 100%, the sensitivity ranged from 94%\nto 100%, the specificity ranged from 96% to 100%, and the AUC of 93% according to the\nresults of supervised machine learning was observed.","recorded":"2024-10-28 14:53:44.909675004","filePath":"null","pinned":false},{"value":"(S. Panigrahi et al. 2023) proposed a novel strategy that utilises transfer learning\nand a recommended CNN model to focus on binary classification of oral histopathology\npictures. By training half of the layers and leaving the other layers frozen, the pretrained\nVGG, ResNet, Inception, and MobileNet are improved. The experimental results show\nthat ResNet50, which has an accuracy of 96.6%, performs significantly better than a few\nfine-tuned DCNN models and the baseline model proposed.","recorded":"2024-10-28 14:53:38.337017325","filePath":"null","pinned":false},{"value":"Rasmussen et al. (2023) suggested a singular cycle dynamic segmentation model utilizing CT and the thickest cranial and caudal slices for each of the 16 organs most susceptible to head and neck cancer. This research included data from 730 planning CTs and clinical characteristics of patients with head and neck cancer. Ninety percent of the dataset was allocated for training, whereas ten percent was designated for an independent test set. Employing nnUNet single folds with default parameters. The models were evaluated utilizing the Dice similarity coefficient, the 95th percentile of the Hausdorff distance, and the average symmetric surface distance.\nThe medians of the dice in the proposed framework increased with single-cycle interactive segmentation, ranging from 0.004 to 0.009.","recorded":"2024-10-28 14:53:31.642686083","filePath":"null","pinned":false},{"value":"(Rasmussen et al. 2023) proposed a single cycle dynamic segmentation model using CT\nand the thickest cranial and caudal slices for each of the 16 organs most at risk for head\nand neck cancer. 730 planning CTs and clinical contours from patients who had head\nand neck cancer treated made up the data for this research. 90% of the data set was used\nfor training, and 10% was used for a separate test set. Using nnUNet single folds and\ndefault parameters. The models were compared using the Dice similarity coefficient, the\n95th percentile of the Hausdorff distance, and the average symmetric surface distance.\nThe medians of the dice in the proposed framework rose with single-cycle interactive\nsegmentation in the range of 0.004 to 0.009.","recorded":"2024-10-28 14:53:14.250694910","filePath":"null","pinned":false},{"value":"Afify, Mohammed, and Hassanien (2023) introduced an innovative deep learning model that use gradient class activation mapping to predict OSCC pictures. The proposed model utilizes a recent public resource containing 1224 normal oral histopathology images at both 100x and 400x magnifications, together with OSCC cells. Once the models' performances have been evaluated, the findings are compared, and the model exhibiting the optimal performance is selected. Diverse deep learning models were utilized to identify the optimal solution, and the Grad-CAM technique was employed to show the localization of cancerous regions. Among the various selected deep learning models for classification, EfficientNet-b0 was a lightweight model that attained an accuracy of 95%.","recorded":"2024-10-28 14:53:08.119998693","filePath":"null","pinned":false},{"value":"lignant area. Amongst the several chosen deep learning models for the classification\npurpose, EfficientNet-b0 was a light weight model that achieved the accuracy of 95%","recorded":"2024-10-28 14:52:37.996944764","filePath":"null","pinned":false},{"value":"(Afify, Mohammed, and Hassanien 2023) proposed a novel model of deep learning\nwhere in order to forecast the OSCC images it uses gradient class activation mapping. The\nsuggested model makes use of a recent public resource with 1224 normal oral histopathol-\nogy pictures both at 100x and 400x magnifications, and OSCC cells. The results are\ncompared once the models performances have been estimated, and the model with the\nbest performance is chosen. Various deep learning models were applied to find the best\nsolution and Grad-cam algorithm was applied to present a visualized localization of ma","recorded":"2024-10-28 14:52:30.416484365","filePath":"null","pinned":false},{"value":"Oral cancer Detection using ensemble techniques","recorded":"2024-10-28 14:52:21.099353888","filePath":"null","pinned":false},{"value":"\\subsection{Oral cancer detection using Deep Learning} \\label{sec-02.01}","recorded":"2024-10-28 14:52:04.270256377","filePath":"null","pinned":false},{"value":"cancer detection using Deep learning","recorded":"2024-10-28 14:51:00.102115137","filePath":"null","pinned":false},{"value":"\\begin{table}[H]\n    \\centering\n    \\caption{Summary of studies on deep learning techniques for oral cancer detection} \\label{Table_01}\n    \\begin{tabular}{|p{3cm}|p{1.5cm}|p{4cm}|p{4cm}|p{4cm}|}\n    \\hline\n    \\textbf{Author} \u0026 \\textbf{Year} \u0026 \\textbf{Techniques} \u0026 \\textbf{Findings} \u0026 \\textbf{Limitations} \\\\ \\hline\n    Leandro Muniz de Lima et al., \u0026 2023 \u0026 A deep learning model pre-trained on ImageNet dataset \u0026 The model has balanced accuracy of 83.24\\% \u0026 Lower accuracy due to unbalanced dataset \\\\ \\hline\n    Jubair, F et al., \u0026 2022 \u0026 A lightweight EfficientNet-B0 was proposed \u0026 The model is lightweight and fast with 85\\% accuracy \u0026 The introduced system was insufficient for a larger dataset \\\\ \\hline\n    Sreerama Prasad et al., \u0026 2022 \u0026 Computer-aided tongue diagnosis system (CATSDNet) was used \u0026 The introduced model provided an accuracy of 92.3\\% \u0026 The dataset quality was poor \\\\ \\hline\n    Sumsum P Sunny et al., \u0026 2022 \u0026 Modified Unet on MSMF dataset \u0026 Model provided IOU of 0.73-0.76 \u0026 Unet not very effective and can be modified further \\\\ \\hline\n    Aritri Ghosh et al., \u0026 2022 \u0026 Modifications in the Raman and FTIR spectra using DNN \u0026 Testing accuracy of 83.33\\% and ROC of 0.88 observed \u0026 Lower accuracy observed due to unbalanced dataset \\\\ \\hline\n    Qiuyun Fu et al., \u0026 2020 \u0026 Two-phase learning with ensemble model \u0026 Algorithm offered an AUC of 0.983 and accuracy of 91.5\\% \u0026 Accuracy achieved at the cost of time complexity \\\\ \\hline\n    \\end{tabular}\n\\end{table}\n","recorded":"2024-10-28 14:43:44.912993789","filePath":"null","pinned":false},{"value":"\\begin{table}[H]\n    \\centering\n    \\caption{Summary of studies on deep learning techniques for oral cancer detection} \\label{Table_01}\n    \\resizebox{\\textwidth}{!}{ % This command scales the table to fit within the text width\n    \\begin{tabular}{|l|c|l|l|l|}\n    \\hline\n    \\textbf{Author} \u0026 \\textbf{Year} \u0026 \\textbf{Techniques} \u0026 \\textbf{Findings} \u0026 \\textbf{Limitations} \\\\ \\hline\n    Leandro Muniz de Lima et al., \u0026 2023 \u0026 A deep learning model pre-trained on ImageNet dataset \u0026 The model has balanced accuracy of 83.24\\% \u0026 Lower accuracy due to unbalanced dataset \\\\ \\hline\n    Jubair, F et al., \u0026 2022 \u0026 A lightweight EfficientNet-B0 was proposed \u0026 The model is lightweight and fast with 85\\% accuracy \u0026 The introduced system was insufficient for a larger dataset \\\\ \\hline\n    Sreerama Prasad et al., \u0026 2022 \u0026 Computer-aided tongue diagnosis system (CATSDNet) was used \u0026 The introduced model provided an accuracy of 92.3\\% \u0026 The dataset quality was poor \\\\ \\hline\n    Sumsum P Sunny et al., \u0026 2022 \u0026 Modified Unet on MSMF dataset \u0026 Model provided IOU of 0.73-0.76 \u0026 Unet not very effective and can be modified further \\\\ \\hline\n    Aritri Ghosh et al., \u0026 2022 \u0026 Modifications in the Raman and FTIR spectra using DNN \u0026 Testing accuracy of 83.33\\% and ROC of 0.88 observed \u0026 Lower accuracy observed due to unbalanced dataset \\\\ \\hline\n    Qiuyun Fu et al., \u0026 2020 \u0026 Two-phase learning with ensemble model \u0026 Algorithm offered an AUC of 0.983 and accuracy of 91.5\\% \u0026 Accuracy achieved at the cost of time complexity \\\\ \\hline\n    \\end{tabular}\n    }\n\\end{table}","recorded":"2024-10-28 14:41:54.476505012","filePath":"null","pinned":false},{"value":"Table \\ref{Table_01}.\n\\begin{table}[H]\n    \\centering\n    %\\small\t\n    \\caption{Summary of studies on deep learning techniques for oral cancer detection} \\label{Table_01}\n    \\addtolength{\\tabcolsep}{2.0pt}\n    \\begin{tabular}{|l|c|l|l|l|}\n    \\hline\n    \\textbf{Author} \u0026 \\textbf{Year} \u0026 \\textbf{Techniques} \u0026 \\textbf{Findings} \u0026 \\textbf{Limitations} \\\\ \\hline\n    Leandro Muniz de Lima et al., \u0026 2023 \u0026 A deep learning model pre-trained on imagenet dataset \u0026 The model has balanced accuracy of 83.24\\% \u0026 Lower accuracy due to unbalanced dataset \\\\ \\hline\n    Jubair, F et al., \u0026 2022 \u0026 A lightweight EfficientNet-B0 was proposed \u0026 The model is lightweight and fast with 85\\% accuracy \u0026 The introduced system was insufficient for a larger dataset \\\\ \\hline\n    Sreerama Prasad et al., \u0026 2022 \u0026 Computer-aided tongue diagnosis system (CATSDNet) was used \u0026 The introduced model provided an accuracy of 92.3\\% \u0026 The dataset quality was poor \\\\ \\hline\n    Sumsum P Sunny et al., \u0026 2022 \u0026 Modified Unet on MSMF dataset \u0026 Model provided IOU of 0.73-0.76 \u0026 Unet not very effective and can be modified further \\\\ \\hline\n    Aritri Ghosh et al., \u0026 2022 \u0026 Modifications in the Raman and FTIR spectra using DNN \u0026 Testing accuracy of 83.33\\% and ROC of 0.88 observed \u0026 Lower accuracy observed due to unbalanced dataset \\\\ \\hline\n    Qiuyun Fu et al., \u0026 2020 \u0026 Two-phase learning with ensemble model \u0026 Algorithm offered an AUC of 0.983 and accuracy of 91.5\\% \u0026 Accuracy achieved at the cost of time complexity \\\\ \\hline\n    \\end{tabular}\n\\end{table}\n","recorded":"2024-10-28 14:40:15.201471483","filePath":"null","pinned":false},{"value":"Table \\ref{Table_01}.\n\\begin{table}[H]\n    \\centering\n    %\\small\t\n    \\caption{Distribution of cases within the dataset} \\label{Table_01}\n    \\addtolength{\\tabcolsep}{2.0pt}\n    \\begin{tabular}{l l l l}\n    \\hline\n    %\\rowcolor[HTML]{EFEFEF}\n        Dataset     \u0026 Number of Cases \u0026 Percentage \u0026 Purpose   \\\\ \\hline\n        Training    \u0026 $34,655$        \u0026 $64\\%$     \u0026 Model Training   \\\\\n        Testing     \u0026 $10,830$        \u0026 $20\\%$     \u0026 Model Evaluation \\\\\n        Validation  \u0026 $8,664$         \u0026 $16\\%$     \u0026 Hyperparameter Tuning \\\\ \\hline\n    \\end{tabular}\n\\end{table}","recorded":"2024-10-28 14:39:43.209359154","filePath":"null","pinned":false},{"value":"Rubin et al. (2019) A unique deep learning technique for medical imaging has been created, targeting the challenge of a restricted training dataset, which is a fundamental limitation of deep learning, and applying it to the categorization of cancerous and healthy cell lines acquired using quantitative phase imaging. The proposed method, known as transferring of pre-trained generative adversarial network (TOP-GAN), integrates transfer learning with generative adversarial networks (GANs). Unstained cancer cell photos were sourced from many origins and amalgamated into a single collection. The suggested model attained a sensitivity rate of 98% in classifying cancer cells. The authors neglected to address class imbalance and time complexity.","recorded":"2024-10-28 14:30:52.769023849","filePath":"null","pinned":false},{"value":"(Rubin et al. 2019) A novel deep learning method for medical imaging has been\ndeveloped, which addresses the issue of a limited training set deep learnings primary\nbottleneck and applies it to the classification of cancer and healthy cell lines obtained\nby quantitative phase imaging. The suggested technique, referred to as transferring of\npre-trained generative adversarial network (TOP-GAN), combines transfer learning with\ngenerative adversarial networks (GANs). Stain free cancer cell images were taken form\nmultiple sources and combined into one. The proposed model achieved an sensitivity rate\nof 98% while classifying the cancer cells. The authors did not take class imbalance and\ntime complexity into consideration","recorded":"2024-10-28 14:30:40.460170394","filePath":"null","pinned":false},{"value":"Ghosh et al. (2022) developed an approach. To categorize the epigenetic modifications identified in the Raman and FTIR spectra with a Deep Reinforcement Neural Network (DRNN). Utilizing data from many domains, RS and FTIR offer substantial benefits compared to conventional molecular biology methodologies. The threshold detection layer, together with the reinforced learning layer, is utilized in the feature extraction layer of the deep learning model to identify significant epigenetic features. The classification layer consists of N layers of back-propagated Artificial Neural Networks (ANN). A substantial spectral dataset was obtained and utilized to train the model. The testing accuracy of the suggested DRNN model is 83.33%. The ROC for the suggested DRNN model is 0.88.","recorded":"2024-10-28 14:26:49.503465879","filePath":"null","pinned":false},{"value":"(Ghosh et al. 2022) proposed a methodology To categories the epigenetic modifica-\ntions discovered in the Raman and FTIR spectra using a Deep neural network which is\nreinforced(DRNN). By using data from different areas, RS and FTIR provide significant\nadvantages over traditional molecular biology techniques. The threshold detection layer\nwith the reinforced learning layer are used in the feature extraction layer of the DL model\nto find important epigenetic features. N numbers of back-propagated Artificial Neural\nNetwork (ANN) layers make up the classification layer. A large spectral dataset was ac-\nquired and used to train the model. The proposed DRNN models testing accuracy is\n83.33%. ROC for the proposed DRNN model is 0.88.","recorded":"2024-10-28 14:24:08.249675617","filePath":"null","pinned":false},{"value":"S. P. Sunny et al. (2022) developed a semantic segmentation model to categorize segmented images following the isolation of Single Epithelial Cells (SEC) from fluorescent, multi-channel, microscopic oral cytology images. A total of 2730 differently stained, multi-channel, fluorescent microscopic images of the cytoplasm and nucleus were utilized to train the model. A novel bespoke Convolutional Neural Network (CNN) model, designated Artefact-Net, alongside the InceptionV3 model, was developed for data classification. The highest overall IoU (0.73-0.76) and for SEC segmentation (0.79) were achieved by the U-Net and modified U-Net models. The Artefact-Net surpassed InceptionV3 in cluster identification, achieving superior precision and F1 score (Precision: 0.91 compared to 0.80; F1: 0.91 compared to 0.86).","recorded":"2024-10-28 14:23:48.131508160","filePath":"null","pinned":false},{"value":"(S. P. Sunny et al. 2022) devised a semantic segmentation model to classify the seg-\nmented images after separating Single Epithelial Cells (SEC) from fluorescent, multi-\nchannel, microscopic oral cytology images. 2730 differentially stained multi-channel,\nfluorescent, microscopic pictures of the cytoplasm and nucleus were used to train the\nmodel. A new bespoke Convolutional-Neural-Network (CNN) model (Artefact-Net) and\nthe InceptionV3 model were trained to classify data. The best IoU overall (0.73-0.76)\nand for SEC segmentation (079) were provided by the U-Net and modified U-Net mod-\nels. When identifying clusters, the Artefact-Net outperformed InceptionV3 in terms of\nprecision and F1 score (Precision: 0.91 vs 0.80; F1: 0.91 vs 0.86).","recorded":"2024-10-28 14:23:39.656846221","filePath":"null","pinned":false},{"value":"Fu et al. (2020) Cascaded convolutional neural networks (CCNN) were emphasized for the detection of oral cavity squamous cell carcinoma (OCSCC) in clinical images of oral cancer. Forty-four thousand clinical photos were acquired from various hospitals in China over a period of fourteen years. The dataset was validated using six prestigious journals in the domain of oral surgery and dentistry. The area under receiver operating characteristic curves (AUCs), together with accuracy, sensitivity, and specificity, accompanied by two-sided 95% confidence intervals, were utilized to evaluate the algorithm's performance on the internal, external, and clinical validation datasets. An ensemble model was utilized for two-phase learning. The evaluation indicated that the suggested algorithm achieved an area under the curve (AUC) of 0.983 and an accuracy of 91.5% in the categorization of OCSCC lesions. The proposed model attained a high AUC, although its complexity escalated.","recorded":"2024-10-28 14:23:22.099705120","filePath":"null","pinned":false},{"value":"under curve (AUC) of 0.983 and accuracy of 91.5% during the classification of OCSCC\nlegions. The proposed model achieved a great AUC but the complexity increased","recorded":"2024-10-28 14:23:15.548872431","filePath":"null","pinned":false},{"value":"(Fu et al. 2020) Cascaded convolutional neural networks (CCNN) were highlighted for\ndetecting OCSCC on clinical images of oral cancer. 44000 clinical images were taken\nform multiple hospitals in China during a span of 14 years. The dataset was validated\nfrom six exemplary journals in the field of oral surgery and dentistry. Area under re-\nceiver operating characteristic curves (AUCs), accuracy, sensitivity, and specificity with\ntwo-sided 95% confidence intervals were used to assess the algorithm performance on\nthe internal, external, and clinical validation datasets. Two Phase learning with ensemble\nmodel was applied. The assessment showed that the proposed algorithm provided the area","recorded":"2024-10-28 14:23:10.665477699","filePath":"null","pinned":false},{"value":"Jubair et al. (2022) suggested a lightweight deep convolutional neural network utilizing EfficientNet-B0 for the diagnosis of oral cancer through clinical photos. The dataset comprised 716 oral cancer photos categorized into two classifications, and transfer learning techniques were employed to create the CNN. The dataset had over double the amount of malignant photos, resulting in a class imbalance. To address this imbalance, bootstrapping was employed with over 120 iterations. The suggested CNN's performance parameters for categorizing clinical images of the tongue as benign or worrisome. The suggested model achieved an accuracy of 85% in classifying oral cancer photos into two categories, which is much superior to prior methods.","recorded":"2024-10-28 14:23:01.350355432","filePath":"null","pinned":false},{"value":"(Jubair et al. 2022) proposed a lightweight deep convolutional neural network based\non EfficientNet-B0 for detection of oral cancer using clinical images. The dataset con-\nsisted of 716 oral cancer images belonging to 2 classes, transfer learning models was used\nto develop the CNN. The dataset contained more than twice numbers of cancerous images\nwhich caused a class imbalance, to solve class imbalance bootstrapping was applied it had\nmore than 120 repetitions. The proposed CNNs performance metrics for classifying clin-\nical images of the tongue as either benign or suspicious. The proposed model provided\nan accuracy of 85% while classification of oral cancer images into 2 classes which was\ncomparably higher than other algorithms.","recorded":"2024-10-28 14:22:51.762611872","filePath":"null","pinned":false},{"value":"Lima et al. (2023) created an approach that evaluates the significance of supplementary information for computer-aided design in the analysis of histological pictures of oral leukoplakia and cancer. Between 2011 and 2021, a novel dataset (NDB-UFES) comprising 237 histopathological image samples and associated data was compiled.\nThe leading models, based on testing results, exhibit a balanced accuracy of 83.24% when utilizing images, demographic information, and clinical data with MetaBlock fusion and ResNetV2 as the foundation.","recorded":"2024-10-28 14:22:33.085434791","filePath":"null","pinned":false},{"value":"(Lima et al. 2023) developed a strategy that assesses the value of additional informa-\ntion for computer-aided design in the interpretation of histological images of leukoplakia\nof the mouth and carcinoma. From 2011 to 2021, a brand-new dataset (NDB-UFES) of\nhistopathological pictures and data was gathered which contained 237 image samples.\nThe top models, according to experimental findings,the model have balanced accuracy of\n83.24% when using pictures, demographic data, and clinical data using MetaBlock fusion\nand ResNetV2 as the foundation.","recorded":"2024-10-28 14:22:13.801307372","filePath":"null","pinned":false},{"value":"Chelluboina and Rao (2023) developed an innovative methodology centered on the CATD-SNet, a computer-aided tongue diagnosis system that utilizes tongue image analysis for disease prediction. The test image undergoes fast nonlocal mean (FNLM) filtering to preprocess the supplied tongue dataset. The pre-processed tongue images are subsequently utilized to extract color features through color moments. Furthermore, the texture features are derived via the grey level co-occurrence matrix (GLCM). The proposed CATDSNet is trained with a hybrid extreme learning machine (HELM) classifier to identify various diseases utilizing the extracted color and texture information.\nThe simulation results of the tongue image dataset indicate that the proposed CATDSNet model outperforms leading approaches such as random forest and support vector machine (SVM), with a classification accuracy of 92.3%.","recorded":"2024-10-28 14:21:59.940137681","filePath":"null","pinned":false},{"value":"order to preprocess the provided tongue dataset. The pre-processed tongue photos are then\nused to extract colour features using colour moments. In addition, the information about\nthe texture features is extracted using the grey level cooccurrence matrix (GLCM). The\nproposed CATDSNet is then trained using a hybrid extreme learning machine (HELM)\nclassifier to diagnose various diseases using the extracted colour and texture information.\nThe tongue image dataset simulation results show that the proposed CATDSNet model\nperforms better than state-of-the-art methods like random forest, support vector machine\n(SVM)with a classification accuracy of 92.3%.","recorded":"2024-10-28 14:21:50.991314504","filePath":"null","pinned":false},{"value":"(Chelluboina and Rao 2023) formulated a novel methodology that focus on the CATD-\nSNet, or computer-aided tongue diagnosis system, which employs tongue image analysis\nto predict disease. the test image is subjected to fast nonlocal mean (FNLM) filtering in","recorded":"2024-10-28 14:21:45.260719982","filePath":"null","pinned":false},{"value":"under curve (AUC) of 0.983 and accuracy of 91.5% during the classification of OCSCC\nlegions. The proposed model achieved a great AUC but the complexity increased\n(S. P. Sunny et al. 2022) devised a semantic segmentation model to classify the seg-\nmented images after separating Single Epithelial Cells (SEC) from fluorescent, multi-\nchannel, microscopic oral cytology images. 2730 differentially stained multi-channel,\nfluorescent, microscopic pictures of the cytoplasm and nucleus were used to train the\nmodel. A new bespoke Convolutional-Neural-Network (CNN) model (Artefact-Net) and\nthe InceptionV3 model were trained to classify data. The best IoU overall (0.73-0.76)\nand for SEC segmentation (079) were provided by the U-Net and modified U-Net mod-\nels. When identifying clusters, the Artefact-Net outperformed InceptionV3 in terms of\nprecision and F1 score (Precision: 0.91 vs 0.80; F1: 0.91 vs 0.86).\n(Ghosh et al. 2022) proposed a methodology To categories the epigenetic modifica-\ntions discovered in the Raman and FTIR spectra using a Deep neural network which is\nreinforced(DRNN). By using data from different areas, RS and FTIR provide significant\nadvantages over traditional molecular biology techniques. The threshold detection layer\nwith the reinforced learning layer are used in the feature extraction layer of the DL model\nto find important epigenetic features. N numbers of back-propagated Artificial Neural\nNetwork (ANN) layers make up the classification layer. A large spectral dataset was ac-\nquired and used to train the model. The proposed DRNN models testing accuracy is\n83.33%. ROC for the proposed DRNN model is 0.88.\n(Rubin et al. 2019) A novel deep learning method for medical imaging has been\ndeveloped, which addresses the issue of a limited training set deep learnings primary\nbottleneck and applies it to the classification of cancer and healthy cell lines obtained\nby quantitative phase imaging. The suggested technique, referred to as transferring of\npre-trained generative adversarial network (TOP-GAN), combines transfer learning with\ngenerative adversarial networks (GANs). Stain free cancer cell images were taken form\nmultiple sources and combined into one. The proposed model achieved an sensitivity rate\nof 98% while classifying the cancer cells. The authors did not take class imbalance and\ntime complexity into consideration","recorded":"2024-10-28 14:21:18.121470436","filePath":"null","pinned":false},{"value":"order to preprocess the provided tongue dataset. The pre-processed tongue photos are then\nused to extract colour features using colour moments. In addition, the information about\nthe texture features is extracted using the grey level cooccurrence matrix (GLCM). The\nproposed CATDSNet is then trained using a hybrid extreme learning machine (HELM)\nclassifier to diagnose various diseases using the extracted colour and texture information.\nThe tongue image dataset simulation results show that the proposed CATDSNet model\nperforms better than state-of-the-art methods like random forest, support vector machine\n(SVM)with a classification accuracy of 92.3%.\n(Lima et al. 2023) developed a strategy that assesses the value of additional informa-\ntion for computer-aided design in the interpretation of histological images of leukoplakia\nof the mouth and carcinoma. From 2011 to 2021, a brand-new dataset (NDB-UFES) of\nhistopathological pictures and data was gathered which contained 237 image samples.\nThe top models, according to experimental findings,the model have balanced accuracy of\n83.24% when using pictures, demographic data, and clinical data using MetaBlock fusion\nand ResNetV2 as the foundation.\n(Jubair et al. 2022) proposed a lightweight deep convolutional neural network based\non EfficientNet-B0 for detection of oral cancer using clinical images. The dataset con-\nsisted of 716 oral cancer images belonging to 2 classes, transfer learning models was used\nto develop the CNN. The dataset contained more than twice numbers of cancerous images\nwhich caused a class imbalance, to solve class imbalance bootstrapping was applied it had\nmore than 120 repetitions. The proposed CNNs performance metrics for classifying clin-\nical images of the tongue as either benign or suspicious. The proposed model provided\nan accuracy of 85% while classification of oral cancer images into 2 classes which was\ncomparably higher than other algorithms.\n(Fu et al. 2020) Cascaded convolutional neural networks (CCNN) were highlighted for\ndetecting OCSCC on clinical images of oral cancer. 44000 clinical images were taken\nform multiple hospitals in China during a span of 14 years. The dataset was validated\nfrom six exemplary journals in the field of oral surgery and dentistry. Area under re-\nceiver operating characteristic curves (AUCs), accuracy, sensitivity, and specificity with\ntwo-sided 95% confidence intervals were used to assess the algorithm performance on\nthe internal, external, and clinical validation datasets. Two Phase learning with ensemble\nmodel was applied. The assessment showed that the proposed algorithm provided the area","recorded":"2024-10-28 14:21:09.761687377","filePath":"null","pinned":false},{"value":"Oral cancer detection using Deep Learning","recorded":"2024-10-28 14:20:36.772371884","filePath":"null","pinned":false},{"value":"https://www.amazon.in/Heavy-Strips-Adhesive-Sticky-Fastener/dp/B081RMGKV6/ref=pd_vtp_h_pd_vtp_h_d_sccl_3/258-7013446-4850045?pd_rd_w=IWJAP\u0026content-id=amzn1.sym.6c9a4279-ad42-4fd6-b9a9-3cd14ede34c9\u0026pf_rd_p=6c9a4279-ad42-4fd6-b9a9-3cd14ede34c9\u0026pf_rd_r=3BH4GE86E9B4NVCT7NYR\u0026pd_rd_wg=F7WkD\u0026pd_rd_r=8e495ce0-1416-49a9-8e18-9c2f176171d6\u0026pd_rd_i=B081RMGKV6\u0026psc=1","recorded":"2024-10-28 14:03:16.200977543","filePath":"null","pinned":false},{"value":"A large number of deaths were recorded from oral cancer as a result of lack of its identification and late treatment. Oral cavity cancer has a significant mortality rate that is rising. It is crucial to develop and put into practise a method for detecting this malignancy early on. By identifying cancer early and adopting preventative measures, it is simple to limit the number of deaths brought on by the disease. Although many researchers have already conducted their research in the field of oral cancer disorders, there is still a great deal of research that may be done in this area owing to performance improvements. Machine learning has advanced to the point where getting more use out of it is all but impossible during the last several years. The performance of deep learning models has increased, but there is still a concern of model size, low accuracy, and high computation time. \n\n\\begin{itemize}\n    \\item To propose and implement an optimized deep learning algorithm for detection of oral cancer in its early stages.\n    \\item To implement metaheuristic optimization for better weight selection of clinical images.\n    \\item To conduct an analysis and compare the proposed approach with state-of-the-art models based on evaluation metrics like accuracy, precision, sensitivity, and specificity.\n\\end{itemize}\n","recorded":"2024-10-28 13:33:22.572209837","filePath":"null","pinned":false},{"value":"A large number of deaths were recorded from oral cancer as a result of lack of its identification and late treatment. Oral cavity cancer has a significant mortality rate that is rising. It is crucial to develop and put into practise a method for detecting this malignancy early on. By identifying cancer early and adopting preventative measures, it is simple to limit the number of deaths brought on by the disease. Although many researchers have already\nconducted their research in the field of oral cancer disorders, there is still a great deal of research that may be done in this area owing to performance improvements. Machine learning has advanced to the point where getting more use out of it is all but impossible during the last several years. The performance of Deep learning models have increased but there is still a concern of model size, low accuracy and high computation time. \n\n To propose and implement optimized Deep learning algorithm for detection of oral cancer in its early stages.\n To implement Metaheuristic optimization for better weight selection of clinical images.\n To conduct an analysis and compare the proposed approach with state of art models on basic of evaluation matrices like accuracy, precision, Sensitivity and Specificity.","recorded":"2024-10-28 13:32:22.877725580","filePath":"null","pinned":false},{"value":"A large number of deaths were recorded from oral cancer as a result of lack of its identifi-\ncation and late treatment. Oral cavity cancer has a significant mortality rate that is rising.\nIt is crucial to develop and put into practise a method for detecting this malignancy early\non. By identifying cancer early and adopting preventative measures, it is simple to limit\nthe number of deaths brought on by the disease. Although many researchers have already\nconducted their research in the field of oral cancer disorders, there is still a great deal of\nresearch that may be done in this area owing to performance improvements.\nMachine learning has advanced to the point where getting more use out of it is all but\nimpossible during the last several years. The performance of Deep learning models have\nincreased but there is still a concern of model size, low accuracy and high computation\ntime.\n To propose and implement optimized Deep learning algorithm for detection of oral\ncancer in its early stages.\n To implement Metaheuristic optimization for better weight selection of clinical im-\nages.\n To conduct an analysis and compare the proposed approach with state of art models\non basic of evaluation matrices like accuracy, precision, Sensitivity and Specificity.","recorded":"2024-10-28 13:31:41.122259700","filePath":"null","pinned":false},{"value":"The recent rise in mouth cancer is negatively impacting human health. Oral cancer is treatable if identified early; given the rising incidence of oral cancer cases, there is a pressing need for a precise and rapid method to detect cancer cells. The risk of oral cancer exists across all age demographics; however, older individuals are more susceptible due to poor lifestyles. Many individuals have had financial difficulties. The early detection of sickness is essential for enabling patients to initiate preventative actions promptly. Artificial intelligence, encompassing machine learning and deep learning, is fundamentally dependent on categorization, grading, segmentation, and computer vision. The primary motivation for conducting research in this topic is to enhance model optimization.\n\n\\begin{itemize}\n    \\item The concept of deep learning captivates my desire to acquire further knowledge in this field. A deep learning-based approach can identify early indications of mouth cancer detectable by contemporary cameras.\n    \\item Clinical images can provide more accurate and rapid results compared to conventional methods employed by physicians.\n\\end{itemize}\n","recorded":"2024-10-28 13:30:45.320221387","filePath":"null","pinned":false},{"value":"The recent rise in mouth cancer is negatively impacting human health. Oral cancer is treatable if identified early; given the rising incidence of oral cancer cases, there is a pressing need for a precise and rapid method to detect cancer cells. The risk of oral cancer exists across all age demographics; however, older individuals are more susceptible due to poor lifestyles. Many individuals have had financial difficulties. The early detection of sickness is essential for enabling patients to initiate preventative actions promptly. Artificial intelligence, encompassing machine learning and deep learning, is fundamentally dependent on categorization, grading, segmentation, and computer vision. The primary motivation for conducting research in this topic is to enhance model optimization.\n The concept of deep learning captivates my desire to acquire further knowledge in this field. A deep learning-based approach can identify early indications of mouth cancer detectable by contemporary cameras.\n Clinical images can provide more accurate and rapid results compared to conventional methods employed by physicians.","recorded":"2024-10-28 13:30:27.489185718","filePath":"null","pinned":false},{"value":"","recorded":"2024-10-28 13:30:23.601806990","filePath":"null","pinned":false},{"value":"The latest trend in increase of oral cancer is having an adverse effect on health of human\nbeing. Oral cancer can be treated if detected early, with the increase in total number of\ncases of oral cancer we need an accurate and fast way to detect cancer cells. The risk\nof oral cancer is in all age groups but elder people are more prone to it due to unhealthy\nlifestyle. A lot of people have experienced financial troubles. It is crucial for the early\ndiagnosis of disease so that patients can start taking preventative measures right away. AI,\nwhich consists of machine learning and deep learning, is heavily reliant on classification,\ngrading, segmentation, and computer vision. To more or less better model optimisation is\nthe main reason for conducting research in this field.\n Deep learning concept fascinate me to learn more in this area. Deep learning based\nmodel can detect oral cancer with early signs that can be captured by modern cam-\n13\neras\n Clinical Images can give an more accurate and fast result as compare to normal\nmethods applied by Doctors","recorded":"2024-10-28 13:29:38.132635645","filePath":"null","pinned":false},{"value":"Deep learning has emerged as a potent method in artificial intelligence, allowing models to independently learn from extensive, unstructured data sets via deep neural network topologies (Dubuc et al. 2022). In contrast to conventional machine learning, deep learning utilizes unsupervised and sophisticated neural networks to discern complicated patterns in data, mirroring certain decision-making processes of the human brain. The significance is highlighted by substantial enhancements in computational speed and memory, enabling models to learn from vast data sets, even when the input is noisy or confusing. The capacity to manage intricate data kinds and subtle characteristics renders deep learning particularly appropriate for applications in medical diagnostics, such as cancer diagnosis.\n\nDeveloping deep learning models from inception necessitates an extensive labeled dataset and substantial time, frequently spanning weeks or months, to fine-tune learning rates and attain precision. Transfer learning has emerged as a widely used technique, facilitating the adaption of pre-trained models, such as AlexNet or GoogleNet, to novel categories with diminished data requirements and lower computational costs. Deep learning facilitates feature extraction, wherein distinct characteristics are derived from several network levels, assisting in applications such as picture classification.\n\nIn medical imaging for cancer diagnosis, deep learning improves diagnostic accuracy, aids clinical decision-making, and minimizes significant data preprocessing by effectively detecting pertinent features. Methods include pre-processing, feature extraction, and feature selection are essential for enhancing model performance and minimizing computation time. This facilitates models that attain significant diagnostic accuracy while enhancing workflow efficiency and patient-clinician interactions, ultimately leading to more tailored treatment options and improved results in cancer care.\n\nMetaheuristic optimization, in conjunction with deep learning, is essential for effectively addressing difficult, nonlinear optimization challenges. Nature-inspired metaheuristic algorithms, such as swarm-based approaches, provide flexible and adaptable solutions to problems characterized by ambiguous search spaces or many local optima. Their simplicity, ease of implementation, and capacity to address complex real-world optimization challenges render them indispensable tools in domains necessitating computing efficiency, such as medical diagnosis and image processing. Deep learning and metaheuristic optimization collaboratively create a synergistic framework that improves the accuracy, efficiency, and application of AI-driven diagnostic tools.","recorded":"2024-10-28 13:27:35.331638169","filePath":"null","pinned":false},{"value":"Deep learning has emerged as a powerful approach within artificial intelligence, enabling models to autonomously learn from vast, unstructured data sets through deep neural network architectures (Dubuc et al. 2022). Unlike traditional machine learning, deep learning leverages unsupervised and complex neural networks to identify intricate patterns in data, emulating some decision-making aspects of the human brain. Its importance is underscored by significant improvements in computational speed and memory, which have empowered models to learn from extensive data pools even when the information is noisy or ambiguous. This ability to handle complex data types and nuanced features makes deep learning especially suited to applications in medical diagnostics, including cancer detection.\n\nTraining deep learning models from scratch requires a large labeled dataset and considerable time, often weeks or months, to optimize learning rates and achieve accuracy. However, transfer learning has become a prevalent method, enabling the adaptation of pre-trained models, such as AlexNet or GoogleNet, to new categories with less data and reduced computational demand. Deep learning also supports feature extraction, whereby specific features are drawn from multiple network layers, aiding in applications like image categorization.\n\nIn the context of medical imaging for cancer detection, deep learnings capabilities enhance diagnostic accuracy, facilitate clinical decision-making, and reduce the need for extensive data preprocessing by efficiently identifying relevant features. Techniques such as pre-processing, feature extraction, and feature selection are crucial in optimizing model performance and reducing computation time. This allows for models that not only achieve a high level of diagnostic precision but also improve workflow efficiency and patient-clinician interactions, ultimately contributing to more personalized treatment options and better outcomes in cancer care.\n\nComplementing deep learning, metaheuristic optimization plays a crucial role in efficiently solving complex, nonlinear optimization problems. Metaheuristic algorithms, inspired by nature, such as swarm-based techniques, offer flexible, adaptable solutions to challenges involving ambiguous search spaces or multiple local optima. Their simplicity, ease of implementation, and capability to handle real-world, intricate optimization problems make them invaluable tools in fields that require computational efficiency, including medical diagnosis and image analysis. Together, deep learning and metaheuristic optimization form a synergistic framework, enhancing the accuracy, efficiency, and applicability of AI-driven diagnostic tools.","recorded":"2024-10-28 13:27:17.404476765","filePath":"null","pinned":false},{"value":"Deep Learning and Metaheuristic Optimization","recorded":"2024-10-28 13:26:55.455734125","filePath":"null","pinned":false},{"value":"C","recorded":"2024-10-28 13:25:56.737733786","filePath":"null","pinned":false},{"value":"1.8\n Metaheuristic Optimization\nReal-world optimisation issues frequently involve a large number of choice variables, in-\ntricate nonlinear constraints, and difficult objective functions, which makes them more\nand more difficult to solve. Using conventional strategies like numerical methods, tglobal optimization is less effective, particularly when limitations or objective functions\ninclude many peaks. Strong instruments for tackling difficult optimisation problems,\nmetaheuristic algorithms are gaining popularity.\nThe simplicity of metaheuristic algorithms is by far their most notable feature. The fun-\ndamental theories or mathematical models underlying these metaheuristic techniques are\nderived from nature. The majority of these techniques are straightforward and simple to\nuse. One can utilise metaheuristics to solve real-world problems thanks to their usability.\nAdditionally, it is simple to create their versions using current techniques.\nThese optimisation technologies can be thought of as \"black boxes,\" capable of providing\na set of outputs for a specific problem for a specific set of inputs. One of the most crucial\naspects of metaheuristic algorithms is randomization. This makes it possible for meta-\nheuristic algorithms to effectively avoid trapping in local optima and to search the whole\nsearch space. More specifically, it enables numerous metaheuristics to handle issues in-\nvolving an ambiguous search space or various local optima. Finally, because of their ex-\ntreme adaptability and flexibility, these metaheuristics can be used to solve a wide range\nof optimisation issues, including non-linear issues, issues involving non-differentiable\nvariables, and issues involving sophisticated numerical calculations and a large number\nof local minima.","recorded":"2024-10-28 13:21:08.837484650","filePath":"null","pinned":false},{"value":"1.7.3\n Working on Deep Learning Networks\nSince most deep learning methods rely on neural network topologies, they are referred\ndescribed as \"deep neural networks\".\nNormal neural nets only have a few hidden levels, whereas deeper networks may contain\nup to 150 layers. Very vast quantity of categorised autonomously generated data and neu-\nral network topology extract features.\na) Training from Scratch:- For a deep network to be trained from beginning, a very large\nlabelled data set must be gathered, and a network architecture must be created that will\nallow the network to gain insight into its characteristics and predict. This is advantageous\nfor newly developed apps or applications with numerous output categories. This is a lefrequent strategy because these networks often take weeks or even months to train be-\ncause to the volume of data and learning rate.\nb) Transfer Learning:- It is a deep learning technique where a pre-trained model is mod-\nified as part of the transfer learning approach. It begins with a reliable network, like\nAlexNet or GoogleNet, then feeds it new values which are previously undiscovered classes.\nThe task can now be carried, out after making network modifications that are minimal.\nMoreover, processing hundreds of photographs as opposed to millions has the advantage\nof requiring much less data, which cuts down computation time to minutes or hours.\nc) Feature Extraction:- The network can be used as a feature extractor, which is a little less\ntypical and a more specialised method of deep learning. Feature Extraction can remove\nspecific features from the network at any point throughout the training process because\nall the layers are charged with learning specific features from images.\n1.7.4\n Purpose of Deep Learning\nThe models developed using Deep Learning have the potential to provide more precise\nand individualised cancer treatment by better predicting the prognosis of the disease. They\nare superior to or on par with the methods now used in clinical settings. Deep learning\ntechniques are anticipated to help in the proper handling of squamous cell carcinoma of\nthe oral cavity through enhanced diagnostic performance, wise clinical decision-making,\nstreamlining of clinicians work, the potential for lowering cancer screening costs, and a\nsuccessful evaluation and detection of the disease. In order to increase the quality of care,\nprofessionals and patients can spend more time talking to one another and deliberating\ntogether. Future research should focus on creating deep learning models that integrate\ndiverse datasets from many modalities.\n Pre Processing: Due to a variety of factors, the original image will always contain\nsome noise. The accuracy of the diagnosis is compromised by these noises. A cru-\ncial part of the image processing process is pre-processing. Asymmetric filtration is\na filter that is frequently used to enhance grayscale photographs by reducing noise\nand improving image arrangement, particularly edge boundaries.\n Feature Extraction: - we can generate new features from the previous feature and\nthen we can delete the original features by doing this we can reduce the features\n11\npresent in the dataset. It helps us to categorize the images into different groups.\n Feature Selection: - Providing a vast amount of features to the model can result\nin a overfitted model with a very high computational time, having a better feature\nextraction will help in reducing the time complexity","recorded":"2024-10-28 13:21:01.547658142","filePath":"null","pinned":false},{"value":"1.7\n Deep Learning\nOne of the main components of an Artificially Intelligent system is learning. Learning\nmeans when a computer program can learn through its surrounding. Artificially intelli-\ngent systems have the ability to mimic the human brain and have the ability to process\ninformation and develop various patterns used to make decisions (Dubuc et al. 2022). A\nsub type of machine learning called \"deep learning\" in artificial intelligence (AI) allows\nnetworks to learn unsupervised from unlabelled input. Deep learning can also refer to\ndeep neural networks or deep learning.\n1.7.1\n Importance of Deep Learning\nMachine learning techniques can now build and learn from a large pool of training data\nbecause to improvements in computer speed and memory over time.\nDeep learning has been a cutting-edge method for humanity, especially when the Informa-\ntion is noisy. Artificial neural networks can learn any function with just one hidden layer,\nregardless of how ambiguous it is, which is why they are regarded as universal function\napproximations.","recorded":"2024-10-28 13:20:41.182409114","filePath":"null","pinned":false},{"value":"The primary problem with manual cancer diagnosis is the delay in diagnosis. It requires\nextremely competent labor, and the number of needed diagnostic tests is increasing dra-\nmatically. Because of the time requirements for a proper diagnosis, it is less likely that\nan early identification of the tumor grade will be made.Pathologists heavy workload is a\nserious worry, and this also affects how well they can anticipate outcomes. It also Prevent\nthe delivery of an accurate diagnosis report as the findings must be carefully crafted to\navoid any fatalities.","recorded":"2024-10-28 12:43:16.800950210","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/102852.png","recorded":"2024-10-28 12:41:39.984905696","filePath":"null","pinned":false},{"value":"Oral cancer is one of the most common cancers globally. The predominant instances of this subtype of head and neck carcinoma originate in the squamous epithelial cells that line the oral cavity, tongue, and facial regions. If this is not discovered and managed promptly, it could be fatal. Approximately 53,000 cases of oral cancer, including three percent of all cancers recognized annually in the United States, are associated with mouth cancer. Oral cancer occurs more frequently in males than in females, with males affected more than twice as often, and individuals over the age of 40 are at the highest risk.\nSmoking, using alcoholic beverages, and having a Human Papillomavirus (HPV) infection are the primary contributors of oral cancer. In 2020, global fatalities from lip and oral cavity cancer are projected to exceed 177,000. Despite advancements, mortality rates from oral cancer have persisted at elevated levels in recent decades.\nThe majority of oral cancer patients, especially in rural areas, are unable to access prompt and appropriate diagnosis and treatment, hence diminishing their survival prospects.\nPatients with cancer exhibit a five-year survival rate of approximately 50%, contingent upon race and geographic area. Reports indicate that the survival rate in wealthy countries may attain 65%.\nConversely, depending on the region impacted by oral cancer, a survival rate of fifteen percent is observed in certain rural locations. Cancer therapy can be prohibitively costly, particularly in advanced stages. Both health experts and the general population possess a limited understanding of mouth cancer. The 2020 Cancer Statistics Report for India indicates that 66.6 percent of patients with head and neck cancer had already experienced local progression at the time of diagnosis. Persistent inflammation or non-healing ulcers, accompanied by irritation and hemorrhage, are indicative of oral cancer.\nOral cancer may be attributed to several activities, with smoking and alcohol consumption being the two most prominent factors. The consumption of maggots is prevalent in India, resulting in internal gum damage.\nGLOBOCAN (Global Cancer Incidence, Mortality and Prevalence) projected that in 2018, there would be 177,384 cancer-related fatalities and 354,864 new cancer cases, representing two percent and one point nine percent of all cancer occurrences and deaths, respectively. Mouth cancer, including approximately one-third of all cancer cases, is a significant cause of mortality in Bangladesh, Pakistan, Taiwan, and India.","recorded":"2024-10-28 12:38:33.906111598","filePath":"null","pinned":false},{"value":"which corresponds to two percent and one point nine percent of all occurrences and fa-\ntalities from cancer, respectively. In summary, mouth cancer, which accounts for around\none-third of all cancer cases, is a major reason for death in Bangladesh, Pakistan, Taiwan,\nand India.","recorded":"2024-10-28 12:38:09.999269887","filePath":"null","pinned":false},{"value":"2018, there would be 177,384 cancer-related deaths and 354,864 new instances of cancer,","recorded":"2024-10-28 12:38:04.041294314","filePath":"null","pinned":false},{"value":"deaths globally from lip and oral cavity cancer, In spite of improvements mouth cancer\nfatality rates have remained high in recent decades.\nThe majority of mouth cancer patients, particularly those located in countryside regions,\ncant obtain fast, effective diagnosis and treatment, which lowers their chance of survival.\nDepending on race and location, patients with cancer have a five-year living rate among\nthe 50%. According to reports, the survival rate in developed nations can reach 65%.\nIn contrast, leaning upon the area of the mouth cancer affected, a living rate of fifteen\npercent is noted in some countryside areas. Its because cancer therapy may be highly\nexpensive, especially in later stages. Health experts and the general public both lack a\nsignificant grasp of oral cancer. The 2020 Cancer Statistics Report for India states 66.6\npercent of patients with head and neck cancer had already progressed locally when they\nreceived their diagnosis. Inflammation or ulcers that do not heal, along with discomfort\nand bleeding, are signs of oral cancer.\nOral cancer can be caused by a number of habits, with smoking and drinking being the\ntwo most significant ones. Consuming maggots is so common in India that it causes in-\nternal gum damage.\nGLOBOCAN (Global Cancer Incidence, Mortality and Prevalence) anticipated that in","recorded":"2024-10-28 12:37:48.647539622","filePath":"null","pinned":false},{"value":"Among the most prevalent malignancies worldwide is oral cancer. The majority of cases\nof this subtype of head and neck cancer begin in the cells of squamous tissue that cover\nthe surface of our mouth, tongue, and faces. When this fails to be identified and if not\naddressed in a timely manner, it could be deadly. About 53,000 incidences of oral cancer,\nor three percent of all cancers identified during the study in US annually, are related to\noral cancer. Oral cancer strikes males more frequently than females, more than twice as\noften, and persons over an age of 40 are most at risk.\nSmoking, drinking alcoholic beverages, or having HPV, short for People Papilloma virus\ninfection are the main causes of oral cancer. In 2020, there are expected to be over 177,000","recorded":"2024-10-28 12:36:18.690354254","filePath":"null","pinned":false},{"value":"/home/karna/.config/waybar/Backup/scripts\n/home/karna/.config/waybar/Backup/config.jsonc\n/home/karna/.config/waybar/Backup/style.css\n/home/karna/.config/waybar/Backup/theme.css","recorded":"2024-10-28 11:48:06.685404465","filePath":"null","pinned":false},{"value":"/home/karna/.config/waybar/scripts\n/home/karna/.config/waybar/backup.css\n/home/karna/.config/waybar/config\n/home/karna/.config/waybar/config-background\n/home/karna/.config/waybar/machiatto.css\n/home/karna/.config/waybar/style.css\n/home/karna/.config/waybar/style-background.css","recorded":"2024-10-28 11:47:54.855972950","filePath":"null","pinned":false},{"value":"hyprland","recorded":"2024-10-28 11:44:06.173653879","filePath":"null","pinned":false},{"value":"JetBrainsMono Nerd Font","recorded":"2024-10-28 11:39:59.278901117","filePath":"null","pinned":false},{"value":"\"tooltip\": true,\n","recorded":"2024-10-28 11:34:06.913000292","filePath":"null","pinned":false},{"value":"        \"tooltip-format\": \"{ipaddr}\",\n","recorded":"2024-10-28 11:33:28.687576143","filePath":"null","pinned":false},{"value":"#custom-left6 {\n    color: @pulseaudio;\n    background: @background;\n    padding-left: 3px;\n}\n","recorded":"2024-10-28 11:31:15.519552766","filePath":"null","pinned":false},{"value":"        \"custom/left6\",\n","recorded":"2024-10-28 11:29:44.377074591","filePath":"null","pinned":false},{"value":"        \"idle_inhibitor\",\n","recorded":"2024-10-28 11:27:09.943600402","filePath":"null","pinned":false},{"value":"    foreground: #DA9320;\n","recorded":"2024-10-28 11:26:01.715829434","filePath":"null","pinned":false},{"value":"#DA9320","recorded":"2024-10-28 11:21:08.906725758","filePath":"null","pinned":false},{"value":"#4C2105","recorded":"2024-10-28 11:20:15.628575784","filePath":"null","pinned":false},{"value":"@define-color background        #11111b;\n@define-color foreground        #cdd6f4;\n\n/* Workspace Button Colors */\n@define-color active-bg         #9399b2;\n@define-color active-fg         #11111b;\n@define-color hover-bg          #45475a;\n@define-color hover-fg          #cdd6f4;\n\n/* Wlogout Colors */\n@define-color bar-bg            #11111b;\n@define-color main-bg           #11111b;\n@define-color main-fg           #cdd6f4;\n@define-color wb-act-bg         #9399b2;\n@define-color wb-act-fg         #11111b;\n@define-color wb-hvr-bg         #6c7086;\n@define-color wb-hvr-fg         #11111b;","recorded":"2024-10-28 11:18:14.540912438","filePath":"null","pinned":false},{"value":"/home/karna/dotfiles/XFCEPic/Pictures/WallPapers/naruto-uzumaki.png","recorded":"2024-10-26 15:36:31.286498910","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/naruto-uzumaki.png","recorded":"2024-10-26 13:42:15.837618266","filePath":"null","pinned":false},{"value":"src=\"https://megacloud.tube/embed-1/e-1/74yheM7k6e8I?autoPlay=0\"","recorded":"2024-10-24 22:37:04.349279158","filePath":"null","pinned":false},{"value":"https://archive.org/download/moonfall-60fps-dualgb/moonfall-60fps-dualgb_archive.torrent","recorded":"2024-10-24 22:24:16.007383104","filePath":"null","pinned":false},{"value":"https://archive.org/download/moonfall-60fps-dualgb/Moonfall60fpsDUALGB.mpv","recorded":"2024-10-24 22:23:51.659750878","filePath":"null","pinned":false},{"value":"There are different techniques that are used for the diagnosis of oral cancer, a few of the clinical techniques used by doctors are discussed below:\n\n\\textbf{Barium Swallow:} The voice box, throat, and surroundings may display abnormalities during a barium swallow test, which is also frequently used to find small, early oral tumors.\n\n\\textbf{Biopsy:} The initial step in identifying mouth cancer is an oral tissue biopsy. A small amount of abnormal tissue from the area where oral cancer is suspected is removed by the surgeon during the biopsy. An oral cancer diagnosis may be confirmed by biopsy. The following types of biopsies are frequently used to identify oral carcinoma:\n\n\\begin{itemize}\n    \\item \\textbf{Incisive biopsies:} The region has a small amount of tissue taken from it that appears to be abnormal. If the abnormal location is easily accessible, the specimen could be obtained at the office of a doctor. If the cancer is more deeply embedded in the mouth or throat, biopsy procedures might have to be carried out in a surgical theatre while receiving anesthesia in order to lessen pain.\n    \n    \\item \\textbf{Exfoliative cytology:} Cell samples are gently scraped from a questionable location. To make the cells visible under a microscope, they are placed on a transparent slide and subsequently colored. A deeper biopsy will be done if any cells seem suspicious.\n\\end{itemize}\n\n\\textbf{Image-based tests:}\n\n\\begin{itemize}\n    \\item \\textbf{Computerized Tomography (CT) Scan:} Information on the size, shape, and location of any tumors can be obtained via a CT scan, which can help detect lymph nodes that are bulging and may contain cancer cells.\n    \n    \\item \\textbf{Magnetic Resonance Imaging (MRI):} Oral cancer may be examined with an MRI scan, although this is less usual. MRIs give a very detailed picture and may be very helpful in figuring out whether other areas of the body, such as the neck, have been affected by the disease's spread.\n    \n    \\item \\textbf{Positron Emission Tomography (PET):} Patients with cancer of the oral cavity might get a scan using PET technology to determine if the disease has migrated to the lymph nodes or has recently progressed to that location.\n    \n    \\item \\textbf{Genomic Testing for Advanced Oral Cancer:} Genomic testing, sometimes known as molecular profiling or cancer sequencing, involves examining the collected cells from a biopsy to check for any genetic mutations (changes in DNA) that might be connected to the persons specific type of cancer.\n\\end{itemize}\n","recorded":"2024-10-24 16:39:18.468177891","filePath":"null","pinned":false},{"value":"There are different techniques that are used for the diagnosis of oral cancer, few of the\nclinical techniques used by doctors are discussed below :-\nBarium Swallow: - The voice box, the throat, referral, and surroundings may display\nabnormalities during a barium swallow test, which is also frequently used to find small,\nearly oral tumors.\nBiopsy: - The initial step in identifying mouth cancer is an oral tissue biopsy. A little bit\nof aberrant tissue from the area where oral cancer is suspected is removed by the surgeon\nduring the biopsy. An oral cancer diagnosis may be confirmed by biopsy. The following\ntypes of biopsies are frequently used to identify oral carcinoma:\n Incisive biopsies: The region has a small amount of tissue taken from it that appears\nto be abnormal. If the abnormal location is easily accessible, the specimen could\nbe obtained at the office of a doctor. If the cancer is more deeply embedded in the\nmouth or throat, biopsy procedures might have to be carried out in a surgical theatre\nwhile receiving anesthesia in order to lessen pain.\n Exfoliative cytology: Cell samples are gently scraped from a questionable loca-\ntion. To make the cells visible under a microscope, they are put upon a transparent\n4\nslide, and subsequently colored. A deeper biopsy will be done if any cells seem\nsuspicious.\nImage-based tests\n Computerized Tomography, or CT, Scanning  Information on the size, shape, and\nlocation of any tumors can be obtained via a CT scan, which can help detect lymph\nnodes that are bulging that may contain cancer cells.\n Magnetic Resonance Imaging (MRI): Oral cancer may be examined with an MRI\nscan, although this is less usual. MRIs give a very thorough picture and may be\nvery helpful in figuring out whether other areas of the body, such as the neck, have\nbeen affected by the diseases spread.\n Positron emission computed tomography (PET): Patients with cancer of the oral\ncavity might get a scan using PET technology to find out whether the disease has\nmigrated to the lymph nodes or whether it has only recently progressed to that\nlocation.\n Genomic testing for advanced oral cancer: -Genomic testing is sometimes known as\nmolecular profiling or cancer sequencing. Examining the collected cells is required\n8 from a biopsy in order to check for any genetic mutations (changes in your DNA)\nthat might be connected to the persons particular type of cancer.","recorded":"2024-10-24 16:36:34.237729235","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/mouth structure.png","recorded":"2024-10-24 14:44:02.307887243","filePath":"null","pinned":false},{"value":"The Mandible (Lower Jawbone)","recorded":"2024-10-24 14:42:09.927807496","filePath":"null","pinned":false},{"value":"Teeth, gums, and alveolar ridge, which is the ridge-like border of the jaws that\ncontains the tooth sockets.","recorded":"2024-10-24 14:41:53.330676270","filePath":"null","pinned":false},{"value":"The roof of the mouth","recorded":"2024-10-24 14:41:48.149986970","filePath":"null","pinned":false},{"value":"The buccal mucosa, which coats the cheekbones interior","recorded":"2024-10-24 14:41:37.876343214","filePath":"null","pinned":false},{"value":"The Uvula and the Tongue","recorded":"2024-10-24 14:41:31.764317912","filePath":"null","pinned":false},{"value":"The Tonsils and The Soft Palate","recorded":"2024-10-24 14:30:07.501523361","filePath":"null","pinned":false},{"value":"The Lips","recorded":"2024-10-24 14:29:37.758953009","filePath":"null","pinned":false},{"value":"The human mouth commences at the junction of the lips and skin. Figure 1.1 illustrates the anatomy of the human oral cavity. The palette consists of both hard and soft components. The soft palate separates the mouth from the nasopharynx, the upper segment of the pharynx, which is linked to the mouth through the oropharynx, the middle region of the pharynx. The lateral aspects of the mouth are constituted by the inner surface of the cheeks (De Angeli et al. 2022). The tongue occupies the majority of the floor of the mouth.\nThe mouth can be categorized into several components, including:","recorded":"2024-10-24 14:29:14.604650366","filePath":"null","pinned":false},{"value":"The start of the human mouth is where the lips and skin converge Figure 1.1 shows the\nstructure of the human mouth. The roof of the mouth is made up of both hard and soft\npalates. A soft palate divides the mouth from the nasopharynx (the upper part of the\npharynx), which is connected to the mouth via the oropharynx (the middle section of the\npharynx). The sides of the mouth are formed by the cheeks inner surface (De Angeli et\nal. 2022). The majority of the mouths floor, or lowest portion, is occupied by the tongue.\nThe mouth can be divided into various sections, including-:","recorded":"2024-10-24 14:29:03.166788082","filePath":"null","pinned":false},{"value":"/mnt/Media/Magician Launcher.app\n/mnt/Media/Magician Launcher.exe\n/mnt/Media/RootCA.crt","recorded":"2024-10-24 14:10:33.155916880","filePath":"null","pinned":false},{"value":"/run/media/karna/T7/Magician Launcher.app\n/run/media/karna/T7/Magician Launcher.exe\n/run/media/karna/T7/RootCA.crt","recorded":"2024-10-24 14:08:36.993377931","filePath":"null","pinned":false},{"value":"Mutations in the cells of the mouth or lips lead to the development of oral carcinomas. DNA contains the directives for cellular functions. When typically functioning cells would perish, modifications induce the persistent proliferation and division of the cells. The anomalous oral carcinoma cells might aggregate into a neoplasm. Eventually, they may disseminate from the oral cavity throughout the entire body, encompassing the neck and other regions of the head.\nMouth cancers typically originate in the flat, thin squamous cells that comprise the surface of the lips and the interior of the mouth. Oral cancer is predominantly induced by squamous cell carcinomas.","recorded":"2024-10-24 13:56:56.620897761","filePath":"null","pinned":false},{"value":"When DNA alterations (mutations) occur in the mouth or lip cells, mouth carcinomas de-\nvelop. DNA includes the instructions for what the cell must accomplish. When normally\nfunctioning cells would die, alterations cause the continued growth and division of the\ncells. The aberrant mouth cancer cells can assemble into a tumor. In time, they might\nspread from the inside of the mouth to the whole body, including the neck or various parts\nof the head.\nMouth cancers tend to start in the flat, thin cells (squamous cells) which define the sur-\nface of the lips and the interior of the mouth. Oral cancer is most frequently caused by\nsquamous cell tumors.","recorded":"2024-10-24 13:56:40.926593375","filePath":"null","pinned":false},{"value":"Cancer can develop in the lips, tongue, inner lining of the cheek, gums, mouth, and the hard and soft palate.","recorded":"2024-10-24 13:55:22.507309735","filePath":"null","pinned":false},{"value":"The following organs can develop cancer:\n\\begin{enumerate}\n    \\item Lips\n    \\item Tongue\n    \\item Inner lining of the cheek\n    \\item Gums\n    \\item Mouth Cancer\n    \\item Hard and Soft Palate\n\\end{enumerate}","recorded":"2024-10-24 13:55:00.043487526","filePath":"null","pinned":false},{"value":"Painful or arduous deglutition","recorded":"2024-10-24 13:54:40.201201929","filePath":"null","pinned":false},{"value":"A growth or protrusion within the oral cavity; mobility of teeth;  Painful or arduous deglutition","recorded":"2024-10-24 13:54:33.615566190","filePath":"null","pinned":false},{"value":"An oral or labial ulcer that remains unhealed","recorded":"2024-10-24 13:54:26.320951419","filePath":"null","pinned":false},{"value":"An intraoral lesion that is either white or red","recorded":"2024-10-24 13:54:20.249709675","filePath":"null","pinned":false},{"value":"An internal mouth patch that is either white or red","recorded":"2024-10-24 13:54:04.870851335","filePath":"null","pinned":false},{"value":"A mouth or lip sore that does not heal","recorded":"2024-10-24 13:53:57.931914779","filePath":"null","pinned":false},{"value":"Mouth cancer, commonly referred to as oral cancer, happens whenever a tumor forms\ninside the mouth lining. It could be located on the surface of the tongue, the interior of\nthe cheeks, the palate, the lips themselves, or the gums. Additionally, the glands that\ncreate tumors saliva, the tonsils in the rear within the mouth. But these occur frequently.\nSymptoms of mouth cancer:","recorded":"2024-10-24 13:53:22.116184847","filePath":"null","pinned":false},{"value":"Mouth cancer, or oral cancer, occurs when a tumor develops within the lining of the mouth. It may be situated on the tongue's surface, the inner cheeks, the palate, the lips, or the gums. Furthermore, the glands responsible for saliva production, specifically the tonsils located at the posterior region of the oral cavity, might develop malignancies. However, these events transpire with regularity.\nManifestations of oral cancer:","recorded":"2024-10-24 13:53:13.703937480","filePath":"null","pinned":false},{"value":"Mouth cancer, commonly referred to as oral cancer, happens whenever a tumor forms\ninside the mouth lining. It could be located on the surface of the tongue, the interior of\nthe cheeks, the palate, the lips themselves, or the gums. Additionally, the glands that\ncreate tumors saliva, the tonsils in the rear within the mouth. But these occur frequently.\nSymptoms of mouth cancer:\n A mouth or lip sore that does not heal\n An internal mouth patch that is either white or red\n A growth or bulge inside your mouth; loose teeth;\n Painful or difficult swallowing","recorded":"2024-10-24 13:52:47.918901975","filePath":"null","pinned":false},{"value":"\\subsection{Oral Cancer} \\label{sec-01.01}","recorded":"2024-10-24 13:52:24.180689948","filePath":"null","pinned":false},{"value":"The following organs can develop cancer:\n Lips\n Tongue\n Inner lining of the cheek\n Gums\n Mouth Cancer\n Hard and Soft Palate","recorded":"2024-10-24 13:34:22.098787690","filePath":"null","pinned":false},{"value":"Type of Oral Cancer","recorded":"2024-10-24 13:33:12.677862193","filePath":"null","pinned":false},{"value":"The cells of the mouth are the initial sites of oral cancer development. A malignant nodule is a cluster of cancer cells capable of infiltrating adjacent tissue and causing significant harm. It can also metastasis to various regions of the body. The lymph nodes in the neck are the primary sites for the metastasis of oral cancer. Oral cancer is also known as mouth cancer. Occasionally, oral cells undergo alterations that impede their growth or correct functioning. These modifications may lead to benign malignancies such as warts and fibromas. Precancerous conditions may also be induced by alterations in the cells of the oral cavity. This suggests that while the aberrant cells are currently not cancerous, there exists a possibility that they may progress to cancer if left untreated. Leukoplakia and erythroplakia are two of the most common precancerous conditions of the oral cavity.\nOral cancer may, however, occasionally arise from modifications to the cellular architecture of the mouth. The oral mucosa is a mucosal membrane that lines the oral cavity. The squamous epithelium, constituting the oral mucosa, consists of squamous cells. Mouth cancer often originates in these thin, flat squamous cells. The designation for this type of malignancy is oral squamous cell carcinoma.","recorded":"2024-10-24 13:32:44.422801143","filePath":"null","pinned":false},{"value":"The mouths cells are the first to develop oral cancer. A cancerous (malignant) nodule is\na group of cancer cells tumor that has the ability to invade neighboring tissue and wreck\nmisery on it. It can also metastasize to different parts of the body. Nodes of lymph in\nthe neck are the part where mouth cancer spreads most frequently. Oral cancer may also\nbe referred to as mouth cancer. Sometimes, cells that are present in the mouth undergo\nchanges and will stop growing or behaving properly. These alterations could result in\nbenign (non-cancerous) tumors like warts and fibromas. Precancerous diseases can also\nbe brought on by changes in the mouths cells. This indicates that although the abnormal\ncells are not now cancer, there is a potential that they could develop into cancer if lefuntreated. Leukoplakia and erythroplakia are two of the most prevalent precancerous\ndisorders of the mouth.\nOral cancer can, however, occasionally result from alterations to the mouths cellular\nstructure. The oral mucosa (mucous membrane) is a lining that lines the mouth. The\nsquamous epithelium, which composes the oral mucosa, is made up of squamous cells.\nThese thin, flat squamous cells are where mouth cancer typically begins. The term for\nthis type of cancer is mouth squamous cell carcinoma.","recorded":"2024-10-24 13:31:23.499089763","filePath":"null","pinned":false},{"value":"Oral Cancer detection using Deep Learning","recorded":"2024-10-24 13:30:48.931760804","filePath":"null","pinned":false},{"value":"Cancer is a disease caused by abnormal cells proliferating uncontrollably within the body. A segment of the body's cells in all cancers initiates fast division and disseminates to adjacent tissues. Cancer can manifest in practically any location within the multitude of cells in the body. Typically, human cells proliferate and divide to generate new cells as needed by the body. When a cell sustains injury or reaches senescence, it ceases to function and is supplanted by a new cell. However, when cancer progresses, this systemic mechanism deteriorates.\nOld or damaged cells that ought to have perished persist, as cells increasingly become erroneous, while new cells are produced even when they are undesirable. These cells possess the capability to proliferate, potentially resulting in tumor-like formations. Solid tumors, or tissue lumps, are a prevalent kind of cancer. Leukemias and other hematologic malignancies generally do not become solid tumors.","recorded":"2024-10-24 13:29:36.710421208","filePath":"null","pinned":false},{"value":"Cancer is a disease brought on by aberrant cells when an internal component is expanding\nout of control. A portion of the bodys cells in all tumors, begin to divide rapidly and\nspread to parts of the neighboring tissues. Among the millions of cells, cancer can appear\nvirtually at any place in the body. Normally, human cells multiply and divide to produce\nnew cells as the body requires them. When a cell becomes damaged or old, it expires and\nis replaced by a fresh cell. But as cancer grows, this systematic mechanism disintegrates.\nOld or injured cells that should have died survive, as cells become more and more erro-\nneous whereas new cells are generated even when they are unwanted. These cells can\ndivide to form new ones, which may lead to tumor-like growth. Solid tumors, or masses\nof tissue, are a common kind of cancer. Leukemias and other blood cancers typically do\nnot develop solid tumors","recorded":"2024-10-24 13:27:07.852367038","filePath":"null","pinned":false},{"value":"1.7\n Deep Learning\nOne of the main components of an Artificially Intelligent system is learning. Learning\nmeans when a computer program can learn through its surrounding. Artificially intelli-\ngent systems have the ability to mimic the human brain and have the ability to process\ninformation and develop various patterns used to make decisions (Dubuc et al. 2022). A\nsub type of machine learning called \"deep learning\" in artificial intelligence (AI) allows\nnetworks to learn unsupervised from unlabelled input. Deep learning can also refer to\ndeep neural networks or deep learning.\n1.7.1\n Importance of Deep Learning\nMachine learning techniques can now build and learn from a large pool of training data\nbecause to improvements in computer speed and memory over time.\nDeep learning has been a cutting-edge method for humanity, especially when the Informa-\ntion is noisy. Artificial neural networks can learn any function with just one hidden layer,\nregardless of how ambiguous it is, which is why they are regarded as universal function\napproximations.\n7\n1.7.2\n CNN\nConvolutional neural network (CNN) is a subtype of ANN. In at least one of their layers,\nCNNs replace conventional matrix multiplication methods with the convolution mathe-\nmatical technique. Since they were developed specifically to handle pixel data, they are\nused in image recognition and processing. The design with which CNN is built is compa-\nrable with the model of neural connection like a persons brain (Jeyaraj, B. K. Panigrahi,\nand Samuel Nadar 2022). Because of the way CNN is built, there are some strong prefer-\nences ingrained in them, which makes it easier to comprehend why they are so effective.\nCNN can be seen as a feed-forward network but having connection with each image can\nFigure 1.3: A CNN Architecture\n(Sun et al. 2019)\nbe inefficient. Therefore, we can prune the useless connection between the hidden layers\nto increase the performance of the layer. A CNN is a special artificial neural network with\nlimited connections between the layers of artificial neural network.\n Max-Pooling: Each feature map produced by processing the input through many\nlayers of convolution is subsequently combined in a pooling layer. Little grids are\nused for input for pooling procedures, which generate only one value for every re-\ngion. The pooling layers provide CNN significant translational consistency since a\n8\ntiny change in the input image causes a slight modification in the activation maps.\nApplying convolutions with longer strides is another method for obtaining the pool-\nings down sampling effect. The network design is made simpler by eliminating the\npooling levels without compromising performance. Max-pooling is the most widely\nemployed of all these pooling techniques.\n Fully-Connected Layers: Matrix multiplications have traditionally been the build-\ning blocks of neural networks, which are scattered with sigmoid nonlinearities. The\nlayers of the multiplication matrices are referred to as connected layers due to the\nconnection between each unit in the layer before and each unit in the layer af-\nter. There is just small-scale spatial connectivity when using convolutional layers.\nSignificant amounts of completely linked layers are typically avoided in modern\nnetworks since they require massive parameters.\n Learning algorithm: Lacking an algorithm to quickly and effectively learn the pa-\nrameters of the model, there is little value for an expensive model. Lacking a tech-\nnique for efficiently acquiring the models parameters, a strong, expressive model\nis of little use. In the pre-AlexNet era, greedy layer-wise pre-training techniques\nattempted to create such an efficient approach. A more straightforward supervised\ntraining approach is sufficient to learn a reliable model for tasks relating to com-\nputer vision.\n Optimization Based on Gradient: - Typically, the backpropagation technique is used\nto train networks, which accelerates mathematical calculation to calculate the gra-\ndient used in the Gradient Descent (GD) algorithm. However, employing GD is\nimpracticable for datasets with many hundreds or even more data points. In these\ncircumstances, Stochastic Gradient Descent (SGD), an approximation where gradi-\nents are computed for data points individually rather than the complete data set, is\nfrequently used. Training using SGD generalizes more successfully than with GD,\nit has been discovered.\n Batch Normalization:- A helpful regularizes that enhances generalisation and sharply\naccelerates convergence is batch normalisation (BN). The order of presentation of\nthe inputs to each layer varies continuously during the training phase, which is a\nproblem caused by inner covariate variation. This effect typically causes training\n9\nto take longer and requires careful initialization. This problem is addressed by BN,\nwhich normalises a layers production stimulation to ensure that its spectrum is\nconstrained to a restricted range. In particular, BN normalises each mini-batchs\nmean-variance statistics using its running average. Recently, BN has been recog-\nnised as a crucial element of very deep networks.\n Activation layer :- Deep networks typically have convolutions after each layer,\nwhich then follows a nonlinear process. This is required because convolutions are\nan example of a cascading linear system. Layer-to-layer nonlinearities make the\nmodel more evocative than a model with linear dynamics. Theoretically, as long\nas nonlinearities are ongoing bounded, and gradually rising, no nonlinearity has a\ngreater capacity for expressiveness than any other. The sigmoid or the tanh were\nnonlinearities employed in classical neural networks that feed forward. However,\nthe Rectified Linear Unit (ReLU) is used in contemporary convolutional networks.\nIt has been discovered that CNNs with this nonlinearity train more quickly. The\nleaky- ReLU is a brand-new category of nonlinearity that has lately been intro-\nduced. Leaky-ReLU(x) = max(0, x) + min(0, x) is its formula, where is a preset\nparameter. It is better since it implies that the characteristic can also be taught,\ncreating a model that is considerably deeper. Leaky ReLUs or adjustable ReLUs\nare examples of variations on ReLU(z)=max (0; z). The feature maps, which are\nfrequently also referred to as feature maps, are fed through a process of activation\nto create new tensors.\n1.7.3\n Working on Deep Learning Networks\nSince most deep learning methods rely on neural network topologies, they are referred\ndescribed as \"deep neural networks\".\nNormal neural nets only have a few hidden levels, whereas deeper networks may contain\nup to 150 layers. Very vast quantity of categorised autonomously generated data and neu-\nral network topology extract features.\na) Training from Scratch:- For a deep network to be trained from beginning, a very large\nlabelled data set must be gathered, and a network architecture must be created that will\nallow the network to gain insight into its characteristics and predict. This is advantageous\nfor newly developed apps or applications with numerous output categories. This is a less\n10\nfrequent strategy because these networks often take weeks or even months to train be-\ncause to the volume of data and learning rate.\nb) Transfer Learning:- It is a deep learning technique where a pre-trained model is mod-\nified as part of the transfer learning approach. It begins with a reliable network, like\nAlexNet or GoogleNet, then feeds it new values which are previously undiscovered classes.\nThe task can now be carried, out after making network modifications that are minimal.\nMoreover, processing hundreds of photographs as opposed to millions has the advantage\nof requiring much less data, which cuts down computation time to minutes or hours.\nc) Feature Extraction:- The network can be used as a feature extractor, which is a little less\ntypical and a more specialised method of deep learning. Feature Extraction can remove\nspecific features from the network at any point throughout the training process because\nall the layers are charged with learning specific features from images.\n1.7.4\n Purpose of Deep Learning\nThe models developed using Deep Learning have the potential to provide more precise\nand individualised cancer treatment by better predicting the prognosis of the disease. They\nare superior to or on par with the methods now used in clinical settings. Deep learning\ntechniques are anticipated to help in the proper handling of squamous cell carcinoma of\nthe oral cavity through enhanced diagnostic performance, wise clinical decision-making,\nstreamlining of clinicians work, the potential for lowering cancer screening costs, and a\nsuccessful evaluation and detection of the disease. In order to increase the quality of care,\nprofessionals and patients can spend more time talking to one another and deliberating\ntogether. Future research should focus on creating deep learning models that integrate\ndiverse datasets from many modalities.\n Pre Processing: Due to a variety of factors, the original image will always contain\nsome noise. The accuracy of the diagnosis is compromised by these noises. A cru-\ncial part of the image processing process is pre-processing. Asymmetric filtration is\na filter that is frequently used to enhance grayscale photographs by reducing noise\nand improving image arrangement, particularly edge boundaries.\n Feature Extraction: - we can generate new features from the previous feature and\nthen we can delete the original features by doing this we can reduce the features\n11\npresent in the dataset. It helps us to categorize the images into different groups.\n Feature Selection: - Providing a vast amount of features to the model can result\nin a overfitted model with a very high computational time, having a better feature\nextraction will help in reducing the time complexity\nFigure 1.4: Flow chart showing different Phases in detection of oral cancer\n1.8\n Metaheuristic Optimization\nReal-world optimisation issues frequently involve a large number of choice variables, in-\ntricate nonlinear constraints, and difficult objective functions, which makes them more\nand more difficult to solve. Using conventional strategies like numerical methods, the\n12\nglobal optimization is less effective, particularly when limitations or objective functions\ninclude many peaks. Strong instruments for tackling difficult optimisation problems,\nmetaheuristic algorithms are gaining popularity.\nThe simplicity of metaheuristic algorithms is by far their most notable feature. The fun-\ndamental theories or mathematical models underlying these metaheuristic techniques are\nderived from nature. The majority of these techniques are straightforward and simple to\nuse. One can utilise metaheuristics to solve real-world problems thanks to their usability.\nAdditionally, it is simple to create their versions using current techniques.\nThese optimisation technologies can be thought of as \"black boxes,\" capable of providing\na set of outputs for a specific problem for a specific set of inputs. One of the most crucial\naspects of metaheuristic algorithms is randomization. This makes it possible for meta-\nheuristic algorithms to effectively avoid trapping in local optima and to search the whole\nsearch space. More specifically, it enables numerous metaheuristics to handle issues in-\nvolving an ambiguous search space or various local optima. Finally, because of their ex-\ntreme adaptability and flexibility, these metaheuristics can be used to solve a wide range\nof optimisation issues, including non-linear issues, issues involving non-differentiable\nvariables, and issues involving sophisticated numerical calculations and a large number\nof local minima.\n1.9\n Motivation\nThe latest trend in increase of oral cancer is having an adverse effect on health of human\nbeing. Oral cancer can be treated if detected early, with the increase in total number of\ncases of oral cancer we need an accurate and fast way to detect cancer cells. The risk\nof oral cancer is in all age groups but elder people are more prone to it due to unhealthy\nlifestyle. A lot of people have experienced financial troubles. It is crucial for the early\ndiagnosis of disease so that patients can start taking preventative measures right away. AI,\nwhich consists of machine learning and deep learning, is heavily reliant on classification,\ngrading, segmentation, and computer vision. To more or less better model optimisation is\nthe main reason for conducting research in this field.\n Deep learning concept fascinate me to learn more in this area. Deep learning based\nmodel can detect oral cancer with early signs that can be captured by modern cam-\n13\neras\n Clinical Images can give an more accurate and fast result as compare to normal\nmethods applied by Doctors\n1.10\n Problem Statement and Research Objective\nA large number of deaths were recorded from oral cancer as a result of lack of its identifi-\ncation and late treatment. Oral cavity cancer has a significant mortality rate that is rising.\nIt is crucial to develop and put into practise a method for detecting this malignancy early\non. By identifying cancer early and adopting preventative measures, it is simple to limit\nthe number of deaths brought on by the disease. Although many researchers have already\nconducted their research in the field of oral cancer disorders, there is still a great deal of\nresearch that may be done in this area owing to performance improvements.\nMachine learning has advanced to the point where getting more use out of it is all but\nimpossible during the last several years. The performance of Deep learning models have\nincreased but there is still a concern of model size, low accuracy and high computation\ntime.\n To propose and implement optimized Deep learning algorithm for detection of oral\ncancer in its early stages.\n To implement Metaheuristic optimization for better weight selection of clinical im-\nages.\n To conduct an analysis and compare the proposed approach with state of art models\non basic of evaluation matrices like accuracy, precision, Sensitivity and Specificity.\n1.11\n Thesis outline\nThe chapters of the thesis are organised consistently into an overview, key facts and fig-\nures, significant content, pertinent data, and a final chapter summary. All references are\nincluded at the end and each Figure, table, and piece of text is correctly referenced. The\nfive chapters that make up this thesis are arranged as follows:\n14\n Chapter 1: It provides a succinct overview of oral cancer, including its kinds, symp-\ntoms, and methods of diagnosis. It describes how the process of making medical\ndiagnoses has been transformed by machine learning, neural networks, and deep\nneural networks. Why has CNN surpassed conventional neural networks? what\nmotivated and inspired you to work in medicine. Additionally, it provides informa-\ntion about the goals and motivation.\n Chapter 2: It gives a brief overview of the literature for a number of researchers who\nworked on various methods for automatic oral cancer diagnosis, image processing,\nand texture-based categorization. Artificial neural networks, deep learning. A re-\nview of all pertinent theories and techniques for diagnosing oral cancer that are\navailable in the literature.\n Chapter 3: The chapter sheds insight on a crucial experiment study and the approach\nused to carry out our investigation. The models and various detection architectures\nemployed by CNN have been described. The proposed model is covered in this\nchapter; it has fewer parameters and a shallower learning curve than the pretrained\nmodel, but it is more accurate.\n Chapter 4: With the use of a graph, bar chart, and other presentation approaches,\nall model and performance metric results are shown. The models shortcomings are\nthen displayed and contrasted with the suggested model.\n Chapter 5: The entire work is concluded in the last chapter. This chapter also\ndiscusses how we might enhance our efforts in the future.","recorded":"2024-10-24 13:23:17.540417744","filePath":"null","pinned":false},{"value":"Cancer is a disease brought on by aberrant cells when an internal component is expanding\nout of control. A portion of the bodys cells in all tumors, begin to divide rapidly and\nspread to parts of the neighboring tissues. Among the millions of cells, cancer can appear\nvirtually at any place in the body. Normally, human cells multiply and divide to produce\nnew cells as the body requires them. When a cell becomes damaged or old, it expires and\nis replaced by a fresh cell. But as cancer grows, this systematic mechanism disintegrates.\nOld or injured cells that should have died survive, as cells become more and more erro-\nneous whereas new cells are generated even when they are unwanted. These cells can\ndivide to form new ones, which may lead to tumor-like growth. Solid tumors, or masses\nof tissue, are a common kind of cancer. Leukemias and other blood cancers typically do\nnot develop solid tumors\n1.1\n Oral Cancer\nThe mouths cells are the first to develop oral cancer. A cancerous (malignant) nodule is\na group of cancer cells tumor that has the ability to invade neighboring tissue and wreck\nmisery on it. It can also metastasize to different parts of the body. Nodes of lymph in\nthe neck are the part where mouth cancer spreads most frequently. Oral cancer may also\nbe referred to as mouth cancer. Sometimes, cells that are present in the mouth undergo\nchanges and will stop growing or behaving properly. These alterations could result in\nbenign (non-cancerous) tumors like warts and fibromas. Precancerous diseases can also\nbe brought on by changes in the mouths cells. This indicates that although the abnormal\ncells are not now cancer, there is a potential that they could develop into cancer if lefuntreated. Leukoplakia and erythroplakia are two of the most prevalent precancerous\ndisorders of the mouth.\nOral cancer can, however, occasionally result from alterations to the mouths cellular\nstructure. The oral mucosa (mucous membrane) is a lining that lines the mouth. The\nsquamous epithelium, which composes the oral mucosa, is made up of squamous cells.\nThese thin, flat squamous cells are where mouth cancer typically begins. The term for\nthis type of cancer is mouth squamous cell carcinoma.\n1.1.1\n Type of Oral Cancer\nThe following organs can develop cancer:\n Lips\n Tongue\n Inner lining of the cheek\n Gums\n Mouth Cancer\n Hard and Soft Palate\n1.2\n Mouth Cancer\nMouth cancer, commonly referred to as oral cancer, happens whenever a tumor forms\ninside the mouth lining. It could be located on the surface of the tongue, the interior of\nthe cheeks, the palate, the lips themselves, or the gums. Additionally, the glands that\ncreate tumors saliva, the tonsils in the rear within the mouth. But these occur frequently.\nSymptoms of mouth cancer:\n A mouth or lip sore that does not heal\n An internal mouth patch that is either white or red\n A growth or bulge inside your mouth; loose teeth;\n Painful or difficult swallowing\n2\n1.2.1\n Cause of Mouth Cancer\nWhen DNA alterations (mutations) occur in the mouth or lip cells, mouth carcinomas de-\nvelop. DNA includes the instructions for what the cell must accomplish. When normally\nfunctioning cells would die, alterations cause the continued growth and division of the\ncells. The aberrant mouth cancer cells can assemble into a tumor. In time, they might\nspread from the inside of the mouth to the whole body, including the neck or various parts\nof the head.\nMouth cancers tend to start in the flat, thin cells (squamous cells) which define the sur-\nface of the lips and the interior of the mouth. Oral cancer is most frequently caused by\nsquamous cell tumors.\n1.3\n Human Mouth Structure\nThe start of the human mouth is where the lips and skin converge Figure 1.1 shows the\nstructure of the human mouth. The roof of the mouth is made up of both hard and soft\npalates. A soft palate divides the mouth from the nasopharynx (the upper part of the\npharynx), which is connected to the mouth via the oropharynx (the middle section of the\npharynx). The sides of the mouth are formed by the cheeks inner surface (De Angeli et\nal. 2022). The majority of the mouths floor, or lowest portion, is occupied by the tongue.\nThe mouth can be divided into various sections, including-:\n The Lips\n The Tonsils and The Soft Palate\n The Uvula and the Tongue\n The buccal mucosa, which coats the cheekbones interior\n The roof of the mouth\n Teeth, gums, and alveolar ridge, which is the ridge-like border of the jaws that\ncontains the tooth sockets.\n The Mandible (Lower Jawbone)\n3\nFigure 1.1: Structure of Human mouth\n(German and Palmer 2006)\n1.4\n Diagnosing Techniques for Oral Cancer\nThere are different techniques that are used for the diagnosis of oral cancer, few of the\nclinical techniques used by doctors are discussed below :-\nBarium Swallow: - The voice box, the throat, referral, and surroundings may display\nabnormalities during a barium swallow test, which is also frequently used to find small,\nearly oral tumors.\nBiopsy: - The initial step in identifying mouth cancer is an oral tissue biopsy. A little bit\nof aberrant tissue from the area where oral cancer is suspected is removed by the surgeon\nduring the biopsy. An oral cancer diagnosis may be confirmed by biopsy. The following\ntypes of biopsies are frequently used to identify oral carcinoma:\n Incisive biopsies: The region has a small amount of tissue taken from it that appears\nto be abnormal. If the abnormal location is easily accessible, the specimen could\nbe obtained at the office of a doctor. If the cancer is more deeply embedded in the\nmouth or throat, biopsy procedures might have to be carried out in a surgical theatre\nwhile receiving anesthesia in order to lessen pain.\n Exfoliative cytology: Cell samples are gently scraped from a questionable loca-\ntion. To make the cells visible under a microscope, they are put upon a transparent\n4\nslide, and subsequently colored. A deeper biopsy will be done if any cells seem\nsuspicious.\nImage-based tests\n Computerized Tomography, or CT, Scanning  Information on the size, shape, and\nlocation of any tumors can be obtained via a CT scan, which can help detect lymph\nnodes that are bulging that may contain cancer cells.\n Magnetic Resonance Imaging (MRI): Oral cancer may be examined with an MRI\nscan, although this is less usual. MRIs give a very thorough picture and may be\nvery helpful in figuring out whether other areas of the body, such as the neck, have\nbeen affected by the diseases spread.\n Positron emission computed tomography (PET): Patients with cancer of the oral\ncavity might get a scan using PET technology to find out whether the disease has\nmigrated to the lymph nodes or whether it has only recently progressed to that\nlocation.\n Genomic testing for advanced oral cancer: -Genomic testing is sometimes known as\nmolecular profiling or cancer sequencing. Examining the collected cells is required\n8 from a biopsy in order to check for any genetic mutations (changes in your DNA)\nthat might be connected to the persons particular type of cancer.\n1.5\n Oral cancer: Globally\nAmong the most prevalent malignancies worldwide is oral cancer. The majority of cases\nof this subtype of head and neck cancer begin in the cells of squamous tissue that cover\nthe surface of our mouth, tongue, and faces. When this fails to be identified and if not\naddressed in a timely manner, it could be deadly. About 53,000 incidences of oral cancer,\nor three percent of all cancers identified during the study in US annually, are related to\noral cancer. Oral cancer strikes males more frequently than females, more than twice as\noften, and persons over an age of 40 are most at risk.\nSmoking, drinking alcoholic beverages, or having HPV, short for People Papilloma virus\ninfection are the main causes of oral cancer. In 2020, there are expected to be over 177,000\n5\ndeaths globally from lip and oral cavity cancer, In spite of improvements mouth cancer\nfatality rates have remained high in recent decades.\nThe majority of mouth cancer patients, particularly those located in countryside regions,\ncant obtain fast, effective diagnosis and treatment, which lowers their chance of survival.\nDepending on race and location, patients with cancer have a five-year living rate among\nthe 50%. According to reports, the survival rate in developed nations can reach 65%.\nIn contrast, leaning upon the area of the mouth cancer affected, a living rate of fifteen\npercent is noted in some countryside areas. Its because cancer therapy may be highly\nexpensive, especially in later stages. Health experts and the general public both lack a\nsignificant grasp of oral cancer. The 2020 Cancer Statistics Report for India states 66.6\npercent of patients with head and neck cancer had already progressed locally when they\nreceived their diagnosis. Inflammation or ulcers that do not heal, along with discomfort\nand bleeding, are signs of oral cancer.\nOral cancer can be caused by a number of habits, with smoking and drinking being the\ntwo most significant ones. Consuming maggots is so common in India that it causes in-\nternal gum damage.\nGLOBOCAN (Global Cancer Incidence, Mortality and Prevalence) anticipated that in\nFigure 1.2: Global age standardized prevalence of tobacco smoking source World Health\nOrganization\n(Dai, Gakidou, and Lopez 2022)\n2018, there would be 177,384 cancer-related deaths and 354,864 new instances of cancer,\n6\nwhich corresponds to two percent and one point nine percent of all occurrences and fa-\ntalities from cancer, respectively. In summary, mouth cancer, which accounts for around\none-third of all cancer cases, is a major reason for death in Bangladesh, Pakistan, Taiwan,\nand India.\n1.6\n Issues with Oral Cancer Manual Diagnoses\nThe primary problem with manual cancer diagnosis is the delay in diagnosis. It requires\nextremely competent labor, and the number of needed diagnostic tests is increasing dra-\nmatically. Because of the time requirements for a proper diagnosis, it is less likely that\nan early identification of the tumor grade will be made.Pathologists heavy workload is a\nserious worry, and this also affects how well they can anticipate outcomes. It also Prevent\nthe delivery of an accurate diagnosis report as the findings must be carefully crafted to\navoid any fatalities.","recorded":"2024-10-24 13:23:05.035134500","filePath":"null","pinned":false},{"value":"Cancer is a disease brought on by aberrant cells when an internal component is expanding\nout of control. A portion of the bodys cells in all tumors, begin to divide rapidly and\nspread to parts of the neighboring tissues. Among the millions of cells, cancer can appear\nvirtually at any place in the body. Normally, human cells multiply and divide to produce\nnew cells as the body requires them. When a cell becomes damaged or old, it expires and\nis replaced by a fresh cell. But as cancer grows, this systematic mechanism disintegrates.\nOld or injured cells that should have died survive, as cells become more and more erro-\nneous whereas new cells are generated even when they are unwanted. These cells can\ndivide to form new ones, which may lead to tumor-like growth. Solid tumors, or masses\nof tissue, are a common kind of cancer. Leukemias and other blood cancers typically do\nnot develop solid tumors\n1.1\n Oral Cancer\nThe mouths cells are the first to develop oral cancer. A cancerous (malignant) nodule is\na group of cancer cells tumor that has the ability to invade neighboring tissue and wreck\nmisery on it. It can also metastasize to different parts of the body. Nodes of lymph in\nthe neck are the part where mouth cancer spreads most frequently. Oral cancer may also\nbe referred to as mouth cancer. Sometimes, cells that are present in the mouth undergo\nchanges and will stop growing or behaving properly. These alterations could result in\nbenign (non-cancerous) tumors like warts and fibromas. Precancerous diseases can also\nbe brought on by changes in the mouths cells. This indicates that although the abnormal\ncells are not now cancer, there is a potential that they could develop into cancer if lefuntreated. Leukoplakia and erythroplakia are two of the most prevalent precancerous\ndisorders of the mouth.\nOral cancer can, however, occasionally result from alterations to the mouths cellular\nstructure. The oral mucosa (mucous membrane) is a lining that lines the mouth. The\nsquamous epithelium, which composes the oral mucosa, is made up of squamous cells.\nThese thin, flat squamous cells are where mouth cancer typically begins. The term for\nthis type of cancer is mouth squamous cell carcinoma.\n1.1.1\n Type of Oral Cancer\nThe following organs can develop cancer:\n Lips\n Tongue\n Inner lining of the cheek\n Gums\n Mouth Cancer\n Hard and Soft Palate\n1.2\n Mouth Cancer\nMouth cancer, commonly referred to as oral cancer, happens whenever a tumor forms\ninside the mouth lining. It could be located on the surface of the tongue, the interior of\nthe cheeks, the palate, the lips themselves, or the gums. Additionally, the glands that\ncreate tumors saliva, the tonsils in the rear within the mouth. But these occur frequently.\nSymptoms of mouth cancer:\n A mouth or lip sore that does not heal\n An internal mouth patch that is either white or red\n A growth or bulge inside your mouth; loose teeth;\n Painful or difficult swallowing\n2\n1.2.1\n Cause of Mouth Cancer\nWhen DNA alterations (mutations) occur in the mouth or lip cells, mouth carcinomas de-\nvelop. DNA includes the instructions for what the cell must accomplish. When normally\nfunctioning cells would die, alterations cause the continued growth and division of the\ncells. The aberrant mouth cancer cells can assemble into a tumor. In time, they might\nspread from the inside of the mouth to the whole body, including the neck or various parts\nof the head.\nMouth cancers tend to start in the flat, thin cells (squamous cells) which define the sur-\nface of the lips and the interior of the mouth. Oral cancer is most frequently caused by\nsquamous cell tumors.\n1.3\n Human Mouth Structure\nThe start of the human mouth is where the lips and skin converge Figure 1.1 shows the\nstructure of the human mouth. The roof of the mouth is made up of both hard and soft\npalates. A soft palate divides the mouth from the nasopharynx (the upper part of the\npharynx), which is connected to the mouth via the oropharynx (the middle section of the\npharynx). The sides of the mouth are formed by the cheeks inner surface (De Angeli et\nal. 2022). The majority of the mouths floor, or lowest portion, is occupied by the tongue.\nThe mouth can be divided into various sections, including-:\n The Lips\n The Tonsils and The Soft Palate\n The Uvula and the Tongue\n The buccal mucosa, which coats the cheekbones interior\n The roof of the mouth\n Teeth, gums, and alveolar ridge, which is the ridge-like border of the jaws that\ncontains the tooth sockets.\n The Mandible (Lower Jawbone)\n3\nFigure 1.1: Structure of Human mouth\n(German and Palmer 2006)\n1.4\n Diagnosing Techniques for Oral Cancer\nThere are different techniques that are used for the diagnosis of oral cancer, few of the\nclinical techniques used by doctors are discussed below :-\nBarium Swallow: - The voice box, the throat, referral, and surroundings may display\nabnormalities during a barium swallow test, which is also frequently used to find small,\nearly oral tumors.\nBiopsy: - The initial step in identifying mouth cancer is an oral tissue biopsy. A little bit\nof aberrant tissue from the area where oral cancer is suspected is removed by the surgeon\nduring the biopsy. An oral cancer diagnosis may be confirmed by biopsy. The following\ntypes of biopsies are frequently used to identify oral carcinoma:\n Incisive biopsies: The region has a small amount of tissue taken from it that appears\nto be abnormal. If the abnormal location is easily accessible, the specimen could\nbe obtained at the office of a doctor. If the cancer is more deeply embedded in the\nmouth or throat, biopsy procedures might have to be carried out in a surgical theatre\nwhile receiving anesthesia in order to lessen pain.\n Exfoliative cytology: Cell samples are gently scraped from a questionable loca-\ntion. To make the cells visible under a microscope, they are put upon a transparent\n4\nslide, and subsequently colored. A deeper biopsy will be done if any cells seem\nsuspicious.\nImage-based tests\n Computerized Tomography, or CT, Scanning  Information on the size, shape, and\nlocation of any tumors can be obtained via a CT scan, which can help detect lymph\nnodes that are bulging that may contain cancer cells.\n Magnetic Resonance Imaging (MRI): Oral cancer may be examined with an MRI\nscan, although this is less usual. MRIs give a very thorough picture and may be\nvery helpful in figuring out whether other areas of the body, such as the neck, have\nbeen affected by the diseases spread.\n Positron emission computed tomography (PET): Patients with cancer of the oral\ncavity might get a scan using PET technology to find out whether the disease has\nmigrated to the lymph nodes or whether it has only recently progressed to that\nlocation.\n Genomic testing for advanced oral cancer: -Genomic testing is sometimes known as\nmolecular profiling or cancer sequencing. Examining the collected cells is required\n8 from a biopsy in order to check for any genetic mutations (changes in your DNA)\nthat might be connected to the persons particular type of cancer.\n1.5\n Oral cancer: Globally\nAmong the most prevalent malignancies worldwide is oral cancer. The majority of cases\nof this subtype of head and neck cancer begin in the cells of squamous tissue that cover\nthe surface of our mouth, tongue, and faces. When this fails to be identified and if not\naddressed in a timely manner, it could be deadly. About 53,000 incidences of oral cancer,\nor three percent of all cancers identified during the study in US annually, are related to\noral cancer. Oral cancer strikes males more frequently than females, more than twice as\noften, and persons over an age of 40 are most at risk.\nSmoking, drinking alcoholic beverages, or having HPV, short for People Papilloma virus\ninfection are the main causes of oral cancer. In 2020, there are expected to be over 177,000\n5\ndeaths globally from lip and oral cavity cancer, In spite of improvements mouth cancer\nfatality rates have remained high in recent decades.\nThe majority of mouth cancer patients, particularly those located in countryside regions,\ncant obtain fast, effective diagnosis and treatment, which lowers their chance of survival.\nDepending on race and location, patients with cancer have a five-year living rate among\nthe 50%. According to reports, the survival rate in developed nations can reach 65%.\nIn contrast, leaning upon the area of the mouth cancer affected, a living rate of fifteen\npercent is noted in some countryside areas. Its because cancer therapy may be highly\nexpensive, especially in later stages. Health experts and the general public both lack a\nsignificant grasp of oral cancer. The 2020 Cancer Statistics Report for India states 66.6\npercent of patients with head and neck cancer had already progressed locally when they\nreceived their diagnosis. Inflammation or ulcers that do not heal, along with discomfort\nand bleeding, are signs of oral cancer.\nOral cancer can be caused by a number of habits, with smoking and drinking being the\ntwo most significant ones. Consuming maggots is so common in India that it causes in-\nternal gum damage.\nGLOBOCAN (Global Cancer Incidence, Mortality and Prevalence) anticipated that in\nFigure 1.2: Global age standardized prevalence of tobacco smoking source World Health\nOrganization\n(Dai, Gakidou, and Lopez 2022)\n2018, there would be 177,384 cancer-related deaths and 354,864 new instances of cancer,\n6\nwhich corresponds to two percent and one point nine percent of all occurrences and fa-\ntalities from cancer, respectively. In summary, mouth cancer, which accounts for around\none-third of all cancer cases, is a major reason for death in Bangladesh, Pakistan, Taiwan,\nand India.\n1.6\n Issues with Oral Cancer Manual Diagnoses\nThe primary problem with manual cancer diagnosis is the delay in diagnosis. It requires\nextremely competent labor, and the number of needed diagnostic tests is increasing dra-\nmatically. Because of the time requirements for a proper diagnosis, it is less likely that\nan early identification of the tumor grade will be made.Pathologists heavy workload is a\nserious worry, and this also affects how well they can anticipate outcomes. It also Prevent\nthe delivery of an accurate diagnosis report as the findings must be carefully crafted to\navoid any fatalities.\n1.7\n Deep Learning\nOne of the main components of an Artificially Intelligent system is learning. Learning\nmeans when a computer program can learn through its surrounding. Artificially intelli-\ngent systems have the ability to mimic the human brain and have the ability to process\ninformation and develop various patterns used to make decisions (Dubuc et al. 2022). A\nsub type of machine learning called \"deep learning\" in artificial intelligence (AI) allows\nnetworks to learn unsupervised from unlabelled input. Deep learning can also refer to\ndeep neural networks or deep learning.\n1.7.1\n Importance of Deep Learning\nMachine learning techniques can now build and learn from a large pool of training data\nbecause to improvements in computer speed and memory over time.\nDeep learning has been a cutting-edge method for humanity, especially when the Informa-\ntion is noisy. Artificial neural networks can learn any function with just one hidden layer,\nregardless of how ambiguous it is, which is why they are regarded as universal function\napproximations.\n7\n1.7.2\n CNN\nConvolutional neural network (CNN) is a subtype of ANN. In at least one of their layers,\nCNNs replace conventional matrix multiplication methods with the convolution mathe-\nmatical technique. Since they were developed specifically to handle pixel data, they are\nused in image recognition and processing. The design with which CNN is built is compa-\nrable with the model of neural connection like a persons brain (Jeyaraj, B. K. Panigrahi,\nand Samuel Nadar 2022). Because of the way CNN is built, there are some strong prefer-\nences ingrained in them, which makes it easier to comprehend why they are so effective.\nCNN can be seen as a feed-forward network but having connection with each image can\nFigure 1.3: A CNN Architecture\n(Sun et al. 2019)\nbe inefficient. Therefore, we can prune the useless connection between the hidden layers\nto increase the performance of the layer. A CNN is a special artificial neural network with\nlimited connections between the layers of artificial neural network.\n Max-Pooling: Each feature map produced by processing the input through many\nlayers of convolution is subsequently combined in a pooling layer. Little grids are\nused for input for pooling procedures, which generate only one value for every re-\ngion. The pooling layers provide CNN significant translational consistency since a\n8\ntiny change in the input image causes a slight modification in the activation maps.\nApplying convolutions with longer strides is another method for obtaining the pool-\nings down sampling effect. The network design is made simpler by eliminating the\npooling levels without compromising performance. Max-pooling is the most widely\nemployed of all these pooling techniques.\n Fully-Connected Layers: Matrix multiplications have traditionally been the build-\ning blocks of neural networks, which are scattered with sigmoid nonlinearities. The\nlayers of the multiplication matrices are referred to as connected layers due to the\nconnection between each unit in the layer before and each unit in the layer af-\nter. There is just small-scale spatial connectivity when using convolutional layers.\nSignificant amounts of completely linked layers are typically avoided in modern\nnetworks since they require massive parameters.\n Learning algorithm: Lacking an algorithm to quickly and effectively learn the pa-\nrameters of the model, there is little value for an expensive model. Lacking a tech-\nnique for efficiently acquiring the models parameters, a strong, expressive model\nis of little use. In the pre-AlexNet era, greedy layer-wise pre-training techniques\nattempted to create such an efficient approach. A more straightforward supervised\ntraining approach is sufficient to learn a reliable model for tasks relating to com-\nputer vision.\n Optimization Based on Gradient: - Typically, the backpropagation technique is used\nto train networks, which accelerates mathematical calculation to calculate the gra-\ndient used in the Gradient Descent (GD) algorithm. However, employing GD is\nimpracticable for datasets with many hundreds or even more data points. In these\ncircumstances, Stochastic Gradient Descent (SGD), an approximation where gradi-\nents are computed for data points individually rather than the complete data set, is\nfrequently used. Training using SGD generalizes more successfully than with GD,\nit has been discovered.\n Batch Normalization:- A helpful regularizes that enhances generalisation and sharply\naccelerates convergence is batch normalisation (BN). The order of presentation of\nthe inputs to each layer varies continuously during the training phase, which is a\nproblem caused by inner covariate variation. This effect typically causes training\n9\nto take longer and requires careful initialization. This problem is addressed by BN,\nwhich normalises a layers production stimulation to ensure that its spectrum is\nconstrained to a restricted range. In particular, BN normalises each mini-batchs\nmean-variance statistics using its running average. Recently, BN has been recog-\nnised as a crucial element of very deep networks.\n Activation layer :- Deep networks typically have convolutions after each layer,\nwhich then follows a nonlinear process. This is required because convolutions are\nan example of a cascading linear system. Layer-to-layer nonlinearities make the\nmodel more evocative than a model with linear dynamics. Theoretically, as long\nas nonlinearities are ongoing bounded, and gradually rising, no nonlinearity has a\ngreater capacity for expressiveness than any other. The sigmoid or the tanh were\nnonlinearities employed in classical neural networks that feed forward. However,\nthe Rectified Linear Unit (ReLU) is used in contemporary convolutional networks.\nIt has been discovered that CNNs with this nonlinearity train more quickly. The\nleaky- ReLU is a brand-new category of nonlinearity that has lately been intro-\nduced. Leaky-ReLU(x) = max(0, x) + min(0, x) is its formula, where is a preset\nparameter. It is better since it implies that the characteristic can also be taught,\ncreating a model that is considerably deeper. Leaky ReLUs or adjustable ReLUs\nare examples of variations on ReLU(z)=max (0; z). The feature maps, which are\nfrequently also referred to as feature maps, are fed through a process of activation\nto create new tensors.\n1.7.3\n Working on Deep Learning Networks\nSince most deep learning methods rely on neural network topologies, they are referred\ndescribed as \"deep neural networks\".\nNormal neural nets only have a few hidden levels, whereas deeper networks may contain\nup to 150 layers. Very vast quantity of categorised autonomously generated data and neu-\nral network topology extract features.\na) Training from Scratch:- For a deep network to be trained from beginning, a very large\nlabelled data set must be gathered, and a network architecture must be created that will\nallow the network to gain insight into its characteristics and predict. This is advantageous\nfor newly developed apps or applications with numerous output categories. This is a less\n10\nfrequent strategy because these networks often take weeks or even months to train be-\ncause to the volume of data and learning rate.\nb) Transfer Learning:- It is a deep learning technique where a pre-trained model is mod-\nified as part of the transfer learning approach. It begins with a reliable network, like\nAlexNet or GoogleNet, then feeds it new values which are previously undiscovered classes.\nThe task can now be carried, out after making network modifications that are minimal.\nMoreover, processing hundreds of photographs as opposed to millions has the advantage\nof requiring much less data, which cuts down computation time to minutes or hours.\nc) Feature Extraction:- The network can be used as a feature extractor, which is a little less\ntypical and a more specialised method of deep learning. Feature Extraction can remove\nspecific features from the network at any point throughout the training process because\nall the layers are charged with learning specific features from images.\n1.7.4\n Purpose of Deep Learning\nThe models developed using Deep Learning have the potential to provide more precise\nand individualised cancer treatment by better predicting the prognosis of the disease. They\nare superior to or on par with the methods now used in clinical settings. Deep learning\ntechniques are anticipated to help in the proper handling of squamous cell carcinoma of\nthe oral cavity through enhanced diagnostic performance, wise clinical decision-making,\nstreamlining of clinicians work, the potential for lowering cancer screening costs, and a\nsuccessful evaluation and detection of the disease. In order to increase the quality of care,\nprofessionals and patients can spend more time talking to one another and deliberating\ntogether. Future research should focus on creating deep learning models that integrate\ndiverse datasets from many modalities.\n Pre Processing: Due to a variety of factors, the original image will always contain\nsome noise. The accuracy of the diagnosis is compromised by these noises. A cru-\ncial part of the image processing process is pre-processing. Asymmetric filtration is\na filter that is frequently used to enhance grayscale photographs by reducing noise\nand improving image arrangement, particularly edge boundaries.\n Feature Extraction: - we can generate new features from the previous feature and\nthen we can delete the original features by doing this we can reduce the features\n11\npresent in the dataset. It helps us to categorize the images into different groups.\n Feature Selection: - Providing a vast amount of features to the model can result\nin a overfitted model with a very high computational time, having a better feature\nextraction will help in reducing the time complexity\nFigure 1.4: Flow chart showing different Phases in detection of oral cancer\n1.8\n Metaheuristic Optimization\nReal-world optimisation issues frequently involve a large number of choice variables, in-\ntricate nonlinear constraints, and difficult objective functions, which makes them more\nand more difficult to solve. Using conventional strategies like numerical methods, the\n12\nglobal optimization is less effective, particularly when limitations or objective functions\ninclude many peaks. Strong instruments for tackling difficult optimisation problems,\nmetaheuristic algorithms are gaining popularity.\nThe simplicity of metaheuristic algorithms is by far their most notable feature. The fun-\ndamental theories or mathematical models underlying these metaheuristic techniques are\nderived from nature. The majority of these techniques are straightforward and simple to\nuse. One can utilise metaheuristics to solve real-world problems thanks to their usability.\nAdditionally, it is simple to create their versions using current techniques.\nThese optimisation technologies can be thought of as \"black boxes,\" capable of providing\na set of outputs for a specific problem for a specific set of inputs. One of the most crucial\naspects of metaheuristic algorithms is randomization. This makes it possible for meta-\nheuristic algorithms to effectively avoid trapping in local optima and to search the whole\nsearch space. More specifically, it enables numerous metaheuristics to handle issues in-\nvolving an ambiguous search space or various local optima. Finally, because of their ex-\ntreme adaptability and flexibility, these metaheuristics can be used to solve a wide range\nof optimisation issues, including non-linear issues, issues involving non-differentiable\nvariables, and issues involving sophisticated numerical calculations and a large number\nof local minima.\n1.9\n Motivation\nThe latest trend in increase of oral cancer is having an adverse effect on health of human\nbeing. Oral cancer can be treated if detected early, with the increase in total number of\ncases of oral cancer we need an accurate and fast way to detect cancer cells. The risk\nof oral cancer is in all age groups but elder people are more prone to it due to unhealthy\nlifestyle. A lot of people have experienced financial troubles. It is crucial for the early\ndiagnosis of disease so that patients can start taking preventative measures right away. AI,\nwhich consists of machine learning and deep learning, is heavily reliant on classification,\ngrading, segmentation, and computer vision. To more or less better model optimisation is\nthe main reason for conducting research in this field.\n Deep learning concept fascinate me to learn more in this area. Deep learning based\nmodel can detect oral cancer with early signs that can be captured by modern cam-\n13\neras\n Clinical Images can give an more accurate and fast result as compare to normal\nmethods applied by Doctors\n1.10\n Problem Statement and Research Objective\nA large number of deaths were recorded from oral cancer as a result of lack of its identifi-\ncation and late treatment. Oral cavity cancer has a significant mortality rate that is rising.\nIt is crucial to develop and put into practise a method for detecting this malignancy early\non. By identifying cancer early and adopting preventative measures, it is simple to limit\nthe number of deaths brought on by the disease. Although many researchers have already\nconducted their research in the field of oral cancer disorders, there is still a great deal of\nresearch that may be done in this area owing to performance improvements.\nMachine learning has advanced to the point where getting more use out of it is all but\nimpossible during the last several years. The performance of Deep learning models have\nincreased but there is still a concern of model size, low accuracy and high computation\ntime.\n To propose and implement optimized Deep learning algorithm for detection of oral\ncancer in its early stages.\n To implement Metaheuristic optimization for better weight selection of clinical im-\nages.\n To conduct an analysis and compare the proposed approach with state of art models\non basic of evaluation matrices like accuracy, precision, Sensitivity and Specificity.\n1.11\n Thesis outline\nThe chapters of the thesis are organised consistently into an overview, key facts and fig-\nures, significant content, pertinent data, and a final chapter summary. All references are\nincluded at the end and each Figure, table, and piece of text is correctly referenced. The\nfive chapters that make up this thesis are arranged as follows:\n14\n Chapter 1: It provides a succinct overview of oral cancer, including its kinds, symp-\ntoms, and methods of diagnosis. It describes how the process of making medical\ndiagnoses has been transformed by machine learning, neural networks, and deep\nneural networks. Why has CNN surpassed conventional neural networks? what\nmotivated and inspired you to work in medicine. Additionally, it provides informa-\ntion about the goals and motivation.\n Chapter 2: It gives a brief overview of the literature for a number of researchers who\nworked on various methods for automatic oral cancer diagnosis, image processing,\nand texture-based categorization. Artificial neural networks, deep learning. A re-\nview of all pertinent theories and techniques for diagnosing oral cancer that are\navailable in the literature.\n Chapter 3: The chapter sheds insight on a crucial experiment study and the approach\nused to carry out our investigation. The models and various detection architectures\nemployed by CNN have been described. The proposed model is covered in this\nchapter; it has fewer parameters and a shallower learning curve than the pretrained\nmodel, but it is more accurate.\n Chapter 4: With the use of a graph, bar chart, and other presentation approaches,\nall model and performance metric results are shown. The models shortcomings are\nthen displayed and contrasted with the suggested model.\n Chapter 5: The entire work is concluded in the last chapter. This chapter also\ndiscusses how we might enhance our efforts in the future.","recorded":"2024-10-24 13:19:00.175188261","filePath":"null","pinned":false},{"value":"chat.tinygrad.org","recorded":"2024-10-24 11:39:19.851389071","filePath":"null","pinned":false},{"value":"\"idle_inhibitor\": {\n        \"format\": \"{icon}\",\n        \"format-icons\": {\n            \"activated\": \"\",\n            \"deactivated\": \"\"\n        }\n    },","recorded":"2024-10-23 22:21:53.227543478","filePath":"null","pinned":false},{"value":"# Load the model\nsaved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_model.h5\"\nmodel = tf.keras.models.load_model(saved_model_path)\n\n# Since training history is saved to CSV, load it from there\nimport pandas as pd\ntraining_history = pd.read_csv(log_path)\n\n# Get generator and discriminator losses\ngenerator_loss = training_history['Generator Loss'].tolist()\ndiscriminator_loss = training_history['Discriminator Loss'].tolist()","recorded":"2024-10-23 15:18:08.864613066","filePath":"null","pinned":false},{"value":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nimport matplotlib.pyplot as plt\nimport csv\nimport os\n\n# Vanilla GAN model\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path, log_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    \n    # Create log directory if it doesn't exist\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Open the log file\n    with open(log_path, mode='w', newline='') as log_file:\n        log_writer = csv.writer(log_file)\n        # Write the header\n        log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n        \n        for epoch in range(epochs):\n            # Train Discriminator\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n            discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n            \n            # Train Generator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            generator_loss = gan.train_on_batch(noise, real_labels)\n            \n            # Log losses\n            print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n            log_writer.writerow([epoch, discriminator_loss[0], generator_loss])\n            vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n            vgan_losses['generator_loss'].append(generator_loss)\n\n            if epoch % 1000 == 0:\n                save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), \n                             path=f\"/workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n\n    # Save the entire GAN model\n    gan.save(save_path)\n\nepochs = 10000\nbatch_size = 64\n\ndef save_images(images, path='/workspace/dataset/chest_xray/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\n# Define the log path\nlog_path = '/workspace/dataset/chest_xray/training_logs.csv'\n\n# Train the GAN with the log path\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size, \n           '/workspace/dataset/chest_xray/lungs_generator_model.h5', log_path)\n","recorded":"2024-10-23 15:18:02.274697649","filePath":"null","pinned":false},{"value":"workspace/dataset/chest_xray/lungs_generator_weights2.h5","recorded":"2024-10-23 15:17:41.112533238","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[10], line 2\n      1 saved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_weights2.h5\"\n----\u003e 2 model = tf.keras.models.load_model(saved_model_path)\n      4 training_history = model.history.history\n      6 # Get generator and discriminator losses\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py:262, in load_model(filepath, custom_objects, compile, safe_mode, **kwargs)\n    254     return saving_lib.load_model(\n    255         filepath,\n    256         custom_objects=custom_objects,\n    257         compile=compile,\n    258         safe_mode=safe_mode,\n    259     )\n    261 # Legacy case.\n--\u003e 262 return legacy_sm_saving_lib.load_model(\n    263     filepath, custom_objects=custom_objects, compile=compile, **kwargs\n    264 )\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.\u003clocals\u003e.error_handler(*args, **kwargs)\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n     68     # To get the full stack trace, call:\n     69     # `tf.debugging.disable_traceback_filtering()`\n---\u003e 70     raise e.with_traceback(filtered_tb) from None\n     71 finally:\n     72     del filtered_tb\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/saving/legacy/hdf5_format.py:197, in load_model_from_hdf5(filepath, custom_objects, compile)\n    195 model_config = f.attrs.get(\"model_config\")\n    196 if model_config is None:\n--\u003e 197     raise ValueError(\n    198         f\"No model config found in the file at {filepath}.\"\n    199     )\n    200 if hasattr(model_config, \"decode\"):\n    201     model_config = model_config.decode(\"utf-8\")\n\nValueError: No model config found in the file at \u003ctensorflow.python.platform.gfile.GFile object at 0x798a1043efd0\u003e.","recorded":"2024-10-23 15:16:50.971438848","filePath":"null","pinned":false},{"value":"saved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_weights2.h5\"\nmodel = tf.keras.models.load_model(saved_model_path)\n\ntraining_history = model.history.history\n\n# Get generator and discriminator losses\ngenerator_loss = training_history['generator_loss']\ndiscriminator_loss = training_history['discriminator_loss']","recorded":"2024-10-23 15:16:45.900218321","filePath":"null","pinned":false},{"value":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nimport matplotlib.pyplot as plt\nimport csv\nimport os\n\n# Vanilla GAN model\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path, log_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    \n    # Create log directory if it doesn't exist\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Open the log file\n    with open(log_path, mode='w', newline='') as log_file:\n        log_writer = csv.writer(log_file)\n        # Write the header\n        log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n        \n        for epoch in range(epochs):\n            # Train Discriminator\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n            discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n            \n            # Train Generator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            generator_loss = gan.train_on_batch(noise, real_labels)\n            \n            # Log losses\n            print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n            log_writer.writerow([epoch, discriminator_loss[0], generator_loss])\n            vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n            vgan_losses['generator_loss'].append(generator_loss)\n\n            if epoch % 1000 == 0:\n                save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), \n                             path=f\"/workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n\n    # Save generator weights\n    generator.save_weights(save_path)\n\nepochs = 10000\nbatch_size = 64\n\ndef save_images(images, path='/workspace/dataset/chest_xray/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\n# Define the log path\nlog_path = '/workspace/dataset/chest_xray/training_logs.csv'\n\n# Train the GAN with the log path\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size, \n           '/workspace/dataset/chest_xray/lungs_generator_weights2.h5', log_path)\n","recorded":"2024-10-23 15:16:40.267587080","filePath":"null","pinned":false},{"value":"tf.keras.models.load_model(save_path)","recorded":"2024-10-23 15:15:38.242544530","filePath":"null","pinned":false},{"value":"# Example of getting losses after training\ntraining_history = {'generator_loss': vgan_losses['generator_loss'], 'discriminator_loss': vgan_losses['discriminator_loss']}\ngenerator_loss = training_history['generator_loss']\ndiscriminator_loss = training_history['discriminator_loss']","recorded":"2024-10-23 15:15:12.522462191","filePath":"null","pinned":false},{"value":"import tensorflow as tf\n\nsaved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_model.h5\"\nmodel = tf.keras.models.load_model(saved_model_path)\n\n# Accessing training history requires saving it separately, as it is not part of the model\n# Example: loading from a CSV file where you saved it during training\nimport pandas as pd\n\nlog_path = '/workspace/dataset/chest_xray/training_logs.csv'\ntraining_history = pd.read_csv(log_path)\n\n# Get generator and discriminator losses\ngenerator_loss = training_history['Generator Loss'].tolist()\ndiscriminator_loss = training_history['Discriminator Loss'].tolist()\n","recorded":"2024-10-23 15:14:30.450980053","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[7], line 2\n      1 saved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_weights2.h5\"\n----\u003e 2 model = tf.keras.models.load_model(saved_model_path)\n      3 training_history = model.history.history\n      5 # Get generator and discriminator losses\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py:262, in load_model(filepath, custom_objects, compile, safe_mode, **kwargs)\n    254     return saving_lib.load_model(\n    255         filepath,\n    256         custom_objects=custom_objects,\n    257         compile=compile,\n    258         safe_mode=safe_mode,\n    259     )\n    261 # Legacy case.\n--\u003e 262 return legacy_sm_saving_lib.load_model(\n    263     filepath, custom_objects=custom_objects, compile=compile, **kwargs\n    264 )\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.\u003clocals\u003e.error_handler(*args, **kwargs)\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n     68     # To get the full stack trace, call:\n     69     # `tf.debugging.disable_traceback_filtering()`\n---\u003e 70     raise e.with_traceback(filtered_tb) from None\n     71 finally:\n     72     del filtered_tb\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/saving/legacy/hdf5_format.py:197, in load_model_from_hdf5(filepath, custom_objects, compile)\n    195 model_config = f.attrs.get(\"model_config\")\n    196 if model_config is None:\n--\u003e 197     raise ValueError(\n    198         f\"No model config found in the file at {filepath}.\"\n    199     )\n    200 if hasattr(model_config, \"decode\"):\n    201     model_config = model_config.decode(\"utf-8\")\n\nValueError: No model config found in the file at \u003ctensorflow.python.platform.gfile.GFile object at 0x798a3057a410\u003e.","recorded":"2024-10-23 15:14:19.150224938","filePath":"null","pinned":false},{"value":"saved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_weights2.h5\"\nmodel = tf.keras.models.load_model(saved_model_path)\ntraining_history = model.history.history\n\n# Get generator and discriminator losses\ngenerator_loss = training_history['generator_loss']\ndiscriminator_loss = training_history['discriminator_loss']","recorded":"2024-10-23 15:14:11.185804051","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/lalitha-j-a9672011b/","recorded":"2024-10-23 15:11:15.447686927","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/priyanka-sharma-p12/","recorded":"2024-10-23 15:10:33.373665656","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/prashar-snigdha/","recorded":"2024-10-23 15:09:46.341044396","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/shreya-padmanabhan/","recorded":"2024-10-23 15:09:02.085985621","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/varshar25/","recorded":"2024-10-23 15:08:16.094438114","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/ishudayma/","recorded":"2024-10-23 15:06:52.278131146","filePath":"null","pinned":false},{"value":"ResMed","recorded":"2024-10-23 14:52:54.099645650","filePath":"null","pinned":false},{"value":"workspace/dataset/chest_xray/generated_imagesImproved","recorded":"2024-10-23 13:29:48.152296524","filePath":"null","pinned":false},{"value":"2024-10-23 07:36:57.832679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43598 MB memory:  -\u003e device: 0, name: NVIDIA A40, pci bus id: 0000:ce:00.0, compute capability: 8.6\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[6], line 101\n     98         plt.savefig(f\"{path}generated_image_{i}.png\")\n     99         plt.close()\n--\u003e 101 train_gan(generator, discriminator, gan, real_data, epochs, batch_size,'/workspace/dataset/chest_xray/lungs_generator_weights2.h5')\n\nTypeError: train_gan() missing 1 required positional argument: 'log_path'","recorded":"2024-10-23 13:09:47.746726859","filePath":"null","pinned":false},{"value":"# Vanilla GAN model\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path, log_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    \n    # Create log directory if it doesn't exist\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Open the log file\n    with open(log_path, mode='w', newline='') as log_file:\n        log_writer = csv.writer(log_file)\n        # Write the header\n        log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n        \n        for epoch in range(epochs):\n            # Train Discriminator\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n            discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n            \n            # Train Generator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            generator_loss = gan.train_on_batch(noise, real_labels)\n            \n            # Log losses\n            print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n            log_writer.writerow([epoch, discriminator_loss[0], generator_loss])\n            vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n            vgan_losses['generator_loss'].append(generator_loss)\n\n            if epoch % 1000 == 0:\n                save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), \n                             path=f\"/workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n\n    # Save generator weights\n    generator.save_weights(save_path)\n\nepochs=10000\nbatch_size=64\ndef save_images(images, path='/workspace/dataset/chest_xray/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size,'/workspace/dataset/chest_xray/lungs_generator_weights2.h5')\n","recorded":"2024-10-23 13:09:37.435511593","filePath":"null","pinned":false},{"value":"def train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path, log_path):\n    # Create log directory if it doesn't exist\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Open the log file\n    with open(log_path, mode='w', newline='') as log_file:\n        log_writer = csv.writer(log_file)\n        # Write the header\n        log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n\n        for epoch in range(epochs):\n            for _ in range(batch_size):\n                idx = np.random.randint(0, real_data.shape[0], batch_size)\n                real_images = real_data[idx]\n\n                # Train the discriminator\n                noise = np.random.normal(0, 1, (batch_size, latent_dim))\n                fake_images = generator.predict(noise)\n                epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n                interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n                interpolated_samples = tf.Variable(interpolated_samples, dtype=tf.float32)\n                \n                with tf.GradientTape() as tape:\n                    tape.watch(interpolated_samples)\n                    pred_real = discriminator(real_images)\n                    pred_fake = discriminator(fake_images)\n                    gradient_penalty = gradient_penalty_loss(real_images, fake_images, discriminator)\n                    d_loss = tf.reduce_mean(pred_fake) - tf.reduce_mean(pred_real) + 10 * gradient_penalty\n                \n                grads = tape.gradient(d_loss, discriminator.trainable_weights)\n                discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n\n            # Train the generator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            g_loss = wgan_gp.train_on_batch(noise, -np.ones((batch_size, 1)))\n\n            # Log losses\n            print(f\"Epoch {epoch}, Discriminator Loss: {d_loss.numpy()}, Generator Loss: {g_loss}\")\n            log_writer.writerow([epoch, d_loss.numpy(), g_loss])\n            wgan_losses['discriminator_loss'].append(d_loss.numpy())\n            wgan_losses['generator_loss'].append(g_loss)\n\n            if epoch % 20 == 0:\n                save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), \n                             path=f\"/workspace/dataset/chest_xray/generated_imagesWGAN1/epoch_{epoch}\")\n\n    # Save generator weights\n    generator.save_weights(save_path)","recorded":"2024-10-23 13:03:49.429649034","filePath":"null","pinned":false},{"value":"def train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path):\n    for epoch in range(epochs):\n        for _ in range(batch_size):\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n\n            # Train the discriminator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n            interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n            interpolated_samples = tf.Variable(interpolated_samples, dtype=tf.float32)\n            with tf.GradientTape() as tape:\n                tape.watch(interpolated_samples)\n                pred_real = discriminator(real_images)\n                pred_fake = discriminator(fake_images)\n                gradient_penalty = gradient_penalty_loss(real_images, fake_images,discriminator)\n                d_loss = tf.reduce_mean(pred_fake) - tf.reduce_mean(pred_real) + 10 * gradient_penalty\n            grads = tape.gradient(d_loss, discriminator.trainable_weights)\n            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n\n        # Train the generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        g_loss = wgan_gp.train_on_batch(noise, -np.ones((batch_size, 1)))\n\n        print(f\"Epoch {epoch}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n        wgan_losses['discriminator_loss'].append(d_loss)\n        wgan_losses['generator_loss'].append(g_loss)\n        if epoch % 20 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"/workspace/dataset/chest_xray/generated_imagesWGAN1/epoch_{epoch}\")\n\n    generator.save_weights(save_path)","recorded":"2024-10-23 13:03:28.181245694","filePath":"null","pinned":false},{"value":"/workspace/dataset/chest_xray","recorded":"2024-10-23 13:02:21.098520401","filePath":"null","pinned":false},{"value":"#WGAN model\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# Define the generator\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    return model\n\n# Define the gradient penalty\ndef gradient_penalty_loss(real_images, fake_images, discriminator):\n    batch_size = tf.shape(real_images)[0]\n\n    # Randomly interpolate between real and fake samples\n    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n    interpolated = alpha * real_images + (1 - alpha) * fake_images\n\n    with tf.GradientTape() as tape:\n        tape.watch(interpolated)\n        pred_interpolated = discriminator(interpolated)\n\n    gradients = tape.gradient(pred_interpolated, interpolated)\n    gradients_l2_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n    gradient_penalty = tf.reduce_mean(tf.square(gradients_l2_norm - 1.0))\n\n    return gradient_penalty\n\n# Define the WGAN-GP model\ndef build_wgan_gp(generator, discriminator, latent_dim):\n    z = layers.Input(shape=(latent_dim,))\n    img = generator(z)\n    valid = discriminator(img)\n    return models.Model(z, valid)\n\n# Define parameters\nlatent_dim = 100\n\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss='mse')\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the WGAN-GP\nwgan_gp = build_wgan_gp(generator, discriminator, latent_dim)\n\n# Compile the WGAN-GP\nwgan_gp.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss=lambda y_true, y_pred: -y_pred)\nwgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the WGAN-GP\ndef train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path):\n    for epoch in range(epochs):\n        for _ in range(batch_size):\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n\n            # Train the discriminator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n            interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n            interpolated_samples = tf.Variable(interpolated_samples, dtype=tf.float32)\n            with tf.GradientTape() as tape:\n                tape.watch(interpolated_samples)\n                pred_real = discriminator(real_images)\n                pred_fake = discriminator(fake_images)\n                gradient_penalty = gradient_penalty_loss(real_images, fake_images,discriminator)\n                d_loss = tf.reduce_mean(pred_fake) - tf.reduce_mean(pred_real) + 10 * gradient_penalty\n            grads = tape.gradient(d_loss, discriminator.trainable_weights)\n            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n\n        # Train the generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        g_loss = wgan_gp.train_on_batch(noise, -np.ones((batch_size, 1)))\n\n        print(f\"Epoch {epoch}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n        wgan_losses['discriminator_loss'].append(d_loss)\n        wgan_losses['generator_loss'].append(g_loss)\n        if epoch % 20 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"/content/drive/My Drive/Lung dataset/generated_imagesWGAN1/epoch_{epoch}\")\n\n    generator.save_weights(save_path)\n\n# Train the WGAN-GP\nepochs = 1000\nbatch_size =64\n\ndef save_images(images, path='/content/drive/My Drive/Lung dataset/generated_imagesWGAN1'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\ntrain_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, '/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights_wgan_gp1.h5')\n","recorded":"2024-10-23 13:02:12.594784467","filePath":"null","pinned":false},{"value":"def train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path, log_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    \n    # Create log directory if it doesn't exist\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Open the log file\n    with open(log_path, mode='w', newline='') as log_file:\n        log_writer = csv.writer(log_file)\n        # Write the header\n        log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n        \n        for epoch in range(epochs):\n            # Train Discriminator\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n            discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n            \n            # Train Generator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            generator_loss = gan.train_on_batch(noise, real_labels)\n            \n            # Log losses\n            print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n            log_writer.writerow([epoch, discriminator_loss[0], generator_loss])\n            vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n            vgan_losses['generator_loss'].append(generator_loss)\n\n            if epoch % 1000 == 0:\n                save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), \n                             path=f\"/workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n\n    # Save generator weights\n    generator.save_weights(save_path)","recorded":"2024-10-23 13:01:17.831338061","filePath":"null","pinned":false},{"value":"learning_rate","recorded":"2024-10-23 13:00:54.075018222","filePath":"null","pinned":false},{"value":"def train_gan(generator, discriminator, gan, real_data, epochs, batch_size,save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n        discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n        vgan_losses['generator_loss'].append(generator_loss)\n        if epoch % 1000 == 0:\n          save_images(generator.predict(np.random.normal(0, 1, (25,latent_dim))), path=f\"/workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n    generator.save_weights(save_path)","recorded":"2024-10-23 13:00:27.519605253","filePath":"null","pinned":false},{"value":"# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n\n        # Discriminator training\n        discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        \n        # If the outputs are not tuples, just assign them directly\n        if isinstance(discriminator_loss_real, tuple):\n            discriminator_loss_real_value = discriminator_loss_real[0]\n            discriminator_accuracy_real = discriminator_loss_real[1]\n        else:\n            discriminator_loss_real_value = discriminator_loss_real\n            discriminator_accuracy_real = 0  # or handle it as needed\n\n        if isinstance(discriminator_loss_fake, tuple):\n            discriminator_loss_fake_value = discriminator_loss_fake[0]\n            discriminator_accuracy_fake = discriminator_loss_fake[1]\n        else:\n            discriminator_loss_fake_value = discriminator_loss_fake\n            discriminator_accuracy_fake = 0  # or handle it as needed\n\n        # Combine the losses and accuracies\n        discriminator_loss = 0.5 * (discriminator_loss_real_value + discriminator_loss_fake_value)\n        discriminator_accuracy = 0.5 * (discriminator_accuracy_real + discriminator_accuracy_fake)\n\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n\n        # Log the results\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss}, Discriminator Accuracy: {discriminator_accuracy}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss)\n        vgan_losses['generator_loss'].append(generator_loss)\n        vgan_losses['discriminator_accuracy'].append(discriminator_accuracy)\n\n        # Log to CSV\n        with open(log_file_path, mode='a', newline='') as log_file:\n            log_writer = csv.writer(log_file)\n            log_writer.writerow([epoch, discriminator_loss, discriminator_accuracy, generator_loss, 'N/A'])  # Generator accuracy can be complex to compute.\n\n        if epoch % 1000 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n    \n    generator.save_weights(save_path)\n","recorded":"2024-10-23 12:56:15.792818230","filePath":"null","pinned":false},{"value":"2024-10-23 07:14:14.261599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43598 MB memory:  -\u003e device: 0, name: NVIDIA A40, pci bus id: 0000:ce:00.0, compute capability: 8.6\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n2024-10-23 07:14:15.319684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8905\n2024-10-23 07:14:15.411235: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n2/2 [==============================] - 1s 4ms/step\n\n2024-10-23 07:14:15.714699: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2024-10-23 07:14:16.327901: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n2024-10-23 07:14:17.508732: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f808c1af080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2024-10-23 07:14:17.508758: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6\n2024-10-23 07:14:17.514387: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2024-10-23 07:14:17.626066: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[6], line 118\n    115 batch_size = 64\n    117 # Start training the GAN\n--\u003e 118 train_gan(generator, discriminator, gan, real_data, epochs, batch_size, 'workspace/dataset/chest_xray/lungs_generator_weights2.h5')\n\nCell In[6], line 89, in train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path)\n     86 generator_loss = gan.train_on_batch(noise, real_labels)\n     88 # Log the results\n---\u003e 89 print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Discriminator Accuracy: {discriminator_accuracy}, Generator Loss: {generator_loss}\")\n     90 vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n     91 vgan_losses['generator_loss'].append(generator_loss)\n\nIndexError: invalid index to scalar variable.","recorded":"2024-10-23 12:55:56.909570122","filePath":"null","pinned":false},{"value":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, optimizers\n\n# Build the Generator\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\n# Hyperparameters\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': [], 'discriminator_accuracy': [], 'generator_accuracy': []}\n\n# Save logs to a CSV file\nimport csv\n\nlog_file_path = 'gan_training_logs.csv'\nwith open(log_file_path, mode='w', newline='') as log_file:\n    log_writer = csv.writer(log_file)\n    log_writer.writerow(['Epoch', 'Discriminator Loss', 'Discriminator Accuracy', 'Generator Loss', 'Generator Accuracy'])\n\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n\n        # Discriminator training\n        discriminator_loss_real, discriminator_accuracy_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake, discriminator_accuracy_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        \n        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n        discriminator_accuracy = 0.5 * np.add(discriminator_accuracy_real, discriminator_accuracy_fake)\n\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n\n        # Log the results\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Discriminator Accuracy: {discriminator_accuracy}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n        vgan_losses['generator_loss'].append(generator_loss)\n        vgan_losses['discriminator_accuracy'].append(discriminator_accuracy)\n\n        # Log to CSV\n        with open(log_file_path, mode='a', newline='') as log_file:\n            log_writer = csv.writer(log_file)\n            log_writer.writerow([epoch, discriminator_loss[0], discriminator_accuracy, generator_loss, 'N/A'])  # Generator accuracy can be complex to compute.\n\n        if epoch % 1000 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n    \n    generator.save_weights(save_path)\n\n# Image saving function\ndef save_images(images, path='workspace/dataset/chest_xray/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\n# Training parameters\nepochs = 10000\nbatch_size = 64\n\n# Start training the GAN\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size, 'workspace/dataset/chest_xray/lungs_generator_weights2.h5')\n","recorded":"2024-10-23 12:55:47.991131234","filePath":"null","pinned":false},{"value":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport csv\n\n# Define the generator\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    return model\n\n# Define the gradient penalty\ndef gradient_penalty_loss(real_images, fake_images, discriminator):\n    batch_size = tf.shape(real_images)[0]\n\n    # Randomly interpolate between real and fake samples\n    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n    interpolated = alpha * real_images + (1 - alpha) * fake_images\n\n    with tf.GradientTape() as tape:\n        tape.watch(interpolated)\n        pred_interpolated = discriminator(interpolated)\n\n    gradients = tape.gradient(pred_interpolated, interpolated)\n    gradients_l2_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n    gradient_penalty = tf.reduce_mean(tf.square(gradients_l2_norm - 1.0))\n\n    return gradient_penalty\n\n# Define the WGAN-GP model\ndef build_wgan_gp(generator, discriminator, latent_dim):\n    z = layers.Input(shape=(latent_dim,))\n    img = generator(z)\n    valid = discriminator(img)\n    return models.Model(z, valid)\n\n# Define parameters\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss='mse')\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the WGAN-GP\nwgan_gp = build_wgan_gp(generator, discriminator, latent_dim)\n\n# Compile the WGAN-GP\nwgan_gp.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss=lambda y_true, y_pred: -y_pred)\n\n# Initialize logs\nwgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n\n# Prepare log file\nlog_file_path = 'wgan_training_logs.csv'\nwith open(log_file_path, mode='w', newline='') as log_file:\n    log_writer = csv.writer(log_file)\n    log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n\n# Train the WGAN-GP\ndef train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path):\n    for epoch in range(epochs):\n        for _ in range(batch_size):\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n\n            # Train the discriminator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n            interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n            interpolated_samples = tf.Variable(interpolated_samples, dtype=tf.float32)\n\n            with tf.GradientTape() as tape:\n                tape.watch(interpolated_samples)\n                pred_real = discriminator(real_images)\n                pred_fake = discriminator(fake_images)\n                gradient_penalty = gradient_penalty_loss(real_images, fake_images, discriminator)\n                d_loss = tf.reduce_mean(pred_fake) - tf.reduce_mean(pred_real) + 10 * gradient_penalty\n            \n            grads = tape.gradient(d_loss, discriminator.trainable_weights)\n            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n\n        # Train the generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        g_loss = wgan_gp.train_on_batch(noise, -np.ones((batch_size, 1)))\n\n        # Log the results\n        print(f\"Epoch {epoch}, Discriminator Loss: {d_loss.numpy()}, Generator Loss: {g_loss}\")\n        wgan_losses['discriminator_loss'].append(d_loss.numpy())\n        wgan_losses['generator_loss'].append(g_loss)\n\n        # Log to CSV\n        with open(log_file_path, mode='a', newline='') as log_file:\n            log_writer = csv.writer(log_file)\n            log_writer.writerow([epoch, d_loss.numpy(), g_loss])\n\n        if epoch % 20 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"workspace/dataset/chest_xray/generated_imagesWGAN1/epoch_{epoch}\")\n\n    generator.save_weights(save_path)\n\n# Image saving function\ndef save_images(images, path='workspace/dataset/chest_xray/generated_imagesWGAN1'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}/generated_image_{i}.png\")\n        plt.close()\n\n# Train the WGAN-GP\nepochs = 1000\nbatch_size = 64\ntrain_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, 'workspace/dataset/chest_xray/lungs_generator_weights_wgan_gp1.h5')\n","recorded":"2024-10-23 12:41:45.350052454","filePath":"null","pinned":false},{"value":"#WGAN model\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# Define the generator\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    return model\n\n# Define the gradient penalty\ndef gradient_penalty_loss(real_images, fake_images, discriminator):\n    batch_size = tf.shape(real_images)[0]\n\n    # Randomly interpolate between real and fake samples\n    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n    interpolated = alpha * real_images + (1 - alpha) * fake_images\n\n    with tf.GradientTape() as tape:\n        tape.watch(interpolated)\n        pred_interpolated = discriminator(interpolated)\n\n    gradients = tape.gradient(pred_interpolated, interpolated)\n    gradients_l2_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n    gradient_penalty = tf.reduce_mean(tf.square(gradients_l2_norm - 1.0))\n\n    return gradient_penalty\n\n# Define the WGAN-GP model\ndef build_wgan_gp(generator, discriminator, latent_dim):\n    z = layers.Input(shape=(latent_dim,))\n    img = generator(z)\n    valid = discriminator(img)\n    return models.Model(z, valid)\n\n# Define parameters\nlatent_dim = 100\n\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss='mse')\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the WGAN-GP\nwgan_gp = build_wgan_gp(generator, discriminator, latent_dim)\n\n# Compile the WGAN-GP\nwgan_gp.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss=lambda y_true, y_pred: -y_pred)\nwgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the WGAN-GP\ndef train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path):\n    for epoch in range(epochs):\n        for _ in range(batch_size):\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n\n            # Train the discriminator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n            interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n            interpolated_samples = tf.Variable(interpolated_samples, dtype=tf.float32)\n            with tf.GradientTape() as tape:\n                tape.watch(interpolated_samples)\n                pred_real = discriminator(real_images)\n                pred_fake = discriminator(fake_images)\n                gradient_penalty = gradient_penalty_loss(real_images, fake_images,discriminator)\n                d_loss = tf.reduce_mean(pred_fake) - tf.reduce_mean(pred_real) + 10 * gradient_penalty\n            grads = tape.gradient(d_loss, discriminator.trainable_weights)\n            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n\n        # Train the generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        g_loss = wgan_gp.train_on_batch(noise, -np.ones((batch_size, 1)))\n\n        print(f\"Epoch {epoch}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n        wgan_losses['discriminator_loss'].append(d_loss)\n        wgan_losses['generator_loss'].append(g_loss)\n        if epoch % 20 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"workspace/dataset/chest_xray/generated_imagesWGAN1/epoch_{epoch}\")\n\n    generator.save_weights(save_path)\n\n# Train the WGAN-GP\nepochs = 1000\nbatch_size =64\n\ndef save_images(images, path='workspace/dataset/chest_xray/generated_imagesWGAN1'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\ntrain_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, 'workspace/dataset/chest_xray/lungs_generator_weights_wgan_gp1.h5')\n","recorded":"2024-10-23 12:41:13.559475795","filePath":"null","pinned":false},{"value":"# Vanilla GAN model\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size,save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n        discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n        vgan_losses['generator_loss'].append(generator_loss)\n        if epoch % 1000 == 0:\n          save_images(generator.predict(np.random.normal(0, 1, (25,latent_dim))), path=f\"workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n    generator.save_weights(save_path)\n\nepochs=10000\nbatch_size=64\ndef save_images(images, path='workspace/dataset/chest_xray/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size,'workspace/dataset/chest_xray/lungs_generator_weights2.h5')\n","recorded":"2024-10-23 12:38:30.013030425","filePath":"null","pinned":false},{"value":"# Function to load and preprocess images with labels\ndef load_images_with_labels(folder, img_width, img_height, label):\n    images = []\n    labels = []\n    for filename in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder, filename))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = cv2.resize(img, (img_width, img_height))\n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        images.append(img)\n        labels.append(label)\n    return np.array(images), np.array(labels)\n\n# Load and preprocess images with labels\nnormal_images, normal_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height, label=\"Normal\")\npneumonia_images, pneumonia_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height, label=\"Pneumonia\")\n\n\nreal_data = np.concatenate((normal_images, pneumonia_images), axis=0)\nreal_labels = np.concatenate((normal_labels, pneumonia_labels), axis=0)\n\n\nshuffled_indices = np.random.permutation(len(real_data))\nreal_data = real_data[shuffled_indices]\nreal_labels = real_labels[shuffled_indices]\n\n\ndef display_images_with_labels(images, labels, num_samples=5):\n    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n    for i in range(num_samples):\n        axes[i].imshow(images[i][:, :, 0], cmap='gray')\n        axes[i].set_title(labels[i])\n        axes[i].axis('off')\n    plt.show()\n\n\nnum_samples_to_display = 5\ndisplay_images_with_labels(real_data, real_labels, num_samples_to_display)\n","recorded":"2024-10-23 12:33:19.936614744","filePath":"null","pinned":false},{"value":"/workspace/dataset/chest_xray/chest_xray/train/NORMAL/.ipynb_checkpoints","recorded":"2024-10-23 12:31:40.528911593","filePath":"null","pinned":false},{"value":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to load and preprocess images with labels\ndef load_images_with_labels(folder, img_width, img_height, label):\n    images = []\n    labels = []\n    for filename in os.listdir(folder):\n        img_path = os.path.join(folder, filename)\n        img = cv2.imread(img_path)\n        \n        # Check if the image was loaded correctly\n        if img is None:\n            print(f\"Warning: Failed to load image: {img_path}\")\n            continue\n        \n        # Convert to grayscale\n        try:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        except cv2.error as e:\n            print(f\"Error converting image {filename} to grayscale: {e}\")\n            continue\n        \n        # Resize the image\n        img = cv2.resize(img, (img_width, img_height))\n        \n        # Normalize and expand dimensions for compatibility with models\n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        \n        images.append(img)\n        labels.append(label)\n    \n    return np.array(images), np.array(labels)\n\n# Load and preprocess images with labels\nnormal_images, normal_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height, label=\"Normal\")\npneumonia_images, pneumonia_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height, label=\"Pneumonia\")\n\n# Concatenate data and labels\nreal_data = np.concatenate((normal_images, pneumonia_images), axis=0)\nreal_labels = np.concatenate((normal_labels, pneumonia_labels), axis=0)\n\n# Shuffle data\nshuffled_indices = np.random.permutation(len(real_data))\nreal_data = real_data[shuffled_indices]\nreal_labels = real_labels[shuffled_indices]\n\n# Function to display images with labels\ndef display_images_with_labels(images, labels, num_samples=5):\n    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n    for i in range(num_samples):\n        axes[i].imshow(images[i][:, :, 0], cmap='gray')\n        axes[i].set_title(labels[i])\n        axes[i].axis('off')\n    plt.show()\n\n# Display a sample of images\nnum_samples_to_display = 5\ndisplay_images_with_labels(real_data, real_labels, num_samples_to_display)\n","recorded":"2024-10-23 12:31:25.987231233","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\nerror                                     Traceback (most recent call last)\nCell In[9], line 16\n     13     return np.array(images), np.array(labels)\n     15 # Load and preprocess images with labels\n---\u003e 16 normal_images, normal_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height, label=\"Normal\")\n     17 pneumonia_images, pneumonia_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height, label=\"Pneumonia\")\n     20 real_data = np.concatenate((normal_images, pneumonia_images), axis=0)\n\nCell In[9], line 7, in load_images_with_labels(folder, img_width, img_height, label)\n      5 for filename in os.listdir(folder):\n      6     img = cv2.imread(os.path.join(folder, filename))\n----\u003e 7     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n      8     img = cv2.resize(img, (img_width, img_height))\n      9     img = img.astype('float32') / 255.0\n\nerror: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'","recorded":"2024-10-23 12:31:12.702278326","filePath":"null","pinned":false},{"value":"/workspace/dataset/chest_xray/chest_xray/train/PNEUMONIA/.DS_Store","recorded":"2024-10-23 12:30:24.781001714","filePath":"null","pinned":false},{"value":"/workspace/dataset/chest_xray/chest_xray/train/NORMAL/.DS_Store","recorded":"2024-10-23 12:30:13.285060041","filePath":"null","pinned":false},{"value":"def load_images(folder, img_width, img_height):\n    images = []\n    for filename in os.listdir(folder):\n        img_path = os.path.join(folder, filename)\n        img = cv2.imread(img_path)\n        \n        # Check if the image was loaded correctly\n        if img is None:\n            print(f\"Warning: Failed to load image: {img_path}\")\n            continue\n        \n        # Convert to grayscale\n        try:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        except cv2.error as e:\n            print(f\"Error converting image {filename} to grayscale: {e}\")\n            continue\n        \n        # Resize the image\n        img = cv2.resize(img, (img_width, img_height))\n        \n        # Normalize and expand dimensions for compatibility with models\n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        \n        images.append(img)\n    \n    return np.array(images)\n\n","recorded":"2024-10-23 12:29:00.844706758","filePath":"null","pinned":false},{"value":"def load_images(folder, img_width, img_height):\n    images = []\n    for filename in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder, filename))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = cv2.resize(img, (img_width, img_height)) \n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        images.append(img)\n    return np.array(images)","recorded":"2024-10-23 12:28:34.471436032","filePath":"null","pinned":false},{"value":"for filename in os.listdir(folder):\n    img_path = os.path.join(folder, filename)\n    img = cv2.imread(img_path)\n    if img is None:\n        print(f\"Failed to load image: {img_path}\")\n        continue\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = cv2.resize(img, (img_width, img_height)) \n    img = img.astype('float32') / 255.0","recorded":"2024-10-23 12:28:24.829231202","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\nerror                                     Traceback (most recent call last)\nCell In[4], line 7\n      5 get_ipython().system('ls \"/workspace/dataset/chest_xray\"')\n      6 # Load and preprocess images from the dataset\n----\u003e 7 normal_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height)\n      8 pneumonia_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height)\n\nCell In[2], line 5, in load_images(folder, img_width, img_height)\n      3 for filename in os.listdir(folder):\n      4     img = cv2.imread(os.path.join(folder, filename))\n----\u003e 5     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n      6     img = cv2.resize(img, (img_width, img_height)) \n      7     img = img.astype('float32') / 255.0\n\nerror: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'","recorded":"2024-10-23 12:27:30.245014925","filePath":"null","pinned":false},{"value":"workspace/dataset/chest_xray/chest_xray/test","recorded":"2024-10-23 12:24:43.323418224","filePath":"null","pinned":false},{"value":"workspace/dataset/chest_xray","recorded":"2024-10-23 12:18:33.385037471","filePath":"null","pinned":false},{"value":"import os\n\n# List files in the directory to check if path is correct\nprint(os.listdir(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\")))","recorded":"2024-10-23 12:17:01.129420987","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\nerror                                     Traceback (most recent call last)\nCell In[17], line 2\n      1 # Load the dataset\n----\u003e 2 normal_images, normal_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height, label=0)\n      3 pneumonia_images, pneumonia_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height, label=1)\n      5 real_data = np.concatenate((normal_images, pneumonia_images), axis=0)\n\nCell In[15], line 18, in load_images_with_labels(folder, img_width, img_height, label)\n     16 for filename in os.listdir(folder):\n     17     img = cv2.imread(os.path.join(folder, filename))\n---\u003e 18     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n     19     img = cv2.resize(img, (img_width, img_height))\n     20     img = img.astype('float32') / 255.0\n\nerror: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'","recorded":"2024-10-23 12:16:40.518388042","filePath":"null","pinned":false},{"value":"chest_xray","recorded":"2024-10-23 12:15:43.877473448","filePath":"null","pinned":false},{"value":"workspace/kaggle.json","recorded":"2024-10-23 12:09:53.294822331","filePath":"null","pinned":false},{"value":"/workspace","recorded":"2024-10-23 12:09:17.107460049","filePath":"null","pinned":false},{"value":"/home/karna/dotfiles/kaggle.json.cpt","recorded":"2024-10-23 12:08:01.502693641","filePath":"null","pinned":false},{"value":"!pip install kaggle","recorded":"2024-10-23 12:06:24.354750017","filePath":"null","pinned":false},{"value":"dataset","recorded":"2024-10-23 12:05:27.011671097","filePath":"null","pinned":false},{"value":"dataset_dir = \"/content/drive/MyDrive/dataset/chest_xray\"","recorded":"2024-10-23 12:04:49.681158398","filePath":"null","pinned":false},{"value":"!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia -p /content/drive/MyDrive/dataset --unzip\n","recorded":"2024-10-23 12:04:07.764340705","filePath":"null","pinned":false},{"value":"!mkdir -p ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n","recorded":"2024-10-23 12:03:44.255108814","filePath":"null","pinned":false},{"value":"!pip install kaggle\n","recorded":"2024-10-23 12:03:31.625319405","filePath":"null","pinned":false},{"value":"# Plot and save WGAN loss\nplot_loss(wgan_losses)","recorded":"2024-10-23 12:00:03.239619535","filePath":"null","pinned":false},{"value":"train_wgan_gp(generator_wgan, discriminator_wgan, wgan_gp, real_data, epochs, batch_size, save_path_wgan)","recorded":"2024-10-23 11:59:58.732493675","filePath":"null","pinned":false},{"value":"# Train WGAN-GP\ngenerator_wgan = build_generator(latent_dim)\ndiscriminator_wgan = build_discriminator((img_width, img_height, channels))\n\ndiscriminator_wgan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.0002, 0.5), metrics=['accuracy'])\nwgan_gp = build_wgan_gp(generator_wgan, discriminator_wgan, latent_dim)\n\nwgan_gp.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.0001, 0.5))","recorded":"2024-10-23 11:59:53.988241875","filePath":"null","pinned":false},{"value":"# WGAN Training Function\nwgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n\ndef train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path):\n    for epoch in range(epochs):\n        for _ in range(batch_size):\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n\n            epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n            interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n\n            d_loss_real = discriminator.train_on_batch(real_images, np.ones((batch_size, 1)))\n            d_loss_fake = discriminator.train_on_batch(fake_images, np.zeros((batch_size, 1)))\n\n            # Generator Training\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            g_loss = wgan_gp.train_on_batch(noise, np.ones((batch_size, 1)))\n\n            wgan_losses['discriminator_loss'].append(d_loss_real + d_loss_fake)\n            wgan_losses['generator_loss'].append(g_loss)\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}/{epochs}, D Loss: {d_loss_real}, G Loss: {g_loss}\")\n\n        if epoch % 1000 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), f\"/content/drive/My Drive/Lung dataset/generated_imagesWGAN/epoch_{epoch}\")\n\n    generator.save_weights(save_path)\n","recorded":"2024-10-23 11:59:45.288796012","filePath":"null","pinned":false},{"value":"# WGAN-GP Model Definition\ndef build_wgan_gp(generator, discriminator, latent_dim):\n    z = layers.Input(shape=(latent_dim,))\n    img = generator(z)\n    valid = discriminator(img)\n    return models.Model(z, valid)","recorded":"2024-10-23 11:59:39.779711983","filePath":"null","pinned":false},{"value":"# Save and plot loss\ndef plot_loss(vgan_losses, save_dir='/content/drive/My Drive/Lung dataset/loss_plots'):\n    os.makedirs(save_dir, exist_ok=True)\n    plt.figure(figsize=(10, 5))\n    plt.plot(vgan_losses['discriminator_loss'], label='Discriminator Loss', color='blue')\n    plt.plot(vgan_losses['generator_loss'], label='Generator Loss', color='orange')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Vanilla GAN Training Losses')\n    plt.legend()\n    plt.savefig(os.path.join(save_dir, 'gan_loss.png'))\n    plt.show()\n\nplot_loss(vgan_losses)","recorded":"2024-10-23 11:59:33.580519643","filePath":"null","pinned":false},{"value":"train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path_gan)","recorded":"2024-10-23 11:59:27.850346119","filePath":"null","pinned":false},{"value":"# Train Vanilla GAN\ngenerator = build_generator(latent_dim)\ndiscriminator = build_discriminator((img_width, img_height, channels))\n\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.0002, 0.5), metrics=['accuracy'])\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.0001, 0.5))\n\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path_gan)","recorded":"2024-10-23 11:59:22.650139943","filePath":"null","pinned":false},{"value":"# GAN Training Function\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n\n        # Discriminator Training\n        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Generator Training\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        g_loss = gan.train_on_batch(noise, real_labels)\n\n        vgan_losses['discriminator_loss'].append(d_loss[0])\n        vgan_losses['generator_loss'].append(g_loss)\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}/{epochs}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n\n        if epoch % 1000 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), f\"/content/drive/My Drive/Lung dataset/generated_imagesImproved/epoch_{epoch}\")\n\n    generator.save_weights(save_path)","recorded":"2024-10-23 11:59:12.614530150","filePath":"null","pinned":false},{"value":"def save_metrics(generator, real_labels, predictions, metrics_dir):\n    os.makedirs(metrics_dir, exist_ok=True)\n    \n    accuracy = accuracy_score(real_labels, predictions)\n    precision = precision_score(real_labels, predictions, average='binary')\n    recall = recall_score(real_labels, predictions, average='binary')\n    f1 = f1_score(real_labels, predictions, average='binary')\n    \n    with open(os.path.join(metrics_dir, 'metrics.txt'), 'w') as f:\n        f.write(f\"Accuracy: {accuracy}\\n\")\n        f.write(f\"Precision: {precision}\\n\")\n        f.write(f\"Recall: {recall}\\n\")\n        f.write(f\"F1-Score: {f1}\\n\")\n\n# Vanilla GAN Model\ndef build_generator(latent_dim):\n    model = models.Sequential([\n        layers.Dense(128 * 16 * 16, input_dim=latent_dim),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Reshape((16, 16, 128)),\n        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same')\n    ])\n    return model\n\ndef build_discriminator(input_shape):\n    model = models.Sequential([\n        layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Dropout(0.4),\n        layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Dropout(0.4),\n        layers.Flatten(),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    return model","recorded":"2024-10-23 11:58:59.771958073","filePath":"null","pinned":false},{"value":"# Display sample images\nnum_samples_to_display = 5\ndisplay_images_with_labels(real_data, real_labels, num_samples_to_display)","recorded":"2024-10-23 11:58:54.043670951","filePath":"null","pinned":false},{"value":"# Load the dataset\nnormal_images, normal_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height, label=0)\npneumonia_images, pneumonia_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height, label=1)\n\nreal_data = np.concatenate((normal_images, pneumonia_images), axis=0)\nreal_labels = np.concatenate((normal_labels, pneumonia_labels), axis=0)\n\n# Shuffle dataset\nshuffled_indices = np.random.permutation(len(real_data))\nreal_data, real_labels = real_data[shuffled_indices], real_labels[shuffled_indices]\n\n# Display images and labels\ndef display_images_with_labels(images, labels, num_samples=5):\n    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n    for i in range(num_samples):\n        axes[i].imshow(images[i][:, :, 0], cmap='gray')\n        axes[i].set_title(f\"Label: {labels[i]}\")\n        axes[i].axis('off')\n    plt.show()\n","recorded":"2024-10-23 11:58:47.203338314","filePath":"null","pinned":false},{"value":"# Parameters\nimg_width, img_height, channels = 64, 64, 1\ndataset_dir = \"/content/drive/My Drive/Lung dataset/archive\"\nlatent_dim = 100\nepochs = 10000\nbatch_size = 64\nsave_path_gan = \"/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights.h5\"\nsave_path_wgan = \"/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights_wgan_gp.h5\"\n","recorded":"2024-10-23 11:58:41.217742266","filePath":"null","pinned":false},{"value":"# Load Images\ndef load_images(folder, img_width, img_height):\n    images = []\n    for filename in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder, filename))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = cv2.resize(img, (img_width, img_height)) \n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        images.append(img)\n    return np.array(images)\n\n# Load and preprocess images with labels\ndef load_images_with_labels(folder, img_width, img_height, label):\n    images, labels = [], []\n    for filename in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder, filename))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = cv2.resize(img, (img_width, img_height))\n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        images.append(img)\n        labels.append(label)\n    return np.array(images), np.array(labels)","recorded":"2024-10-23 11:58:25.115630389","filePath":"null","pinned":false},{"value":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, models, optimizers\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","recorded":"2024-10-23 11:57:47.445430535","filePath":"null","pinned":false},{"value":"latent_dim=100\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\ndef display_generated_images(generator, z_dim, num_images, title):\n    random_latent_vectors = np.random.normal(0, 1, (num_images, z_dim))\n    generated_images = generator.predict(random_latent_vectors)\n\n    # Rescale images to the [0,1] range\n    generated_images = 0.5 * generated_images + 0.5\n\n    plt.figure(figsize=(10, 10))\n    for i in range(num_images):\n        plt.subplot(10, 10, i+1)\n        plt.imshow(generated_images[i, :, :, :])\n        plt.axis('off')\n\n    plt.suptitle(title)\n    plt.show()\n\n\ngenerator_lungs = build_generator(latent_dim)\ngenerator_lungs.load_weights('/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights_wgan_gp.h5')\nprint(generator_lungs.summary())\n# Generate and display cat images\ndisplay_generated_images(generator_lungs, latent_dim, 100, 'Generated Lung Images')","recorded":"2024-10-23 11:53:05.730696317","filePath":"null","pinned":false},{"value":"# for epoch in range(epochs):\n\nplt.figure(figsize=(10, 5))\n\n    # Plot Vanilla GAN losses\nplt.plot(vgan_losses['discriminator_loss'], label='Vanilla GAN - Discriminator Loss', color='blue')\nplt.plot(vgan_losses['generator_loss'], label='Vanilla GAN - Generator Loss', color='orange')\n\n\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('GAN Training Losses')\nplt.legend()\nplt.show()","recorded":"2024-10-23 11:52:57.707153821","filePath":"null","pinned":false},{"value":"#GAN model\ndef build_generatorSimple(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\ndef display_generated_images(generator, z_dim, num_images, title):\n    random_latent_vectors = np.random.normal(0, 1, (num_images, z_dim))\n    generated_images = generator.predict(random_latent_vectors)\n\n    # Rescale images to the [0,1] range\n    generated_images = 0.5 * generated_images + 0.5\n\n    plt.figure(figsize=(10, 10))\n    for i in range(num_images):\n        plt.subplot(10, 10, i+1)\n        plt.imshow(generated_images[i, :, :, :])\n        plt.axis('off')\n\n    plt.suptitle(title)\n    plt.show()\n\n\ngenerator_lungs = build_generatorSimple(latent_dim)\ngenerator_lungs.load_weights('/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights.h5')\nprint(generator_lungs.summary())\n# Generate and display cat images\ndisplay_generated_images(generator_lungs, latent_dim, 100, 'Generated Lung Images')","recorded":"2024-10-23 11:52:52.648158548","filePath":"null","pinned":false},{"value":"saved_model_path = \"/content/drive/MyDrive/Lung dataset/archive/lungs_generator_weights2.h5\"\nmodel = tf.keras.models.load_model(saved_model_path)\ntraining_history = model.history.history\n\n# Get generator and discriminator losses\ngenerator_loss = training_history['generator_loss']\ndiscriminator_loss = training_history['discriminator_loss']","recorded":"2024-10-23 11:52:48.442264990","filePath":"null","pinned":false},{"value":"# Vanilla GAN model\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size,save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n        discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n        vgan_losses['generator_loss'].append(generator_loss)\n        if epoch % 1000 == 0:\n          save_images(generator.predict(np.random.normal(0, 1, (25,latent_dim))), path=f\"/content/drive/My Drive/Lung dataset/generated_imagesImproved/epoch_{epoch}\")\n    generator.save_weights(save_path)\n\nepochs=10000\nbatch_size=64\ndef save_images(images, path='/content/drive/My Drive/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size,'/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights2.h5')\n","recorded":"2024-10-23 11:52:42.244270266","filePath":"null","pinned":false},{"value":"img_width, img_height = 64, 64\nchannels = 1  # Grayscale\n\ndataset_dir = \"/content/drive/My Drive/Lung dataset/archive\"\n!ls \"/connt/drive/My Drive/Lung dataset/archizve\"\n# Load and preprocess images from the dataset\nnormal_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height)\npneumonia_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height)","recorded":"2024-10-23 11:52:33.807025394","filePath":"null","pinned":false},{"value":"from google.colab import drive\ndrive.mount('/content/drive')","recorded":"2024-10-23 11:52:30.772763401","filePath":"null","pinned":false},{"value":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, models, optimizers","recorded":"2024-10-23 11:52:18.279024124","filePath":"null","pinned":false},{"value":"https://www.ieltsadvantage.com/2015/03/10/ielts-writing-task-1-grammar-guide/","recorded":"2024-10-23 11:29:34.613496751","filePath":"null","pinned":false},{"value":"/mnt/Karna/aco-main\n/mnt/Karna/CP\n/mnt/Karna/Git\n/mnt/Karna/Motor Imagery Classification Performance Enhancement with EEG Data Augmentation\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main\n/mnt/Karna/Pratik Project GAN Generate lung images\n/mnt/Karna/Reinforcement-learning-approach-for-prognosis-in-ICU\n/mnt/Karna/aco-main.zip\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main.zip","recorded":"2024-10-23 10:36:38.011631544","filePath":"null","pinned":false},{"value":"/10.1016/j.jcp.2018.10.045","recorded":"2024-10-21 16:33:00.580348637","filePath":"null","pinned":false},{"value":"Scopus author profile","recorded":"2024-10-21 16:24:32.801406379","filePath":"null","pinned":false},{"value":"import torch\nimport torch.nn as nn\n\n# Define the generator\nclass Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super(Generator, self).__init__()\n        # Fully connected layer to project latent vector into a 2D shape\n        self.fc = nn.Linear(latent_dim, 128 * 8 * 8)  # Modify this based on your target shape\n        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n        \n        # Transpose convolutions to upscale the image\n        self.conv_trans1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # Output: (batch_size, 64, 16, 16)\n        self.conv_trans2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)   # Output: (batch_size, 32, 32, 32)\n        self.conv_trans3 = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)   # Output: (batch_size, 16, 64, 64)\n        \n        # Final output layer (e.g., grayscale image)\n        self.conv_out = nn.Conv2d(16, 1, kernel_size=7, padding=3)  # Output: (batch_size, 1, 64, 64)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, z):\n        x = self.fc(z)  # Project the latent vector to a 2D shape\n        x = self.lrelu(x)\n        x = x.view(-1, 128, 8, 8)  # Reshape to (batch_size, 128, 8, 8) for ConvTranspose layers\n        \n        x = self.lrelu(self.conv_trans1(x))  # (batch_size, 64, 16, 16)\n        x = self.lrelu(self.conv_trans2(x))  # (batch_size, 32, 32, 32)\n        x = self.lrelu(self.conv_trans3(x))  # (batch_size, 16, 64, 64)\n        x = self.sigmoid(self.conv_out(x))   # (batch_size, 1, 64, 64)\n        \n        return x","recorded":"2024-10-21 15:36:01.577885091","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\n\nRuntimeError                              Traceback (most recent call last)\n\n\u003cipython-input-18-565860103019\u003e in \u003ccell line: 1\u003e()\n----\u003e 1 train_gan(generator, discriminator, dataloader, epochs, latent_dim, '/content/drive/My Drive/Lung dataset/generated_images')\n\n6 frames\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    115 \n    116     def forward(self, input: Tensor) -\u003e Tensor:\n--\u003e 117         return F.linear(input, self.weight, self.bias)\n    118 \n    119     def extra_repr(self) -\u003e str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (6400x1 and 100x32768)","recorded":"2024-10-21 15:35:21.146513092","filePath":"null","pinned":false},{"value":"import torch\nimport torch.nn as nn\n\n# Define the generator\nclass Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super(Generator, self).__init__()\n        self.fc = nn.Linear(latent_dim, 128 * 16 * 16)\n        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n        self.conv_trans1 = nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n        self.conv_trans2 = nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n        self.conv_out = nn.Conv2d(128, 1, kernel_size=7, padding=3)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, z):\n        x = self.fc(z)\n        x = self.lrelu(x)\n        x = x.view(-1, 128, 16, 16)  # Reshape to (batch_size, 128, 16, 16)\n        x = self.lrelu(self.conv_trans1(x))\n        x = self.lrelu(self.conv_trans2(x))\n        x = self.sigmoid(self.conv_out(x))\n        return x\n\nlatent_dim = 100\ngenerator = Generator(latent_dim)","recorded":"2024-10-21 15:34:05.212110566","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\n\nRuntimeError                              Traceback (most recent call last)\n\n\u003cipython-input-13-565860103019\u003e in \u003ccell line: 1\u003e()\n----\u003e 1 train_gan(generator, discriminator, dataloader, epochs, latent_dim, '/content/drive/My Drive/Lung dataset/generated_images')\n\n9 frames\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    115 \n    116     def forward(self, input: Tensor) -\u003e Tensor:\n--\u003e 117         return F.linear(input, self.weight, self.bias)\n    118 \n    119     def extra_repr(self) -\u003e str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (6400x1 and 100x32768)","recorded":"2024-10-21 15:33:29.572857291","filePath":"null","pinned":false},{"value":"# Save model weights\ntorch.save(generator.state_dict(), '/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights.pth')\n","recorded":"2024-10-21 15:32:44.118202991","filePath":"null","pinned":false},{"value":"\ntrain_gan(generator, discriminator, dataloader, epochs, latent_dim, '/content/drive/My Drive/Lung dataset/generated_images')\n\n# Save model weights\ntorch.save(generator.state_dict(), '/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights.pth')\n","recorded":"2024-10-21 15:26:36.359391248","filePath":"null","pinned":false},{"value":"epochs = 10000\nbatch_size = 64\n\ndef train_gan(generator, discriminator, dataloader, epochs, latent_dim, save_path):\n    for epoch in range(epochs):\n        for i, real_images in enumerate(dataloader):\n            real_images = real_images.to(device)\n\n            # Train Discriminator\n            optimizer_D.zero_grad()\n            batch_size = real_images.size(0)\n            real_labels = torch.ones(batch_size, 1).to(device)\n            fake_labels = torch.zeros(batch_size, 1).to(device)\n\n            outputs = discriminator(real_images)\n            d_loss_real = adversarial_loss(outputs, real_labels)\n\n            z = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n            fake_images = generator(z)\n            outputs = discriminator(fake_images.detach())\n            d_loss_fake = adversarial_loss(outputs, fake_labels)\n\n            d_loss = d_loss_real + d_loss_fake\n            d_loss.backward()\n            optimizer_D.step()\n\n            # Train Generator\n            optimizer_G.zero_grad()\n            outputs = discriminator(fake_images)\n            g_loss = adversarial_loss(outputs, real_labels)\n            g_loss.backward()\n            optimizer_G.step()\n\n        print(f\"Epoch [{epoch}/{epochs}] - D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n        if epoch % 100 == 0:\n            save_images(fake_images, path=f\"{save_path}/epoch_{epoch}\", num_images=25)\n","recorded":"2024-10-21 15:26:31.106015733","filePath":"null","pinned":false},{"value":"# Function to save generated images\ndef save_images(images, path, num_images=25):\n    grid = vutils.make_grid(images[:num_images], nrow=5, normalize=True)\n    plt.figure(figsize=(10, 10))\n    plt.imshow(np.transpose(grid.cpu().numpy(), (1, 2, 0)))\n    plt.axis('off')\n    os.makedirs(path, exist_ok=True)\n    plt.savefig(f\"{path}/generated_images.png\")\n    plt.close()\n","recorded":"2024-10-21 15:26:23.680078723","filePath":"null","pinned":false},{"value":"\n# Initialize Generator and Discriminator\nlatent_dim = 100\ngenerator = Generator(latent_dim).to(device)\ndiscriminator = Discriminator().to(device)\n\n# Optimizers\nlr = 0.0002\noptimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# Loss function\nadversarial_loss = nn.BCELoss()","recorded":"2024-10-21 15:26:19.255118352","filePath":"null","pinned":false},{"value":"# Define the Discriminator model\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.4),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.4),\n            nn.Flatten(),\n            nn.Linear(128 * 16 * 16, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, img):\n        return self.model(img)","recorded":"2024-10-21 15:26:11.197326329","filePath":"null","pinned":false},{"value":"# Define the Generator model\nclass Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 128 * 16 * 16),\n            nn.LeakyReLU(0.2),\n            nn.Unflatten(1, (128, 16, 16)),\n            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 1, kernel_size=7, padding=3),\n            nn.Tanh()  # Output is in range [-1, 1]\n        )\n\n    def forward(self, z):\n        return self.model(z)","recorded":"2024-10-21 15:26:05.777196512","filePath":"null","pinned":false},{"value":"# Display sample images\nsample_images = next(iter(dataloader))\ndisplay_images_with_labels(sample_images, num_samples=5)","recorded":"2024-10-21 15:25:56.620628129","filePath":"null","pinned":false},{"value":"\n# Function to display images\ndef display_images_with_labels(images, num_samples=5):\n    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n    for i in range(num_samples):\n        axes[i].imshow(images[i][0], cmap='gray')\n        axes[i].axis('off')\n    plt.show()","recorded":"2024-10-21 15:25:51.227129099","filePath":"null","pinned":false},{"value":"# Combine the datasets\ncombined_dataset = normal_dataset + pneumonia_dataset\ndataloader = DataLoader(combined_dataset, batch_size=64, shuffle=True)","recorded":"2024-10-21 15:25:42.139262685","filePath":"null","pinned":false},{"value":"# Load the datasets\ndataset_dir = \"/content/drive/My Drive/Lung dataset/archive\"\nnormal_dataset = LungDataset(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), 64, 64, transform)\npneumonia_dataset = LungDataset(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), 64, 64, transform)\n","recorded":"2024-10-21 15:25:36.818057651","filePath":"null","pinned":false},{"value":"# Define image transformation (resizing and normalization)\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n])","recorded":"2024-10-21 15:25:28.273902682","filePath":"null","pinned":false},{"value":"# Dataset class to load and preprocess images\nclass LungDataset(Dataset):\n    def __init__(self, folder, img_width, img_height, transform=None):\n        self.folder = folder\n        self.img_width = img_width\n        self.img_height = img_height\n        self.transform = transform\n        self.image_paths = [os.path.join(folder, fname) for fname in os.listdir(folder)]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('L')\n        if self.transform:\n            image = self.transform(image)\n        return image","recorded":"2024-10-21 15:25:22.110850466","filePath":"null","pinned":false},{"value":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","recorded":"2024-10-21 15:25:15.206548255","filePath":"null","pinned":false},{"value":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport matplotlib.pyplot as plt\nfrom PIL import Image","recorded":"2024-10-21 15:25:05.905438143","filePath":"null","pinned":false},{"value":"# Vanilla GAN model\ndef build_generator(latent_dim):\n\n    # model = models.Sequential()\n    # model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    # model.add(layers.BatchNormalization())\n    # model.add(layers.LeakyReLU(alpha=0.2))\n    # model.add(layers.Reshape((16, 16, 128)))\n    # model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    # model.add(layers.BatchNormalization())\n    # model.add(layers.LeakyReLU(alpha=0.2))\n    # model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    # model.add(layers.BatchNormalization())\n    # model.add(layers.LeakyReLU(alpha=0.2))\n    # model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    # return model\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size,save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n        discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n        vgan_losses['generator_loss'].append(generator_loss)\n        if epoch % 1000 == 0:\n          save_images(generator.predict(np.random.normal(0, 1, (25,latent_dim))), path=f\"/content/drive/My Drive/Lung dataset/generated_imagesImproved/epoch_{epoch}\")\n    generator.save_weights(save_path)\n\nepochs=10000\nbatch_size=64\ndef save_images(images, path='/content/drive/My Drive/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size,'/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights2.h5')\n","recorded":"2024-10-21 15:21:36.672098692","filePath":"null","pinned":false},{"value":"img_width, img_height = 64, 64\nchannels = 1  # Grayscale\n\ndataset_dir = \"/content/drive/My Drive/Lung dataset/archive\"\n!ls \"/connt/drive/My Drive/Lung dataset/archive\"\n# Load and preprocess images from the dataset\nnormal_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height)\npneumonia_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height)","recorded":"2024-10-21 15:21:26.078676251","filePath":"null","pinned":false},{"value":"from tensorflow.keras import layers, models, optimizers","recorded":"2024-10-21 15:21:08.130368589","filePath":"null","pinned":false},{"value":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","recorded":"2024-10-21 15:21:01.221079794","filePath":"null","pinned":false},{"value":"Pratik Gupta","recorded":"2024-10-21 13:16:42.650612739","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Results.zip","recorded":"2024-10-21 13:07:37.042481862","filePath":"null","pinned":false},{"value":"INTRODUCTION\nCancer is a disease brought on by aberrant cells when an internal component is expanding\nout of control. A portion of the bodys cells in all tumors, begin to divide rapidly and\nspread to parts of the neighboring tissues. Among the millions of cells, cancer can appear\nvirtually at any place in the body. Normally, human cells multiply and divide to produce\nnew cells as the body requires them. When a cell becomes damaged or old, it expires and\nis replaced by a fresh cell. But as cancer grows, this systematic mechanism disintegrates.\nOld or injured cells that should have died survive, as cells become more and more erro-\nneous whereas new cells are generated even when they are unwanted. These cells can\ndivide to form new ones, which may lead to tumor-like growth. Solid tumors, or masses\nof tissue, are a common kind of cancer. Leukemias and other blood cancers typically do\nnot develop solid tumors\n1.1\n Oral Cancer\nThe mouths cells are the first to develop oral cancer. A cancerous (malignant) nodule is\na group of cancer cells tumor that has the ability to invade neighboring tissue and wreck\nmisery on it. It can also metastasize to different parts of the body. Nodes of lymph in\nthe neck are the part where mouth cancer spreads most frequently. Oral cancer may also\nbe referred to as mouth cancer. Sometimes, cells that are present in the mouth undergo\nchanges and will stop growing or behaving properly. These alterations could result in\nbenign (non-cancerous) tumors like warts and fibromas. Precancerous diseases can also\nbe brought on by changes in the mouths cells. This indicates that although the abnormal\ncells are not now cancer, there is a potential that they could develop into cancer if left\n1\nuntreated. Leukoplakia and erythroplakia are two of the most prevalent precancerous\ndisorders of the mouth.\nOral cancer can, however, occasionally result from alterations to the mouths cellular\nstructure. The oral mucosa (mucous membrane) is a lining that lines the mouth. The\nsquamous epithelium, which composes the oral mucosa, is made up of squamous cells.\nThese thin, flat squamous cells are where mouth cancer typically begins. The term for\nthis type of cancer is mouth squamous cell carcinoma.\n1.1.1\n Type of Oral Cancer\nThe following organs can develop cancer:\n Lips\n Tongue\n Inner lining of the cheek\n Gums\n Mouth Cancer\n Hard and Soft Palate\n1.2\n Mouth Cancer\nMouth cancer, commonly referred to as oral cancer, happens whenever a tumor forms\ninside the mouth lining. It could be located on the surface of the tongue, the interior of\nthe cheeks, the palate, the lips themselves, or the gums. Additionally, the glands that\ncreate tumors saliva, the tonsils in the rear within the mouth. But these occur frequently.\nSymptoms of mouth cancer:\n A mouth or lip sore that does not heal\n An internal mouth patch that is either white or red\n A growth or bulge inside your mouth; loose teeth;\n Painful or difficult swallowing\n2\n1.2.1\n Cause of Mouth Cancer\nWhen DNA alterations (mutations) occur in the mouth or lip cells, mouth carcinomas de-\nvelop. DNA includes the instructions for what the cell must accomplish. When normally\nfunctioning cells would die, alterations cause the continued growth and division of the\ncells. The aberrant mouth cancer cells can assemble into a tumor. In time, they might\nspread from the inside of the mouth to the whole body, including the neck or various parts\nof the head.\nMouth cancers tend to start in the flat, thin cells (squamous cells) which define the sur-\nface of the lips and the interior of the mouth. Oral cancer is most frequently caused by\nsquamous cell tumors.\n1.3\n Human Mouth Structure\nThe start of the human mouth is where the lips and skin converge Figure 1.1 shows the\nstructure of the human mouth. The roof of the mouth is made up of both hard and soft\npalates. A soft palate divides the mouth from the nasopharynx (the upper part of the\npharynx), which is connected to the mouth via the oropharynx (the middle section of the\npharynx). The sides of the mouth are formed by the cheeks inner surface (De Angeli et\nal. 2022). The majority of the mouths floor, or lowest portion, is occupied by the tongue.\nThe mouth can be divided into various sections, including-:\n The Lips\n The Tonsils and The Soft Palate\n The Uvula and the Tongue\n The buccal mucosa, which coats the cheekbones interior\n The roof of the mouth\n Teeth, gums, and alveolar ridge, which is the ridge-like border of the jaws that\ncontains the tooth sockets.\n The Mandible (Lower Jawbone)\n3\nFigure 1.1: Structure of Human mouth\n(German and Palmer 2006)\n1.4\n Diagnosing Techniques for Oral Cancer\nThere are different techniques that are used for the diagnosis of oral cancer, few of the\nclinical techniques used by doctors are discussed below :-\nBarium Swallow: - The voice box, the throat, referral, and surroundings may display\nabnormalities during a barium swallow test, which is also frequently used to find small,\nearly oral tumors.\nBiopsy: - The initial step in identifying mouth cancer is an oral tissue biopsy. A little bit\nof aberrant tissue from the area where oral cancer is suspected is removed by the surgeon\nduring the biopsy. An oral cancer diagnosis may be confirmed by biopsy. The following\ntypes of biopsies are frequently used to identify oral carcinoma:\n Incisive biopsies: The region has a small amount of tissue taken from it that appears\nto be abnormal. If the abnormal location is easily accessible, the specimen could\nbe obtained at the office of a doctor. If the cancer is more deeply embedded in the\nmouth or throat, biopsy procedures might have to be carried out in a surgical theatre\nwhile receiving anesthesia in order to lessen pain.\n Exfoliative cytology: Cell samples are gently scraped from a questionable loca-\ntion. To make the cells visible under a microscope, they are put upon a transparent\n4\nslide, and subsequently colored. A deeper biopsy will be done if any cells seem\nsuspicious.\nImage-based tests\n Computerized Tomography, or CT, Scanning  Information on the size, shape, and\nlocation of any tumors can be obtained via a CT scan, which can help detect lymph\nnodes that are bulging that may contain cancer cells.\n Magnetic Resonance Imaging (MRI): Oral cancer may be examined with an MRI\nscan, although this is less usual. MRIs give a very thorough picture and may be\nvery helpful in figuring out whether other areas of the body, such as the neck, have\nbeen affected by the diseases spread.\n Positron emission computed tomography (PET): Patients with cancer of the oral\ncavity might get a scan using PET technology to find out whether the disease has\nmigrated to the lymph nodes or whether it has only recently progressed to that\nlocation.\n Genomic testing for advanced oral cancer: -Genomic testing is sometimes known as\nmolecular profiling or cancer sequencing. Examining the collected cells is required\n8 from a biopsy in order to check for any genetic mutations (changes in your DNA)\nthat might be connected to the persons particular type of cancer.\n1.5\n Oral cancer: Globally\nAmong the most prevalent malignancies worldwide is oral cancer. The majority of cases\nof this subtype of head and neck cancer begin in the cells of squamous tissue that cover\nthe surface of our mouth, tongue, and faces. When this fails to be identified and if not\naddressed in a timely manner, it could be deadly. About 53,000 incidences of oral cancer,\nor three percent of all cancers identified during the study in US annually, are related to\noral cancer. Oral cancer strikes males more frequently than females, more than twice as\noften, and persons over an age of 40 are most at risk.\nSmoking, drinking alcoholic beverages, or having HPV, short for People Papilloma virus\ninfection are the main causes of oral cancer. In 2020, there are expected to be over 177,000\n5\ndeaths globally from lip and oral cavity cancer, In spite of improvements mouth cancer\nfatality rates have remained high in recent decades.\nThe majority of mouth cancer patients, particularly those located in countryside regions,\ncant obtain fast, effective diagnosis and treatment, which lowers their chance of survival.\nDepending on race and location, patients with cancer have a five-year living rate among\nthe 50%. According to reports, the survival rate in developed nations can reach 65%.\nIn contrast, leaning upon the area of the mouth cancer affected, a living rate of fifteen\npercent is noted in some countryside areas. Its because cancer therapy may be highly\nexpensive, especially in later stages. Health experts and the general public both lack a\nsignificant grasp of oral cancer. The 2020 Cancer Statistics Report for India states 66.6\npercent of patients with head and neck cancer had already progressed locally when they\nreceived their diagnosis. Inflammation or ulcers that do not heal, along with discomfort\nand bleeding, are signs of oral cancer.\nOral cancer can be caused by a number of habits, with smoking and drinking being the\ntwo most significant ones. Consuming maggots is so common in India that it causes in-\nternal gum damage.\nGLOBOCAN (Global Cancer Incidence, Mortality and Prevalence) anticipated that in\nFigure 1.2: Global age standardized prevalence of tobacco smoking source World Health\nOrganization\n(Dai, Gakidou, and Lopez 2022)\n2018, there would be 177,384 cancer-related deaths and 354,864 new instances of cancer,\n6\nwhich corresponds to two percent and one point nine percent of all occurrences and fa-\ntalities from cancer, respectively. In summary, mouth cancer, which accounts for around\none-third of all cancer cases, is a major reason for death in Bangladesh, Pakistan, Taiwan,\nand India.\n1.6\n Issues with Oral Cancer Manual Diagnoses\nThe primary problem with manual cancer diagnosis is the delay in diagnosis. It requires\nextremely competent labor, and the number of needed diagnostic tests is increasing dra-\nmatically. Because of the time requirements for a proper diagnosis, it is less likely that\nan early identification of the tumor grade will be made.Pathologists heavy workload is a\nserious worry, and this also affects how well they can anticipate outcomes. It also Prevent\nthe delivery of an accurate diagnosis report as the findings must be carefully crafted to\navoid any fatalities.\n1.7\n Deep Learning\nOne of the main components of an Artificially Intelligent system is learning. Learning\nmeans when a computer program can learn through its surrounding. Artificially intelli-\ngent systems have the ability to mimic the human brain and have the ability to process\ninformation and develop various patterns used to make decisions (Dubuc et al. 2022). A\nsub type of machine learning called \"deep learning\" in artificial intelligence (AI) allows\nnetworks to learn unsupervised from unlabelled input. Deep learning can also refer to\ndeep neural networks or deep learning.\n1.7.1\n Importance of Deep Learning\nMachine learning techniques can now build and learn from a large pool of training data\nbecause to improvements in computer speed and memory over time.\nDeep learning has been a cutting-edge method for humanity, especially when the Informa-\ntion is noisy. Artificial neural networks can learn any function with just one hidden layer,\nregardless of how ambiguous it is, which is why they are regarded as universal function\napproximations.\n7\n1.7.2\n CNN\nConvolutional neural network (CNN) is a subtype of ANN. In at least one of their layers,\nCNNs replace conventional matrix multiplication methods with the convolution mathe-\nmatical technique. Since they were developed specifically to handle pixel data, they are\nused in image recognition and processing. The design with which CNN is built is compa-\nrable with the model of neural connection like a persons brain (Jeyaraj, B. K. Panigrahi,\nand Samuel Nadar 2022). Because of the way CNN is built, there are some strong prefer-\nences ingrained in them, which makes it easier to comprehend why they are so effective.\nCNN can be seen as a feed-forward network but having connection with each image can\nFigure 1.3: A CNN Architecture\n(Sun et al. 2019)\nbe inefficient. Therefore, we can prune the useless connection between the hidden layers\nto increase the performance of the layer. A CNN is a special artificial neural network with\nlimited connections between the layers of artificial neural network.\n Max-Pooling: Each feature map produced by processing the input through many\nlayers of convolution is subsequently combined in a pooling layer. Little grids are\nused for input for pooling procedures, which generate only one value for every re-\ngion. The pooling layers provide CNN significant translational consistency since a\n8\ntiny change in the input image causes a slight modification in the activation maps.\nApplying convolutions with longer strides is another method for obtaining the pool-\nings down sampling effect. The network design is made simpler by eliminating the\npooling levels without compromising performance. Max-pooling is the most widely\nemployed of all these pooling techniques.\n Fully-Connected Layers: Matrix multiplications have traditionally been the build-\ning blocks of neural networks, which are scattered with sigmoid nonlinearities. The\nlayers of the multiplication matrices are referred to as connected layers due to the\nconnection between each unit in the layer before and each unit in the layer af-\nter. There is just small-scale spatial connectivity when using convolutional layers.\nSignificant amounts of completely linked layers are typically avoided in modern\nnetworks since they require massive parameters.\n Learning algorithm: Lacking an algorithm to quickly and effectively learn the pa-\nrameters of the model, there is little value for an expensive model. Lacking a tech-\nnique for efficiently acquiring the models parameters, a strong, expressive model\nis of little use. In the pre-AlexNet era, greedy layer-wise pre-training techniques\nattempted to create such an efficient approach. A more straightforward supervised\ntraining approach is sufficient to learn a reliable model for tasks relating to com-\nputer vision.\n Optimization Based on Gradient: - Typically, the backpropagation technique is used\nto train networks, which accelerates mathematical calculation to calculate the gra-\ndient used in the Gradient Descent (GD) algorithm. However, employing GD is\nimpracticable for datasets with many hundreds or even more data points. In these\ncircumstances, Stochastic Gradient Descent (SGD), an approximation where gradi-\nents are computed for data points individually rather than the complete data set, is\nfrequently used. Training using SGD generalizes more successfully than with GD,\nit has been discovered.\n Batch Normalization:- A helpful regularizes that enhances generalisation and sharply\naccelerates convergence is batch normalisation (BN). The order of presentation of\nthe inputs to each layer varies continuously during the training phase, which is a\nproblem caused by inner covariate variation. This effect typically causes training\n9\nto take longer and requires careful initialization. This problem is addressed by BN,\nwhich normalises a layers production stimulation to ensure that its spectrum is\nconstrained to a restricted range. In particular, BN normalises each mini-batchs\nmean-variance statistics using its running average. Recently, BN has been recog-\nnised as a crucial element of very deep networks.\n Activation layer :- Deep networks typically have convolutions after each layer,\nwhich then follows a nonlinear process. This is required because convolutions are\nan example of a cascading linear system. Layer-to-layer nonlinearities make the\nmodel more evocative than a model with linear dynamics. Theoretically, as long\nas nonlinearities are ongoing bounded, and gradually rising, no nonlinearity has a\ngreater capacity for expressiveness than any other. The sigmoid or the tanh were\nnonlinearities employed in classical neural networks that feed forward. However,\nthe Rectified Linear Unit (ReLU) is used in contemporary convolutional networks.\nIt has been discovered that CNNs with this nonlinearity train more quickly. The\nleaky- ReLU is a brand-new category of nonlinearity that has lately been intro-\nduced. Leaky-ReLU(x) = max(0, x) + min(0, x) is its formula, where is a preset\nparameter. It is better since it implies that the characteristic can also be taught,\ncreating a model that is considerably deeper. Leaky ReLUs or adjustable ReLUs\nare examples of variations on ReLU(z)=max (0; z). The feature maps, which are\nfrequently also referred to as feature maps, are fed through a process of activation\nto create new tensors.\n1.7.3\n Working on Deep Learning Networks\nSince most deep learning methods rely on neural network topologies, they are referred\ndescribed as \"deep neural networks\".\nNormal neural nets only have a few hidden levels, whereas deeper networks may contain\nup to 150 layers. Very vast quantity of categorised autonomously generated data and neu-\nral network topology extract features.\na) Training from Scratch:- For a deep network to be trained from beginning, a very large\nlabelled data set must be gathered, and a network architecture must be created that will\nallow the network to gain insight into its characteristics and predict. This is advantageous\nfor newly developed apps or applications with numerous output categories. This is a less\n10\nfrequent strategy because these networks often take weeks or even months to train be-\ncause to the volume of data and learning rate.\nb) Transfer Learning:- It is a deep learning technique where a pre-trained model is mod-\nified as part of the transfer learning approach. It begins with a reliable network, like\nAlexNet or GoogleNet, then feeds it new values which are previously undiscovered classes.\nThe task can now be carried, out after making network modifications that are minimal.\nMoreover, processing hundreds of photographs as opposed to millions has the advantage\nof requiring much less data, which cuts down computation time to minutes or hours.\nc) Feature Extraction:- The network can be used as a feature extractor, which is a little less\ntypical and a more specialised method of deep learning. Feature Extraction can remove\nspecific features from the network at any point throughout the training process because\nall the layers are charged with learning specific features from images.\n1.7.4\n Purpose of Deep Learning\nThe models developed using Deep Learning have the potential to provide more precise\nand individualised cancer treatment by better predicting the prognosis of the disease. They\nare superior to or on par with the methods now used in clinical settings. Deep learning\ntechniques are anticipated to help in the proper handling of squamous cell carcinoma of\nthe oral cavity through enhanced diagnostic performance, wise clinical decision-making,\nstreamlining of clinicians work, the potential for lowering cancer screening costs, and a\nsuccessful evaluation and detection of the disease. In order to increase the quality of care,\nprofessionals and patients can spend more time talking to one another and deliberating\ntogether. Future research should focus on creating deep learning models that integrate\ndiverse datasets from many modalities.\n Pre Processing: Due to a variety of factors, the original image will always contain\nsome noise. The accuracy of the diagnosis is compromised by these noises. A cru-\ncial part of the image processing process is pre-processing. Asymmetric filtration is\na filter that is frequently used to enhance grayscale photographs by reducing noise\nand improving image arrangement, particularly edge boundaries.\n Feature Extraction: - we can generate new features from the previous feature and\nthen we can delete the original features by doing this we can reduce the features\n11\npresent in the dataset. It helps us to categorize the images into different groups.\n Feature Selection: - Providing a vast amount of features to the model can result\nin a overfitted model with a very high computational time, having a better feature\nextraction will help in reducing the time complexity\nFigure 1.4: Flow chart showing different Phases in detection of oral cancer\n1.8\n Metaheuristic Optimization\nReal-world optimisation issues frequently involve a large number of choice variables, in-\ntricate nonlinear constraints, and difficult objective functions, which makes them more\nand more difficult to solve. Using conventional strategies like numerical methods, the\n12\nglobal optimization is less effective, particularly when limitations or objective functions\ninclude many peaks. Strong instruments for tackling difficult optimisation problems,\nmetaheuristic algorithms are gaining popularity.\nThe simplicity of metaheuristic algorithms is by far their most notable feature. The fun-\ndamental theories or mathematical models underlying these metaheuristic techniques are\nderived from nature. The majority of these techniques are straightforward and simple to\nuse. One can utilise metaheuristics to solve real-world problems thanks to their usability.\nAdditionally, it is simple to create their versions using current techniques.\nThese optimisation technologies can be thought of as \"black boxes,\" capable of providing\na set of outputs for a specific problem for a specific set of inputs. One of the most crucial\naspects of metaheuristic algorithms is randomization. This makes it possible for meta-\nheuristic algorithms to effectively avoid trapping in local optima and to search the whole\nsearch space. More specifically, it enables numerous metaheuristics to handle issues in-\nvolving an ambiguous search space or various local optima. Finally, because of their ex-\ntreme adaptability and flexibility, these metaheuristics can be used to solve a wide range\nof optimisation issues, including non-linear issues, issues involving non-differentiable\nvariables, and issues involving sophisticated numerical calculations and a large number\nof local minima.\n1.9\n Motivation\nThe latest trend in increase of oral cancer is having an adverse effect on health of human\nbeing. Oral cancer can be treated if detected early, with the increase in total number of\ncases of oral cancer we need an accurate and fast way to detect cancer cells. The risk\nof oral cancer is in all age groups but elder people are more prone to it due to unhealthy\nlifestyle. A lot of people have experienced financial troubles. It is crucial for the early\ndiagnosis of disease so that patients can start taking preventative measures right away. AI,\nwhich consists of machine learning and deep learning, is heavily reliant on classification,\ngrading, segmentation, and computer vision. To more or less better model optimisation is\nthe main reason for conducting research in this field.\n Deep learning concept fascinate me to learn more in this area. Deep learning based\nmodel can detect oral cancer with early signs that can be captured by modern cam-\n13\neras\n Clinical Images can give an more accurate and fast result as compare to normal\nmethods applied by Doctors\n1.10\n Problem Statement and Research Objective\nA large number of deaths were recorded from oral cancer as a result of lack of its identifi-\ncation and late treatment. Oral cavity cancer has a significant mortality rate that is rising.\nIt is crucial to develop and put into practise a method for detecting this malignancy early\non. By identifying cancer early and adopting preventative measures, it is simple to limit\nthe number of deaths brought on by the disease. Although many researchers have already\nconducted their research in the field of oral cancer disorders, there is still a great deal of\nresearch that may be done in this area owing to performance improvements.\nMachine learning has advanced to the point where getting more use out of it is all but\nimpossible during the last several years. The performance of Deep learning models have\nincreased but there is still a concern of model size, low accuracy and high computation\ntime.\n To propose and implement optimized Deep learning algorithm for detection of oral\ncancer in its early stages.\n To implement Metaheuristic optimization for better weight selection of clinical im-\nages.\n To conduct an analysis and compare the proposed approach with state of art models\non basic of evaluation matrices like accuracy, precision, Sensitivity and Specificity.\n1.11\n Thesis outline\nThe chapters of the thesis are organised consistently into an overview, key facts and fig-\nures, significant content, pertinent data, and a final chapter summary. All references are\nincluded at the end and each Figure, table, and piece of text is correctly referenced. The\nfive chapters that make up this thesis are arranged as follows:\n14\n Chapter 1: It provides a succinct overview of oral cancer, including its kinds, symp-\ntoms, and methods of diagnosis. It describes how the process of making medical\ndiagnoses has been transformed by machine learning, neural networks, and deep\nneural networks. Why has CNN surpassed conventional neural networks? what\nmotivated and inspired you to work in medicine. Additionally, it provides informa-\ntion about the goals and motivation.\n Chapter 2: It gives a brief overview of the literature for a number of researchers who\nworked on various methods for automatic oral cancer diagnosis, image processing,\nand texture-based categorization. Artificial neural networks, deep learning. A re-\nview of all pertinent theories and techniques for diagnosing oral cancer that are\navailable in the literature.\n Chapter 3: The chapter sheds insight on a crucial experiment study and the approach\nused to carry out our investigation. The models and various detection architectures\nemployed by CNN have been described. The proposed model is covered in this\nchapter; it has fewer parameters and a shallower learning curve than the pretrained\nmodel, but it is more accurate.\n Chapter 4: With the use of a graph, bar chart, and other presentation approaches,\nall model and performance metric results are shown. The models shortcomings are\nthen displayed and contrasted with the suggested model.\n Chapter 5: The entire work is concluded in the last chapter. This chapter also\ndiscusses how we might enhance our efforts in the future.","recorded":"2024-10-21 13:03:50.213154809","filePath":"null","pinned":false},{"value":"\\address[2]{Department of Computer Science \\\u0026 Engineering, DR. B.R. Ambedkar National Institute of Technology, Jalandhar -- $144027$, India}","recorded":"2024-10-21 12:58:49.563837468","filePath":"null","pinned":false},{"value":"Prognostic tools","recorded":"2024-10-21 12:57:44.248084303","filePath":"null","pinned":false},{"value":"Evolutionary optimization","recorded":"2024-10-21 12:57:36.356951912","filePath":"null","pinned":false},{"value":"Metaheuristic optimization","recorded":"2024-10-21 12:57:25.665484771","filePath":"null","pinned":false},{"value":"Manta Ray Foraging Optimization (MRFO)","recorded":"2024-10-21 12:57:16.333872857","filePath":"null","pinned":false},{"value":"Transfer learning","recorded":"2024-10-21 12:57:06.035801206","filePath":"null","pinned":false},{"value":"Clinical image analysis","recorded":"2024-10-21 12:56:58.668525204","filePath":"null","pinned":false},{"value":"Deep learning","recorded":"2024-10-21 12:56:50.072238422","filePath":"null","pinned":false},{"value":"Oral cancer detection","recorded":"2024-10-21 12:56:45.380432613","filePath":"null","pinned":false},{"value":"Oral cancer constitutes a considerable worldwide health challenge, especially in areas like India, where it is the sixth most common malignancy, resulting in roughly 130,000 deaths each year. Contemporary diagnostic methods, notwithstanding their diversity, encounter constraints in precision, especially in differentiating malignant cells. Recent advancements in deep learning have demonstrated potential in improving diagnostic accuracy, providing a means to decrease false positives and negatives, and facilitating more dependable prognostics and therapy strategies.\n\nDeep learning architectures, despite their computational complexity, have exhibited remarkable efficacy in numerous categorization tasks, including medical image analysis. Image-based deep learning algorithms for oral cancer diagnosis primarily employ two categories of datasets: clinical images and histopathological images. Due to the availability of high-quality imaging tools, clinical images are now more practical for extensive diagnostic applications. Conversely, histopathology images necessitate specialist equipment and high magnification, presenting practical hurdles for general application.\n\nThis research aims to utilize clinical picture datasets to develop a deep learning framework for the identification of oral cancer. We present an optimization-based metaheuristic strategy that integrates numerous pre-trained models, including VGG19, ResNet50, and EfficientNet, to improve prediction reliability. The novelty resides in the utilization of Manta Ray Foraging Optimization (MRFO) to optimize these models. This ensemble method markedly enhances diagnostic performance, attaining accuracy, sensitivity, and specificity ratings of 97.4%, 95.63%, and 94.12%, respectively.\n\nOur research illustrates the efficacy of deep learning in the identification of cancer through clinical imaging, providing a more accessible and efficient alternative. Future research avenues encompass the expansion of this study to multiclass cancer diagnosis and the investigation of evolutionary techniques for enhanced optimization of deep networks.","recorded":"2024-10-21 12:53:56.507944646","filePath":"null","pinned":false},{"value":"histopathological","recorded":"2024-10-21 12:52:36.198872543","filePath":"null","pinned":false},{"value":"Oral cancer remains a significant global health challenge, particularly in regions such as India, where it ranks as the fifth most prevalent cancer, leading to approximately 130,000 fatalities annually. Current diagnostic techniques, though varied, face limitations in their accuracy, particularly in distinguishing cancerous cells. Recent advances in deep learning have shown promise in enhancing diagnostic precision, offering an avenue to reduce false positives and negatives, and enabling more reliable prognostics and therapeutic interventions.\n\nDeep learning architectures, despite their computational complexity, have demonstrated exceptional performance in various classification tasks, including medical image analysis. For oral cancer diagnosis, image-based deep learning models primarily utilize two types of datasets: clinical images and histopathological images. Given the accessibility of high-quality imaging devices, clinical images have become more feasible for large-scale diagnostic applications. In contrast, histopathological images, requiring specialized equipment and high magnification, pose practical challenges in widespread usage.\n\nThis study focuses on leveraging clinical image datasets to build a deep learning framework for the detection of oral cancer. We propose a metaheuristic optimization-based approach, integrating multiple pre-trained models, including VGG19, ResNet50, and EfficientNet, to enhance prediction reliability. The innovation lies in the application of Manta Ray Foraging Optimization (MRFO) to fine-tune these models. This ensemble-based method significantly improves diagnostic performance, achieving accuracy, sensitivity, and specificity scores of 97.4%, 95.63%, and 94.12%, respectively.\n\nOur findings demonstrate the potential of deep learning in clinical image-based cancer detection, offering a more accessible and efficient solution. Future research directions include extending this work to multiclass cancer detection and exploring evolutionary algorithms for further optimization of deep networks.","recorded":"2024-10-21 12:50:40.139528367","filePath":"null","pinned":false},{"value":"Oral cancer is a prevalent and challenging cancer with a high mortality rate. It is the fifth\nmost common cancer in India, with 130,000 fatalities annually. There are a number of\ndiagnostic techniques for oral cancer, however their precision in identifying cancer cells\nis constrained.\nDeep architectures are becoming more popular as a result of their aptitude for solving\ncomplex problems. Deep architectures have proven effective in numerous classification\nproblems. Despite their incredible representational capacity, deep networks are difficult\nto train computationally. Deep learning minimises false-positive and false-negative errors\nin the detection and diagnosis of this condition, creating a new opportunity to provide\npatients with quick and safe prognostics treatments.\nDeep Learning models work on images, for oral cancer there are 2 dominant datasets\nof Clinical Images and Histopathological Images. The dataset used in study is clinical\ndataset, as now a days phones that can click a high quality images are available with ev-\nerybody therefore anyone can click a image and provide to model to check whether the\nimage id cancerous or not, on the other hand creating Histopathalogical images is a big\ntask, these images are taken at 400X magnification and such tools might not be readily\navailable. Therefore this study aims to use a model that can predict the outcome on clini-\ncal images\nHowever, the majority of research relies on a specific model prediction, and the final find-\nings may or may not be reliable. We suggested a metaheuristic optimization-based deep\nlearning technique. We employ many pre-trained models, including VGG19, ResNet50,\nand EfficientNet, using images of oral cancer. To evaluate the performance of the sug-\ngested method, all of the models output was compared to it. Various Transfer of Learning\nmodels with Manta Ray Foraging optimisation are used in our studies, and they produce\nbetter outcomes, with accuracy, sensitivity, and specificity ratings of 97.4, 95.63, and\n94.12, respectively. Future work on this project might include detecting multiclass can-\ncer photos and investigating deep network optimisation systems based on evolutionary\ntechniques.","recorded":"2024-10-21 12:49:22.986939210","filePath":"null","pinned":false},{"value":"bind = $mainMod SHIFT, N, exec, $fileManager\n","recorded":"2024-10-21 12:45:32.578800500","filePath":"null","pinned":false},{"value":"# CNN Model\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n# Initialising the CNN\nclassifier = Sequential()\n\n# Step 1 - Convolution\nclassifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n\n# Step 2 - Pooling\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\n# Adding a second convolutional layer\nclassifier.add(Conv2D(32, (3, 3), activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\n","recorded":"2024-10-21 12:38:38.407438932","filePath":"null","pinned":false},{"value":"def fib(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fib(n-1) + fib(n-2)\n\n\nn = int(input())\nprint(fib(n))\n","recorded":"2024-10-21 12:37:40.270187376","filePath":"null","pinned":false},{"value":"eyJhbGciOiJSUzI1NiIsImtpZCI6IjcxOGY0ZGY5MmFkMTc1ZjZhMDMwN2FiNjVkOGY2N2YwNTRmYTFlNWYiLCJ0eXAiOiJKV1QifQ.eyJuYW1lIjoiQ2hhZ2FudGkgIFJlZGR5IiwiaXNzIjoiaHR0cHM6Ly9zZWN1cmV0b2tlbi5nb29nbGUuY29tL2V4YTItZmIxNzAiLCJhdWQiOiJleGEyLWZiMTcwIiwiYXV0aF90aW1lIjoxNzI5NDkxMTY2LCJ1c2VyX2lkIjoiZGhFNEd1SldoUlZwM0M3Q3JxWk8yYjJ4dGtnMiIsInN1YiI6ImRoRTRHdUpXaFJWcDNDN0NycVpPMmIyeHRrZzIiLCJpYXQiOjE3Mjk0OTQwNzQsImV4cCI6MTcyOTQ5NzY3NCwiZW1haWwiOiJjaGFnYW50aXZlbmthdGFyYW1pcmVkZHkxQGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJmaXJlYmFzZSI6eyJpZGVudGl0aWVzIjp7ImVtYWlsIjpbImNoYWdhbnRpdmVua2F0YXJhbWlyZWRkeTFAZ21haWwuY29tIl19LCJzaWduX2luX3Byb3ZpZGVyIjoicGFzc3dvcmQifX0.nBrwYOnQvc87QZxrIsH0YAuscaqPixgvvNfkzYu-ENc_fmAf1dVlO7Ce4T_GxTIl1yjwQz7hubzgfWSOrNe5ol9fQdAFkdhh8KFFNmMlqw5AdjZ_-rrvIkXkIVdkCb0RfBuh3zJF7QYAy0diPQMVwCwftCG_sCDsDHI7VQw_oA9nh_TuPfS8ZHbzdlzhfUFphm6K7i7M4jAhCRcqcY_SCMRH6h9Gx6lzlFrAyjQqJDVzk6USaeeBwDF2kUSw4_zdLh67H2kkiHKOrpitWMQ4iXa1CvH_COlJjiu26T-EokZBitVextI0OFz9z6qqhm1V78E9LWWY5ggFrhA4ppuDXw","recorded":"2024-10-21 12:31:16.382651156","filePath":"null","pinned":false},{"value":"e","recorded":"2024-10-21 12:28:49.982735639","filePath":"null","pinned":false},{"value":"vim.g.codeium_enabled = false","recorded":"2024-10-21 12:23:42.739653911","filePath":"null","pinned":false},{"value":"config = function ()\n    -- Change '\u003cC-g\u003e' here to any keycode you like.\n    vim.keymap.set('i', '\u003cC-g\u003e', function () return vim.fn['codeium#Accept']() end, { expr = true, silent = true })\n    vim.keymap.set('i', '\u003cc-;\u003e', function() return vim.fn['codeium#CycleCompletions'](1) end, { expr = true, silent = true })\n    vim.keymap.set('i', '\u003cc-,\u003e', function() return vim.fn['codeium#CycleCompletions'](-1) end, { expr = true, silent = true })\n    vim.keymap.set('i', '\u003cc-x\u003e', function() return vim.fn['codeium#Clear']() end, { expr = true, silent = true })\n  end","recorded":"2024-10-21 12:23:03.084152391","filePath":"null","pinned":false},{"value":"vim.g.codeium_disable_bindings = 1","recorded":"2024-10-21 12:22:42.184635808","filePath":"null","pinned":false},{"value":"Clear current suggestion \tcodeium#Clear() \t\u003cC-]\u003e\nNext suggestion \tcodeium#CycleCompletions(1) \t\u003cM-]\u003e\nPrevious suggestion \tcodeium#CycleCompletions(-1) \t\u003cM-[\u003e\nInsert suggestion \tcodeium#Accept() \t\u003cTab\u003e\nManually trigger suggestion \tcodeium#Complete() \t\u003cM-Bslash\u003e\nAccept word from suggestion \tcodeium#AcceptNextWord() \t\u003cC-k\u003e\nAccept line from suggestion \tcodeium#AcceptNextLine() \t\u003cC-l\u003e","recorded":"2024-10-21 12:21:06.836901678","filePath":"null","pinned":false},{"value":"{\n  'Exafunction/codeium.vim',\n  event = 'BufEnter'\n}","recorded":"2024-10-21 12:20:15.441259524","filePath":"null","pinned":false},{"value":"cursor.sh","recorded":"2024-10-21 11:42:12.978260554","filePath":"null","pinned":false},{"value":"workspace_swipe_direction_lock","recorded":"2024-10-21 11:39:14.431692895","filePath":"null","pinned":false},{"value":"Institute","recorded":"2024-10-20 17:03:37.368255083","filePath":"null","pinned":false},{"value":"Technology","recorded":"2024-10-20 17:03:32.239256667","filePath":"null","pinned":false},{"value":"DR. B.R. AMBEDKAR NATIONAL INSTITUTE OF TECHNOLOGY","recorded":"2024-10-20 17:03:16.693009910","filePath":"null","pinned":false},{"value":"Department of Computer Science \\\u0026 Engineering","recorded":"2024-10-20 17:03:05.839678702","filePath":"null","pinned":false},{"value":"\\address[3]{Faculty of Sciences and Mathematics, University of Ni\\v s, Vi\\v segradska 33, 18000 Ni\\v s, Serbia}","recorded":"2024-10-20 17:02:48.931873839","filePath":"null","pinned":false},{"value":"/mnt/Windows/Documents and Settings/chaga/Downloads/Transfer/(58) Evaluating Web Metrics to Enhance Web Page Quality.pptx","recorded":"2024-10-20 14:38:36.203395895","filePath":"null","pinned":false},{"value":"/mnt/Windows/Documents and Settings/chaga/Downloads/Transfer/(166) Blockchain-Driven Roundabout Production Network.pptx","recorded":"2024-10-20 14:38:25.752314342","filePath":"null","pinned":false},{"value":"Mukesh","recorded":"2024-10-19 19:05:29.731382357","filePath":"null","pinned":false},{"value":"Assistant Professor","recorded":"2024-10-19 19:05:20.119179976","filePath":"null","pinned":false},{"value":"Aman Chandra Kaushik","recorded":"2024-10-19 19:01:17.834488447","filePath":"null","pinned":false},{"value":"+86-15618987739","recorded":"2024-10-19 19:01:03.863543452","filePath":"null","pinned":false},{"value":"Jiangsu,","recorded":"2024-10-19 19:00:22.145628204","filePath":"null","pinned":false},{"value":"No. 1800, Li Lake Avenue","recorded":"2024-10-19 19:00:17.164427989","filePath":"null","pinned":false},{"value":"School of Medicine, Jiangnan University","recorded":"2024-10-19 19:00:08.021786114","filePath":"null","pinned":false},{"value":"Rakesh Prasad","recorded":"2024-10-19 18:57:45.502644311","filePath":"null","pinned":false},{"value":"Jeedimetla ","recorded":"2024-10-19 18:52:47.846436967","filePath":"null","pinned":false},{"value":"Bahadurpally","recorded":"2024-10-19 18:52:41.152402121","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Venkat LoR - University of Maryland_Rakesh.pdf","recorded":"2024-10-19 18:42:36.771026937","filePath":"null","pinned":false},{"value":"Venkat LoR - University of Maryland_Rakesh","recorded":"2024-10-19 18:42:19.481414719","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/LoR - Venkat_Rakesh.pdf","recorded":"2024-10-19 18:41:46.134210945","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Masters-Documents/Applications/TAMU/LoR - Venkat_Rakesh.pdf","recorded":"2024-10-19 18:39:33.254839670","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Venkat LoR - University of Maryland_Mann.pdf","recorded":"2024-10-19 18:38:17.047426300","filePath":"null","pinned":false},{"value":"Venkat LoR - University of Maryland_Mann","recorded":"2024-10-19 18:37:48.010101730","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Venkat LoR - University of Maryland_Aman.pdf","recorded":"2024-10-19 18:37:05.807854852","filePath":"null","pinned":false},{"value":"Venkat LoR - University of Maryland_Aman","recorded":"2024-10-19 18:36:59.564463678","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Masters-Documents/Applications/LOR/Aman_Sir_lor.docx\n/mnt/Karna/Git/Masters-Documents/Applications/LOR/Mann_Recommendation_Letter.pdf\n/mnt/Karna/Git/Masters-Documents/Applications/LOR/Rakesh_Recommendation_Letter.pdf","recorded":"2024-10-19 18:35:00.019725221","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Venkat LoR - University of Maryland_Aman.docx\n/home/karna/Downloads/Venkat LoR - University of Maryland_Aman.pdf\n/home/karna/Downloads/Venkat LoR - University of Maryland_Mann.docx\n/home/karna/Downloads/Venkat LoR - University of Maryland_Mann.pdf\n/home/karna/Downloads/Venkat LoR - University of Maryland_Rakesh.docx\n/home/karna/Downloads/Venkat LoR - University of Maryland_Rakesh.pdf","recorded":"2024-10-19 18:34:32.765742089","filePath":"null","pinned":false},{"value":"Venkat LoR - University of Maryland_Mukesh","recorded":"2024-10-19 18:32:38.655115174","filePath":"null","pinned":false},{"value":"Mentoring Venkat on a major project allowed me to observe his exceptional technical acumen and research skills, which have consistently impressed me.  His projects demonstrated his deep understanding of the subject matter and the ability to implement complex methodologies. His initiative to submit this work to a reputed journal reflects his dedication and ambition to contribute to bioinformatics and drug development.\n\nOne of Venkats most admirable qualities is his versatility in working independently and within a team. His leadership skills were evident as he effectively coordinated with team members, yet he excelled when tasked with individual responsibilities, ensuring timely and high-quality output in both scenarios. His problem-solving abilities, particularly in improving model performance and integrating diverse datasets, set him apart as a proactive and analytical thinker.\n\nThroughout our collaboration, I have seen Venkat grow immensely in tackling interdisciplinary challenges, combining his machine learning and computational biology expertise to devise innovative solutions. His proficiency in transfer learning techniques and meticulous attention to enhancing model accuracy underscore his technical depth and analytical mindset. While he has shown remarkable progress, further strengthening his statistical foundation will amplify his research potential.","recorded":"2024-10-19 18:29:02.817881970","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Masters-Documents/Applications/UMD/Venkat LoR - University of Maryland_Aman.tex","recorded":"2024-10-19 18:25:47.485855962","filePath":"null","pinned":false},{"value":"Venkat LoR - University of Massachusetts Amherst_Aman","recorded":"2024-10-19 18:25:27.759218536","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Masters-Documents/Applications/UMD/leadership.tex","recorded":"2024-10-19 18:23:42.862162284","filePath":"null","pinned":false},{"value":"https://www.kaggle.com/competitions/npci-credit-card-default-risk-analysis/leaderboard","recorded":"2024-10-19 17:26:31.214129086","filePath":"null","pinned":false},{"value":"https://github.com/Chaganti-Reddy/CJPR-Report.git","recorded":"2024-10-19 16:20:01.346870626","filePath":"null","pinned":false},{"value":"git@github.com:Chaganti-Reddy/CJPR-Report.git","recorded":"2024-10-19 16:19:58.597453011","filePath":"null","pinned":false},{"value":"Hyderabad@2003","recorded":"2024-10-19 13:23:16.002307804","filePath":"null","pinned":false},{"value":"venkataramireddychaganti41@gmail.com","recorded":"2024-10-19 13:23:09.066026227","filePath":"null","pinned":false},{"value":"VEN3150927","recorded":"2024-10-19 12:57:25.455648729","filePath":"null","pinned":false},{"value":"graduate@rice.edu","recorded":"2024-10-19 12:40:38.164596429","filePath":"null","pinned":false},{"value":"mcisnero@rice.edu","recorded":"2024-10-19 12:40:16.589536145","filePath":"null","pinned":false},{"value":"I wanted to let you know that I checked my ASU portal for any unfinished chores after receiving the email below, but I was unable to locate any on my application site. Since I am eager to enroll in the university, could you kindly let me know if there is anything further I need to do to complete the application?","recorded":"2024-10-19 12:13:50.368299329","filePath":"null","pinned":false},{"value":"Just wanted to inform that after receiving the below email, I have checked my ASU portal for any incomplete tasks but couldn't find any in my application portal. So, could you please confirm if anything is pending from myside so that I can finish the application as I am very keen in getting into the university.","recorded":"2024-10-19 12:13:33.715238020","filePath":"null","pinned":false},{"value":"Dear Sir/Madam,\n\nHope this email finds you well\n\n    Just wanted to inform that after receiving the below email, I have checked my ASU portal for any incomplete tasks but couldn't find any in my application portal. So, could you please confirm if anything is pending from myside so that I can finish the application as I am very keen in getting into the university.\n\nThanks \u0026 Regards\nVenkatarami Reddy Chaganti\n\n---------- Forwarded message ---------\nFrom: Chandan on behalf of ASU \u003casu@mail.kaplanpathways.com\u003e\nDate: Fri, Oct 18, 2024 at 6:34PM\nSubject: Your ASU application is incomplete\nTo: \u003cvenkataramireddychaganti41@gmail.com\u003e\n\n\nView in browser\nKAPLAN INTERNATIONAL IN PARTNERSHIP WITH\nVenkatarami Reddy Chaganti, your ASU application needs further action\nYour application details\nCampus: ASU Tempe campus\n\t\nStart Date: August 2025\nWe have submitted your application to Arizona State University (ASU), but there are still some tasks you need to complete before it can be evaluated.\nYour application is incomplete\nYour application is currently marked as incomplete. To find out why, youll need to log into your MyASU student portal.\nCheck MyASU student portal\nPlease complete any outstanding tasks as soon as possible. ASU will not consider you for admission until your application is complete. \n\nIf you have recently completed your application, you can ignore this message.\nBy completing your application, youre taking the first  and most important  step toward joining the inspiring community of 11,000+ international students at ASU.  \n\nWhile you complete your application, check out this video on why Phoenix is a fantastic study destination. \nwhy Connecticut is a fantastic destination to study abroad. \nIf you have any questions, please contact us by replying to this email, or reach out to your agent, IMFS KP Singh education Services pvt ltd. To ensure a quick response, one of my helpful colleagues may reply.\nBest wishes,\n\nChandan Sharma\nDirector of Application Management, US\nKaplan International on behalf of Arizona State University\nABOUT US\nWe are Kaplan International. We help students follow their path to leading universities across the world. We offer access to exceptional teaching, thousands of degrees and deeply rewarding experiences.\n\nKaplan International works in partnership with Arizona State University to provide application counselling and admissions support to international students.\n\nIn the UK, Kaplan International Pathways is the trading name of Kaplan International Colleges UK Ltd.Company No. 05268303. Registered in England. Registered office: Palace House, 3 Cathedral Street London, SE1 9DE, United Kingdom.\n\t\nYOUR SUBSCRIPTION DETAILS\nVenkatarami Reddy Chaganti, you are receiving this email because you have submitted an application to study at Arizona State University.\n\nWe need to keep you informed throughout your application and admission process with important email updates. You can update your details or manage your subscription. Alternatively, you can unsubscribe if you no longer want to receive these updates.\nPrivacy Policy\nKaplan International 2024. All rights reserved.\nFacebook\nTwitter\nInstagram\nYouTube\nLinkedIn\n","recorded":"2024-10-19 12:12:41.420710595","filePath":"null","pinned":false},{"value":"asu@mail.kaplanpathways.com","recorded":"2024-10-19 12:07:06.719742161","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Files/aco-main\n/home/karna/Downloads/Files/CP\n/home/karna/Downloads/Files/Git\n/home/karna/Downloads/Files/Motor Imagery Classification Performance Enhancement with EEG Data Augmentation\n/home/karna/Downloads/Files/Oral-cancer-detection-using-deep-learning-main\n/home/karna/Downloads/Files/Pratik Project GAN Generate lung images\n/home/karna/Downloads/Files/Reinforcement-learning-approach-for-prognosis-in-ICU\n/home/karna/Downloads/Files/aco-main.zip\n/home/karna/Downloads/Files/Bus_Routing_Problems__June_06th__2024_.pdf\n/home/karna/Downloads/Files/format.pptx\n/home/karna/Downloads/Files/Oral-cancer-detection-using-deep-learning-main.zip","recorded":"2024-10-17 17:52:46.816018517","filePath":"null","pinned":false},{"value":"Karna","recorded":"2024-10-17 17:52:29.994697881","filePath":"null","pinned":false},{"value":"Backup","recorded":"2024-10-17 17:46:10.904625718","filePath":"null","pinned":false},{"value":"/run/media/karna/Xtras/EndeavourOS_Endeavour_neo-2024.09.22.iso","recorded":"2024-10-17 17:32:32.021445202","filePath":"null","pinned":false},{"value":"/mnt/Karna/aco-main\n/mnt/Karna/CP\n/mnt/Karna/Git\n/mnt/Karna/Motor Imagery Classification Performance Enhancement with EEG Data Augmentation\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main\n/mnt/Karna/Pratik Project GAN Generate lung images\n/mnt/Karna/Reinforcement-learning-approach-for-prognosis-in-ICU\n/mnt/Karna/aco-main.zip\n/mnt/Karna/Bus_Routing_Problems__June_06th__2024_.pdf\n/mnt/Karna/format.pptx\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main.zip","recorded":"2024-10-17 17:30:49.088953163","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/chaganti-reddy.github.io/static/uploads/resume.tex","recorded":"2024-10-17 12:33:52.255110051","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Books/Quantum Computing/Griffiths - Introduction to quantum mechanics.pdf","recorded":"2024-10-17 12:31:59.750544890","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Evaluating_Web_Metrics_for_Enhancing_WebPage_Quality.pptx\n/home/karna/Downloads/venkat_blockchain_6321.pptx","recorded":"2024-10-17 12:31:09.426441434","filePath":"null","pinned":false},{"value":"haskell-skylighting-format-latex-0.1-135-x86_64                                       29.8 KiB   109 KiB/s 00:00 [--------------------------------------------------------------------] 100%","recorded":"2024-10-17 12:11:28.291771970","filePath":"null","pinned":false},{"value":"haskell-hslua-marshalling-2.3.1-13-x86_64","recorded":"2024-10-17 11:55:35.322241289","filePath":"null","pinned":false},{"value":"haskell-indexed-traversable-0.1.4-4-x86_64","recorded":"2024-10-17 11:55:34.657888545","filePath":"null","pinned":false},{"value":"haskell-fast-logger-3.1.2-85-x86_64","recorded":"2024-10-17 11:55:33.917732938","filePath":"null","pinned":false},{"value":"haskell-hslua-module-doclayout-1.1.0-70-x86_64","recorded":"2024-10-17 11:55:32.836506099","filePath":"null","pinned":false},{"value":"                                       73.6 KiB   116 KiB/s 00:01 [--------------------------------------------------------------------] 100%\n haskell-fas","recorded":"2024-10-17 11:55:31.438995978","filePath":"null","pinned":false},{"value":"haskell-hslua-module-docla","recorded":"2024-10-17 11:55:30.101757100","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/HITA2024/Blockchain-driven Roundabout Production Network/Blockchain_empowered_Roundabout_Production_Network_in_the_Agri-food_Supply_Chain_for_Spanning_Trust_T.pptx","recorded":"2024-10-16 20:38:56.068859882","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/HITA2024/Empirical Validation on Web Pages/Evaluating_Web_Metrics_for_Enhancing_WebPage_Quality_.pptx","recorded":"2024-10-16 20:38:51.148275063","filePath":"null","pinned":false},{"value":"/mnt/Karna/format.pptx","recorded":"2024-10-16 20:37:49.806377121","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/format.pptx","recorded":"2024-10-16 20:37:45.510997482","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Files/aco-main\n/home/karna/Downloads/Files/CP\n/home/karna/Downloads/Files/Git\n/home/karna/Downloads/Files/Motor Imagery Classification Performance Enhancement with EEG Data Augmentation\n/home/karna/Downloads/Files/Oral-cancer-detection-using-deep-learning-main\n/home/karna/Downloads/Files/Pratik Project GAN Generate lung images\n/home/karna/Downloads/Files/Reinforcement-learning-approach-for-prognosis-in-ICU\n/home/karna/Downloads/Files/aco-main.zip\n/home/karna/Downloads/Files/Bus_Routing_Problems__June_06th__2024_.pdf\n/home/karna/Downloads/Files/Oral-cancer-detection-using-deep-learning-main.zip","recorded":"2024-10-16 20:37:19.476187140","filePath":"null","pinned":false},{"value":"Windows","recorded":"2024-10-16 20:35:35.566081274","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Files/Git/HITA2024/Blockchain-driven Roundabout Production Network/Blockchain_empowered_Roundabout_Production_Network_in_the_Agri-food_Supply_Chain_for_Spanning_Trust_T.pptx","recorded":"2024-10-16 16:42:48.866499802","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Files/Git/HITA2024/Empirical Validation on Web Pages/Evaluating_Web_Metrics_for_Enhancing_WebPage_Quality_.pptx","recorded":"2024-10-16 16:42:41.637230516","filePath":"null","pinned":false},{"value":"This study focuses on evaluating key web metrics to enhance the quality of web pages.","recorded":"2024-10-16 16:39:15.007246002","filePath":"null","pinned":false},{"value":"overview","recorded":"2024-10-16 16:33:46.897316240","filePath":"null","pinned":false},{"value":"This study focuses on evaluating key web metrics to enhance the quality of web pages.\nData is sourced from 600 websites nominated for the Webby Awards between 2017 and 2022.\n\nKey performance and quality metrics include:\nSpeed Index\nTotal Blocking Time\nTime to Interactive (TTI)\nFirst Contentful Paint (FCP)\n\nA Python-based automated tool was developed to analyze:\n16 quality measures\n6 performance indicators\n\nWebsites were categorized into high and low quality based on these metrics.\n\nThe findings offer insights into optimizing web design and improving user experiences on online platforms.","recorded":"2024-10-16 14:41:34.168033252","filePath":"null","pinned":false},{"value":" 109676-797521221.png","recorded":"2024-10-16 14:40:19.840872922","filePath":"/home/karna/.config/clipse/tmp_files/109676-797521221.png","pinned":false},{"value":" 8528-415532030.png","recorded":"2024-10-16 14:39:35.420247983","filePath":"/home/karna/.config/clipse/tmp_files/8528-415532030.png","pinned":false},{"value":"\u003cmeta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\"\u003e\u003cimg src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRSSl4axzcEv9vzUQU6zJqcaknKrYeyBb7QYQ\u0026amp;s\" class=\"sFlh5c FyHeAf\" alt=\"Mahindra University | Hyderabad\" jsname=\"JuXqh\" style=\"max-width:761px;\" data-ilt=\"1729069761536\"\u003e","recorded":"2024-10-16 14:39:35.416280525","filePath":"null","pinned":false},{"value":"Mahindra University,","recorded":"2024-10-16 14:38:06.470891760","filePath":"null","pinned":false},{"value":"Rakesh Prasad Badoni","recorded":"2024-10-16 14:37:30.403429629","filePath":"null","pinned":false},{"value":"Evaluating Web Metrics for Enhancing Web Page Quality","recorded":"2024-10-16 14:29:28.853951628","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Files/Internships","recorded":"2024-10-15 13:16:32.871616056","filePath":"null","pinned":false},{"value":" 4412-465009130.png","recorded":"2024-10-15 10:55:35.468435878","filePath":"/home/karna/.config/clipse/tmp_files/4412-465009130.png","pinned":false},{"value":"https://catalog.arizona.edu/programs/COSCMS","recorded":"2024-10-15 10:55:35.466170212","filePath":"null","pinned":false},{"value":" 3962-572380516.png","recorded":"2024-10-15 10:55:24.575993323","filePath":"/home/karna/.config/clipse/tmp_files/3962-572380516.png","pinned":false},{"value":"https://degrees.apps.asu.edu/masters-phd/major/ASU00/ESCSEBDMS/computer-science-big-data-systems-ms","recorded":"2024-10-15 10:55:24.573466893","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Win10_22H2_EnglishInternational_x64v1.iso","recorded":"2024-10-14 19:10:15.445627020","filePath":"null","pinned":false},{"value":"/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/boot\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/efi\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/sources\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/support\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/autorun.inf\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/bootmgr\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/bootmgr.efi\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/setup.exe","recorded":"2024-10-14 18:59:16.981858978","filePath":"null","pinned":false},{"value":"exo-open","recorded":"2024-10-14 18:53:55.910158476","filePath":"null","pinned":false},{"value":"TerminalEmulator","recorded":"2024-10-14 18:53:06.282987859","filePath":"null","pinned":false},{"value":"afc://00008120-0010246C11E8201E:3/org.mozilla.ios.Firefox/Downloads/www.5MovieRulz.top%20-%20Mangalavaaram%20(2023)%201080p%20Telugu%20HQ%20HDRip%20-%20HEVC%20-%20%20(DD%205.1%20-%20192kbps%20_%20AAC)%20-%201.8GB%20-%20ESub.mkv.zip","recorded":"2024-10-14 18:48:38.054554017","filePath":"null","pinned":false},{"value":"/mnt/Karna/aco-main\n/mnt/Karna/CP\n/mnt/Karna/Git\n/mnt/Karna/ISRO\n/mnt/Karna/Motor Imagery Classification Performance Enhancement with EEG Data Augmentation\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main\n/mnt/Karna/Pratik Project GAN Generate lung images\n/mnt/Karna/Reinforcement-learning-approach-for-prognosis-in-ICU\n/mnt/Karna/1.mp3\n/mnt/Karna/acknowledgementSlip_S1858416595000.pdf\n/mnt/Karna/aco-main.zip\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main.zip","recorded":"2024-10-14 18:45:39.154727189","filePath":"null","pinned":false},{"value":"/run/media/karna/Xtras/Win10_22H2_EnglishInternational_x64v1.iso","recorded":"2024-10-14 18:45:22.361748592","filePath":"null","pinned":false},{"value":"#!/bin/bash\nnotify_levels=(3 5 10 20)\nBAT=$(ls /sys/class/power_supply |grep BAT |head -n 1)\nlast_notify=100\n\nwhile true; do\n    bat_lvl=$(cat /sys/class/power_supply/${BAT}/capacity)\n    if [ $bat_lvl -gt $last_notify ]; then\n            last_notify=$bat_lvl\n    fi\n    for notify_level in ${notify_levels[@]}; do\n        if [ $bat_lvl -le $notify_level ]; then\n            if [ $notify_level -lt $last_notify ]; then\n                notify-send -u critical \"Low Battery\" \"$bat_lvl% battery remaining.\"\n                last_notify=$bat_lvl\n            fi\n        fi\n    done\nsleep 60\ndone","recorded":"2024-10-11 14:07:39.939515930","filePath":"null","pinned":false},{"value":"/mnt/Karna/Pratik Project GAN Generate lung images/archive.zip","recorded":"2024-10-11 14:04:00.079000123","filePath":"null","pinned":false},{"value":"exec-once = numlockx on \u0026\n","recorded":"2024-10-11 11:12:16.372495951","filePath":"null","pinned":false},{"value":"AA2005233711@@","recorded":"2024-10-10 18:03:28.843983252","filePath":"null","pinned":false},{"value":"ramchaganti200@gmail.com","recorded":"2024-10-10 18:03:24.922993123","filePath":"null","pinned":false},{"value":"https://leetcode.com/discuss/interview-question/5886397/DSA-Patterns-you-need-to-know-!!!","recorded":"2024-10-10 18:01:00.267913418","filePath":"null","pinned":false},{"value":"Minor_Project_Generating_Lung_Images_GAN_network","recorded":"2024-10-10 17:46:32.731696132","filePath":"null","pinned":false},{"value":"            // Bluetooth Devices Bluetooth icon \n","recorded":"2024-10-10 17:30:19.699745624","filePath":"null","pinned":false},{"value":"            \n","recorded":"2024-10-10 17:30:18.833974826","filePath":"null","pinned":false},{"value":"\n","recorded":"2024-10-10 17:30:18.229474834","filePath":"null","pinned":false},{"value":"            \"Bluetooth Devices\": \"\u003cspan foreground='#a6adc8'\u003e \u003c/span\u003e Bluetooth Devices\",\n","recorded":"2024-10-10 17:30:17.426970677","filePath":"null","pinned":false},{"value":"            \"(.*)Bluetooth Devices\": \"\u003cspan foreground='#a6adc8'\u003e \u003c/span\u003e $1\",\n","recorded":"2024-10-10 17:30:16.423901188","filePath":"null","pinned":false},{"value":"            \"(.*) - Bluetooth Devices\": \"\u003cspan foreground='#f38ba8'\u003e \u003c/span\u003e $1\",\n","recorded":"2024-10-10 17:28:37.921056718","filePath":"null","pinned":false},{"value":"            \"Bluetooth Devices\": \"\u003cspan foreground='#f38ba8'\u003e \u003c/span\u003e Bluetooth Devices\",\n","recorded":"2024-10-10 17:28:37.615069268","filePath":"null","pinned":false},{"value":"        \"custom/kernelinfo\",\n","recorded":"2024-10-10 17:23:27.113485597","filePath":"null","pinned":false},{"value":"Generating Radiologically Realistic Lung Images\nwith Generative Adversarial Networks","recorded":"2024-10-10 16:20:58.922037573","filePath":"null","pinned":false},{"value":"train","recorded":"2024-10-10 16:10:55.268502000","filePath":"null","pinned":false},{"value":"img_width, img_height = 64, 64\nchannels = 1  # Grayscale\n\ndataset_dir = \"/content/drive/My Drive/Lung dataset/archive\"\n!ls \"/content/drive/My Drive/Lung dataset/archive\"\n# Load and preprocess images from the dataset\nnormal_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height)\npneumonia_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height)","recorded":"2024-10-10 16:06:31.174340605","filePath":"null","pinned":false},{"value":"archive","recorded":"2024-10-10 16:04:57.653750846","filePath":"null","pinned":false},{"value":"Lung dataset","recorded":"2024-10-10 16:04:48.029545501","filePath":"null","pinned":false},{"value":"Reviewer #1: The authors could provide good work. However, there are some concerns to be resolved.","recorded":"2024-10-10 16:01:56.372229701","filePath":"null","pinned":false},{"value":"The abstract needs to be improved. The first sentence in the abstract, it is necessary for the authors to add a sentence to describe the problem or motivation to focus on this topic. The second sentence should provide the literature gap. In the third sentence, the authors should say what you are doing, and then provide the empirical findings. Finally, the significance of the finding should be offered.","recorded":"2024-10-10 16:01:50.999127571","filePath":"null","pinned":false},{"value":"This study addresses the vehicle routing problem (VRP) en-\ncountered by transportation bus service providers with the goal of opti-\nmizing their bus routes. The hypothesis posits that each vehicle should\nreach a pickup point, considered a boarding location, before proceed-\ning to the designated destination. At each pickup point, a list of one or\nmore passengers departing from that specific location exists. The primary\nobjective of this study is to minimize the overall transportation cost by\nenhancing routing efficiency while accounting for application constraints,\ncapacity limitations, and time constraints. To achieve this, we propose a\nmathematical model encompassing various boarding and dropping points\nwith diverse time periods to enhance vehicle routing. The subsequent\nphase focuses on the ant colony optimization algorithm, addressing the\nproblem comprehensively within its expansive scope. Finally, to validate\nthe effectiveness of the proposed algorithm, we conduct tests on real-\nworld data to ascertain its practical viability.","recorded":"2024-10-10 16:01:11.156488946","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/LOR check list .docx","recorded":"2024-10-10 15:58:11.341078197","filePath":"null","pinned":false},{"value":"The strength of Mr. Venkatarami Reddy lies in his ability to adapt and embed his knowledge into various real world problems with his research ability. The same has done under my guidance as well, embedded machine learning in predicting protein structures using Deep Learning techniques. One of the most important and unique skill that I had observed is his ability to work independently as well as working with team. He can complete the work on time in both the scenario, which resembles his team work and leadership.\n\tYes, his work on protein structure prediction using deep learning, specifically with CNNs and transformer-based models, would certainly stand out. He achieved high precision and accuracy which has practical applications in bioinformatics and drug development. His initiative to submit this work to a prestigious journal further underscores his dedication to advancing knowledge in the field.\n\tI would highlight Venkataramis problem-solving skills, as demonstrated by his ability to improve model performance and integrate complex datasets. \n\tI have seen significant growth in Venkataramis ability to tackle interdisciplinary problems, blending his knowledge of machine learning with computational biology. His technical depth and understanding of advanced models have matured throughout our collaboration, as evidenced by his innovative solutions to complex biological challenges.\n\tStrengthening his statistical knowledge would further enhance his research capabilities, especially in fields requiring rigorous validation and testing.\nCertainly, his proficiency in implementing and optimizing advanced machine learning models, along with his ability to apply techniques like transfer learning, highlights his strong technical and problem-solving skills. He paid close attention to detail in improving model performance and accuracy, which reflects his analytical mindset.","recorded":"2024-10-10 14:58:16.623405641","filePath":"null","pinned":false},{"value":"    11. Can you recall any specific examples of the student's strengths in your class or projects?\n\t\n\tThe strength of Mr. Venkatarami Reddy lies in his ability to adapt and embed his knowledge into various real world problems with his research ability. The same has done under my guidance as well, embedded machine learning in predicting protein structures using Deep Learning techniques.\n\n    12. What qualities do you think graduate admissions committees value most, and how does the student compare?\n\n\tOne of the most important and unique skill that I had observed is his ability to work independently as well as working with team. He can complete the work on time in both the scenario, which resembles his team work and leadership.\n \n    13. Are there any examples of the student's work that you believe would stand out to an admissions committee?\n\t\n\tYes, his work on protein structure prediction using deep learning, specifically with CNNs and transformer-based models, would certainly stand out. He achieved high precision and accuracy which has practical applications in bioinformatics and drug development. His initiative to submit this work to a prestigious journal further underscores his dedication to advancing knowledge in the field.\n\n    14. What specific traits, like problem-solving or teamwork, would you highlight in the student's recommendation?\n\n\tI would highlight Venkataramis problem-solving skills, as demonstrated by his ability to improve model performance and integrate complex datasets. \n\n    15. Where have you seen the most academic or professional growth in the student?\n\n\tI have seen significant growth in Venkataramis ability to tackle interdisciplinary problems, blending his knowledge of machine learning with computational biology. His technical depth and understanding of advanced models have matured throughout our collaboration, as evidenced by his innovative solutions to complex biological challenges.\n\n    16. What areas should the student work on before starting this program?\n\n\tStrengthening his statistical knowledge would further enhance his research capabilities, especially in fields requiring rigorous validation and testing.\n\n    17. Would you be able to provide examples to support the key skills the student plans to emphasize in their application?\n\nCertainly, his proficiency in implementing and optimizing advanced machine learning models, along with his ability to apply techniques like transfer learning, highlights his strong technical and problem-solving skills. He paid close attention to detail in improving model performance and accuracy, which reflects his analytical mindset. ","recorded":"2024-10-10 14:57:24.794451561","filePath":"null","pinned":false},{"value":"The strength of Mr. Venkatarami Reddy lies in his ability to adapt and embed his knowledge into various real world problems with his research ability. The same has done under my guidance as well, embedded machine learning in predicting protein structures using Deep Learning techniques.","recorded":"2024-10-10 14:57:19.428796595","filePath":"null","pinned":false},{"value":"I've known Mr. Venkatarami Reddy Chaganti for the past 9 months. He worked as a research assistant under my supervision. I asked him to work on a research topic of his interest and he choose AI and it's working in various fields. \nHe chose the project titled \"Court Judgement Prediction and Recommendation\"\nBeing impressed by his research work, I gave him an opportunity to work as a full time research assistant under my guidance.\n\nDuring his project I noticed that he has always been very passionate about learning new things and tried implementing them. He always completed the task in the given time. He has an eye to detail and is fastidious about the results for every minor task that he does.\n\nOne of the most important and unique quality that I've observed in Venkat Is time management and his arduous zeal to finish the given task on time with accuracy. \n\nExamples: 1) Submission of multiple manuscripts on time for the conference \n2) Conducting research and creating pipelines accurately and precisely in given time.\n\nHe has submitted 4 research manuscripts within this span of 9-10 months and Not just restricted to a single domain, he worked on various domains and he  always wants to understand the usage of a domain in multiple domains, such as incorporating AI in law and Reinforcement learning in traffic signals and also AI In Computational Biology.  This shows his quest to knowledge and consistency which makes him unique.\n\nIn this tenure there has always been an exponential growth in both his academic performance and research work managing both equally.\n\nHe always wants to take the research work to the next level in an advanced way , \nbut due to limited resources \u0026 limited guidance he faces small obstacles that hinders him from achieving great things.\n\nI believe Computer Science course work at your esteemed institution and advanced facilities helps him to hone his skills.","recorded":"2024-10-10 14:33:58.874633602","filePath":"null","pinned":false},{"value":"I take the opportunity to write this letter to support Mr. Venkatarami Reddy Chaganti to pursue his masters at your esteemed university. I am a professor at the Computer Science department at Indian Institute of Information Technology Sonepat. I have taught several students in my 9 years of experience, and I confidently say that Venkat is one of the brightest among them. I have known him for the past 4 years, during which I taught him Computer Programming, DBMS, Data Structures, Software Engineering, Soft Computing. I have also mentored him during his Smart Indian Hackathon 2023 and two of his major final year projects. During this time, I observed Venkat picking a team of students with relevant skills to get the job done. This unique skill, combined with his academic brilliance, helped him complete the projects quickly and efficiently. \n\nFrom an academic perspective, Venkat is a gold medalist and a brilliant student with innate curiosity and a willingness to understand academic concepts by any means possible. These qualities helped him rank in the top 2% of his batch. This is a testament to his high academic caliber. I've seen Venkat be well prepared for lecturescompleting pre-reads, being up to speed with course content, and completing assignments on time. His enthusiasm towards the subject has always made the class more interactive as he would ask questions and introduce discussions. His interpersonal skills allow him to get along with his peers. He illustrated eminent verbal articulation in the project seminar, which was an integral part of the course.\n\nDuring his second year he started AI \u0026 ML club.  His main intention was to share the profound knowledge that he has on AI \u0026 ML to his peers and his juniors. Initially, he did not get facilitated with enough resources \u0026 support. However, with an unwavering determination and commitment he formed a club \u0026 actively participated and engaged all the members of the club to participate in many hackathons. Not just restricted to active participation. He made his team win some of them. This shows his team work \u0026 excellent leadership skills. During these 4 years I've noticed he has developed overall not just academically but also personally. I had the privilege to recommend him to multiple internship positions.\n\nObserving Venkat over four years, I firmly believe that he has the potential to succeed in all of his future endeavors, both academically and professionally. His academic knowledge, perseverance, and attitude will ensure his ascent to greater heights. I am confident that he/she will be a valuable asset to your cohort. I would strongly recommend his candidature for the program at your esteemed university.","recorded":"2024-10-10 14:32:06.873296071","filePath":"null","pinned":false},{"value":"Certainly. The students proficiency in implementing and optimizing advanced machine learning models, along with their ability to apply techniques like transfer learning, highlights their strong technical and problem-solving skills. They pay close attention to detail in improving model performance and accuracy, which reflects their analytical mindset. ","recorded":"2024-10-10 14:31:01.220676018","filePath":"null","pinned":false},{"value":"I have seen significant growth in Venkataramis ability to tackle interdisciplinary problems, blending his knowledge of machine learning with computational biology. His technical depth and understanding of advanced models have matured throughout our collaboration, as evidenced by his innovative solutions to complex biological challenges.","recorded":"2024-10-10 14:30:25.875194023","filePath":"null","pinned":false},{"value":"I would highlight Venkataramis problem-solving skills, as demonstrated by his ability to improve model performance and integrate complex datasets. ","recorded":"2024-10-10 14:30:05.042907396","filePath":"null","pinned":false},{"value":"Would you be able to provide examples to support the key skills the student plans to emphasize in their application? Certainly. Venkataramis proficiency in implementing and optimizing deep learning models for biological data, his use of transfer learning, and his attention to detail in model performance metrics are all excellent examples of his technical and problem-solving skills. His leadership in managing a lab and his collaborative efforts with peers demonstrate his ability to work in a team-oriented research environment, which he plans to highlight in his application.","recorded":"2024-10-10 14:29:37.105899628","filePath":"null","pinned":false},{"value":"Strengthening his statistical knowledge would further enhance his research capabilities, especially in fields requiring rigorous validation and testing.","recorded":"2024-10-10 14:28:59.542457827","filePath":"null","pinned":false},{"value":"which has practical applications in bioinformatics and drug development. His initiative to submit this work to a prestigious journal further underscores his dedication to advancing knowledge in the field.","recorded":"2024-10-10 14:28:13.779178443","filePath":"null","pinned":false},{"value":"Yes, his work on protein structure prediction using deep learning, specifically with CNNs and transformer-based models, would certainly stand out. He achieved high precision and ","recorded":"2024-10-10 14:28:00.298402300","filePath":"null","pinned":false},{"value":"    1. Can you recall any specific examples of the student's strengths in your class or projects?\n\n    2. How has the student demonstrated leadership, critical thinking, or problem-solving in your course?\n\n    3. What qualities do you think graduate admissions committees value most, and how does the student compare?\n\n    4. How does the student's academic performance align with the requirements of the program they're applying to?\n \n    5. Are there any examples of the student's work that you believe would stand out to an admissions committee?\n\n    6. What specific traits, like problem-solving or teamwork, would you highlight in the student's recommendation?\n\n    7. Where have you seen the most academic or professional growth in the student?\n\n    8. What areas should the student work on before starting this program?\n\n    9. How do the student's goals align with the work youve seen them do in your class?\n\n    10. Would you be able to provide examples to support the key skills the student plans to emphasize in their application?\n","recorded":"2024-10-10 14:19:27.351344754","filePath":"null","pinned":false},{"value":"It is a pleasure to write a recommendation for a student as dynamic and bright as Venkatarami Reddy Chaganti. I have had the privilege of evaluating his work as an advisor for his undergraduate final-year project. I taught him Machine Learning, Deep Learning, and Computational Biology in UG's 3rd and 4th years. Throughout our association, I have witnessed his unwavering passion for his studies. His academic performance in our college is a testament to his excellence and his potential to excel in the MS in Computer Science.\nVenkatarami is a person who can combine knowledge and diligence to accomplish his goals. He is a keen student with high level of acumen and has a good grasp of his subjects. He is highly motivated and is always on the lookout to learn something new. Venkatarami is receptive to new ideas. He can work well in a team and has been managing the computational biology laboratory proficiently since last two years.\nFurthermore, Venkatarami developed and trained advanced deep learning models using convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformer-based models. He employed techniques like transfer learning and data augmentation to enhance the performance and accuracy of these models. His efforts led to high precision in predicting protein structures, reflecting his deep understanding of machine learning algorithms and computational biology principles.  Venkatarami's final product, notable for its accuracy and user-friendly interface, has significant practical applications in bioinformatics and drug development. He is now preparing to submit it to a prestigious journal, highlighting his commitment to contributing to the field through scholarly research.  \nBased on his excellent academic performance and well-honed interpersonal skills, I am confident that Mr. Venkatarami Reddy Chaganti will not only meet but also surpass your expectations. Therefore, I strongly recommend him for graduate studies at your university and financial assistance in the form of a research assistantship. I am certain he will be a source of pride for your institution.","recorded":"2024-10-10 14:17:42.444380643","filePath":"null","pinned":false},{"value":"sir what about the mail?","recorded":"2024-10-10 14:08:53.810105481","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Masters-Documents/Applications/Certificates BTech/CMM.pdf","recorded":"2024-10-10 13:45:20.125020557","filePath":"null","pinned":false},{"value":"ASU ID: 1236373673","recorded":"2024-10-10 13:42:12.437698105","filePath":"null","pinned":false},{"value":"transcripts@asu.edu","recorded":"2024-10-10 13:41:53.969405003","filePath":"null","pinned":false},{"value":"Can you recall any specific examples of the student's strengths in your class or projects?","recorded":"2024-10-10 12:55:57.286988587","filePath":"null","pinned":false},{"value":"    1. Can you recall any specific examples of the student's strengths in your class or projects?\n\n    2. How has the student demonstrated leadership, critical thinking, or problem-solving in your course?\n\n    3. What qualities do you think graduate admissions committees value most, and how does the student compare?\n\n    4. How does the student's academic performance align with the requirements of the program they're applying to?\n \n    5. Are there any examples of the student's work that you believe would stand out to an admissions committee?\n\n    6. What specific traits, like problem-solving or teamwork, would you highlight in the student's recommendation?\n\n    7. Where have you seen the most academic or professional growth in the student?\n\n    8. What areas should the student work on before starting this program?\n\n    9. How do the student's goals align with the work youve seen them do in your class?\n\n    10. Would you be able to provide examples to support the key skills the student plans to emphasize in their application?","recorded":"2024-10-10 12:45:11.895827843","filePath":"null","pinned":false},{"value":"known","recorded":"2024-10-10 12:43:00.213948367","filePath":"null","pinned":false},{"value":"I have had the privilege of evaluating his work as an advisor for his undergraduate final-year project.","recorded":"2024-10-10 12:37:03.802921821","filePath":"null","pinned":false},{"value":"I've known Mr. Venkatarami Reddy Chaganti for the past 9 months. He worked as a research assistant under my supervision.","recorded":"2024-10-10 12:36:21.374666062","filePath":"null","pinned":false},{"value":"LOR  2 Mann Sir","recorded":"2024-10-10 12:24:33.253439128","filePath":"null","pinned":false},{"value":"Observing XXXX over four years, I firmly believe that he/she has the potential to succeed in all of his/her future endeavors, both academically and professionally. His/Her academic knowledge, perseverance, and attitude will ensure his/her ascent to greater heights. I am confident that he/she will be a valuable asset to your cohort. I would strongly recommend his/her candidature for the program at your esteemed university.","recorded":"2024-10-10 12:23:27.316832739","filePath":"null","pinned":false},{"value":"During these 4 years I've noticed he has developed overall not just academically but also personally. I had the privilege to recommend him to multiple internship positions.","recorded":"2024-10-10 12:18:19.067274552","filePath":"null","pinned":false},{"value":"This shows his team work \u0026 excellent leadership skills","recorded":"2024-10-10 12:18:13.680667210","filePath":"null","pinned":false},{"value":"During his second year he started AI \u0026 ML club. \nHis main intention was to share the profound knowledge that he has on AI \u0026 ML to his peers and his juniors.\n\nInitially, he did not get facilitated with enough resources \u0026 support.\nHowever, with an unwavering determination and commitment he formed a club \u0026 actively participated and engaged all the members of the club to participate in many hackathons.\nNot just restricted to active participation. He made his team win many hackathons.","recorded":"2024-10-10 12:17:59.516196225","filePath":"null","pinned":false},{"value":"From an academic perspective, Venkat is a gold medalist and a brilliant student with innate curiosity and a willingness to understand academic concepts by any means possible. These qualities helped him rank in the top 5% of his batch. This is a testament to his high academic caliber. I've seen Venkat be well prepared for lecturescompleting pre-reads, being up to speed with course content, and completing assignments on time. His enthusiasm towards the subject has always made the class more interactive as he would ask questions and introduce discussions. His interpersonal skills allow him to get along with his peers. He illustrated eminent verbal articulation in the project seminar, which was an integral part of the course.","recorded":"2024-10-10 12:06:38.744252873","filePath":"null","pinned":false},{"value":"I take the opportunity to write this letter to support Mr. Venkatarami Reddy Chaganti to pursue his masters at your esteemed university. I am a professor at the Computer Science department at Indian Institute of Information Technology Sonepat. I have taught several students in my 9 years of experience, and I confidently say that Venkat is one of the brightest among them. I have known him for the past 4 years, during which I taught him Computer Programming, DBMS, Data Structures, Software Engineering, Soft Computing. I have also mentored him during his Smart Indian Hackathon 2023 and two of his major final year projects. During this time, I observed Venkat picking a team of students with relevant skills to get the job done. This unique skill, combined with his academic brilliance, helped him complete the projects quickly and efficiently. ","recorded":"2024-10-10 12:04:30.458736208","filePath":"null","pinned":false},{"value":" LOR  1 Rakesh Sir","recorded":"2024-10-10 11:55:23.365785403","filePath":"null","pinned":false},{"value":"Not just restricted to a single domain, he worked on various domains ","recorded":"2024-10-10 11:54:10.384414976","filePath":"null","pinned":false},{"value":"I've known venkat for the past 9 months. He worked as a research assistant under my supervision. I asked him to work on a research topic of his interest and he choose AI and it's working in various fields. \nHe chose the project titled \" \"\nBeing impressed by his research work, I gave him an opportunity to work as a full time research assistant under my guidance.\n\n\nDuring his project I noticed that he has always been very passionate about learning new things and tried implementing them. He always completed the task in the given time. He has an eye to detail and is fastidious about the results for every minor task that he does.\n\nOne of the most important quality that I've observed in VRR\nIs time management and his arduous zeal to finish the given task on time with accuracy. \nEg's\n\n\nHe has submitted 4 research papers on various topics like :\n1.\n2.\n3.\nNot just restricted to a single domain, he worked on various domains \n\nThis shows his quest to knowledge and consistency which makes him unique.\n\nIn this 9 months there has always been an exponential growth in both his academic performance and research work managing both equally.\n\nHe always wants to take the research work to the next level in an advanced way , \nbut due to limited resources \u0026 limited guidance he faces small obstacles that hinders him from achieving great things.\nI believe XYZ course work at your esteemed institution and advanced facilities helps him to hone his skills.","recorded":"2024-10-10 11:46:53.979828630","filePath":"null","pinned":false},{"value":"https://meet.google.com/kju-thvw-pze","recorded":"2024-10-10 10:38:13.959460561","filePath":"null","pinned":false}]}